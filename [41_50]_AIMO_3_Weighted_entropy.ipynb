{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaH100",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 289055161,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 510391,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 404485,
          "modelId": 422384
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "[41/50] AIMO 3: Weighted entropy",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantexolution/aimo/blob/main/%5B41_50%5D_AIMO_3_Weighted_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "yPU7in_VD8R9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "andreasbis_aimo_3_utils_path = kagglehub.notebook_output_download('andreasbis/aimo-3-utils')\n",
        "danielhanchen_gpt_oss_120b_transformers_default_1_path = kagglehub.model_download('danielhanchen/gpt-oss-120b/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "EdHcIhyzD8R-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "EAgEaZ9kD8R-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; warnings.simplefilter('ignore')\n",
        "import os, sys, subprocess, gc, re, math, time, queue, threading, contextlib"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "t28QgsLkD8R-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def set_env(archive, tmp):\n",
        "    if not os.path.exists(tmp):\n",
        "        os.makedirs(tmp, exist_ok=True)\n",
        "        subprocess.run(['tar', '-xzf', archive, '-C', tmp], check=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-index', '--find-links', f'{tmp}/wheels',\n",
        "                    'unsloth', 'trl', 'vllm', 'openai_harmony'], check=True)\n",
        "\n",
        "set_env('/kaggle/input/aimo-3-utils/wheels.tar.gz', '/kaggle/tmp/setup')"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "M3QQ2BHID8R-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "Se0iKhrJD8R-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in [('TRANSFORMERS_NO_TF', '1'), ('TRANSFORMERS_NO_FLAX', '1'), ('CUDA_VISIBLE_DEVICES', '0'),\n",
        "             ('TOKENIZERS_PARALLELISM', 'false'), ('TRITON_PTXAS_PATH', '/usr/local/cuda/bin/ptxas'),\n",
        "             ('TIKTOKEN_ENCODINGS_BASE', '/kaggle/tmp/setup/tiktoken_encodings')]:\n",
        "    os.environ[k] = v"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "lXyMV2cOD8R-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from jupyter_client import KernelManager\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
        "import pandas as pd, polars as pl\n",
        "from openai import OpenAI\n",
        "from openai_harmony import (HarmonyEncodingName, load_harmony_encoding, SystemContent, ReasoningEffort,\n",
        "                             ToolNamespaceConfig, Author, Message, Role, TextContent, Conversation)\n",
        "from transformers import set_seed\n",
        "import kaggle_evaluation.aimo_3_inference_server"
      ],
      "metadata": {
        "trusted": true,
        "id": "BYnf4uZsD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    system_prompt = ('You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
        "                    'The final answer must be a non-negative integer between 0 and 99999. '\n",
        "                    'You must place the final integer answer inside \\\\boxed{}.')\n",
        "    tool_prompt = ('Use this tool to execute Python code. The environment is a stateful Jupyter notebook. '\n",
        "                  'You must use print() to output results.')\n",
        "    preference_prompt = 'You have access to `math`, `numpy` and `sympy` to solve the problem.'\n",
        "    served_model_name, model_path = 'gpt-oss', '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
        "    kv_cache_dtype, dtype = 'fp8_e4m3', 'auto'\n",
        "    high_problem_timeout, base_problem_timeout = 900, 270\n",
        "    notebook_limit, server_timeout = 17400, 180\n",
        "    session_timeout, jupyter_timeout, sandbox_timeout = 960, 6, 3\n",
        "    stream_interval, context_tokens, buffer_tokens, search_tokens = 200, 65536, 512, 32\n",
        "    top_logprobs, batch_size, early_stop, attempts, workers, turns = 5, 256, 4, 8, 16, 128\n",
        "    gpu_memory_utilization, temperature, min_p, seed = 0.96, 1.0, 0.02, 42"
      ],
      "metadata": {
        "trusted": true,
        "id": "I2MLFHEJD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SFmRRM-eD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Template:\n",
        "    def get_system_content(self, prompt, tool_cfg):\n",
        "        return SystemContent.new().with_model_identity(prompt).with_reasoning_effort(\n",
        "            reasoning_effort=ReasoningEffort.HIGH).with_tools(tool_cfg)\n",
        "\n",
        "    def apply_chat_template(self, sys_prompt, usr_prompt, tool_cfg):\n",
        "        return [Message.from_role_and_content(Role.SYSTEM, self.get_system_content(sys_prompt, tool_cfg)),\n",
        "                Message.from_role_and_content(Role.USER, usr_prompt)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "fkWX2yt0D8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Sandbox:\n",
        "    _port_lock, _next_port = threading.Lock(), 50000\n",
        "\n",
        "    @classmethod\n",
        "    def _get_next_ports(cls, count=5):\n",
        "        with cls._port_lock:\n",
        "            ports = list(range(cls._next_port, cls._next_port + count))\n",
        "            cls._next_port += count\n",
        "            return ports\n",
        "\n",
        "    def __init__(self, timeout):\n",
        "        self._default_timeout, self._owns_kernel, self._client, self._km = timeout, False, None, None\n",
        "        ports = self._get_next_ports(5)\n",
        "        env = os.environ.copy()\n",
        "        env.update({'PYDEVD_DISABLE_FILE_VALIDATION': '1', 'PYDEVD_WARN_EVALUATION_TIMEOUT': '0',\n",
        "                   'JUPYTER_PLATFORM_DIRS': '1', 'PYTHONWARNINGS': 'ignore', 'MPLBACKEND': 'Agg'})\n",
        "        self._km = KernelManager()\n",
        "        self._km.shell_port, self._km.iopub_port, self._km.stdin_port, self._km.hb_port, self._km.control_port = ports\n",
        "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
        "        self._client = self._km.blocking_client()\n",
        "        self._client.start_channels()\n",
        "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
        "        self._owns_kernel = True\n",
        "        self.execute('import math, numpy, sympy, mpmath, itertools, collections\\nmpmath.mp.dps = 64\\n')\n",
        "\n",
        "    def _format_error(self, tb):\n",
        "        return ''.join(re.sub(r'\\x1b\\[[0-9;]*m', '', f) for f in tb\n",
        "                      if 'File \"' not in f or 'ipython-input' in f)\n",
        "\n",
        "    def execute(self, code, timeout=None):\n",
        "        effective_timeout = timeout or self._default_timeout\n",
        "        msg_id = self._client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n",
        "        stdout, stderr, start = [], [], time.time()\n",
        "        while True:\n",
        "            if time.time() - start > effective_timeout:\n",
        "                self._km.interrupt_kernel()\n",
        "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
        "            try:\n",
        "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            if msg.get('parent_header', {}).get('msg_id') != msg_id: continue\n",
        "            mt, c = msg.get('msg_type'), msg.get('content', {})\n",
        "            if mt == 'stream':\n",
        "                (stdout if c.get('name') == 'stdout' else stderr).append(c.get('text', ''))\n",
        "            elif mt == 'error':\n",
        "                stderr.append(self._format_error(c.get('traceback', [])))\n",
        "            elif mt in {'execute_result', 'display_data'}:\n",
        "                if txt := c.get('data', {}).get('text/plain'):\n",
        "                    stdout.append(txt if txt.endswith('\\n') else f'{txt}\\n')\n",
        "            elif mt == 'status' and c.get('execution_state') == 'idle':\n",
        "                break\n",
        "        out, err = ''.join(stdout), ''.join(stderr)\n",
        "        return f'{out.rstrip()}\\n{err}' if err and out else (err or out or '[WARN] No output. Use print() to see results.')\n",
        "\n",
        "    def close(self):\n",
        "        with contextlib.suppress(Exception):\n",
        "            if self._client: self._client.stop_channels()\n",
        "        if self._owns_kernel and self._km:\n",
        "            with contextlib.suppress(Exception): self._km.shutdown_kernel(now=True)\n",
        "            with contextlib.suppress(Exception): self._km.cleanup_resources()\n",
        "\n",
        "    def reset(self):\n",
        "        self.execute('%reset -f\\nimport math, numpy, sympy, mpmath, itertools, collections\\nmpmath.mp.dps = 64\\n')\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "id": "28ZPxw2GD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Tool:\n",
        "    def __init__(self, timeout, prompt, sandbox=None):\n",
        "        self._local_jupyter_timeout, self._tool_prompt, self._jupyter_session = timeout, prompt, sandbox\n",
        "        self._owns_session, self._execution_lock, self._init_lock = sandbox is None, threading.Lock(), threading.Lock()\n",
        "\n",
        "    def _ensure_session(self):\n",
        "        if self._jupyter_session is None:\n",
        "            with self._init_lock:\n",
        "                if self._jupyter_session is None:\n",
        "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
        "\n",
        "    def _ensure_last_print(self, code):\n",
        "        lines = code.strip().split('\\n')\n",
        "        if not lines: return code\n",
        "        last = lines[-1].strip()\n",
        "        if any(x in last for x in ['print', 'import']) or not last or last.startswith('#'): return code\n",
        "        lines[-1] = 'print(' + last + ')'\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    @property\n",
        "    def instruction(self): return self._tool_prompt\n",
        "\n",
        "    @property\n",
        "    def tool_config(self): return ToolNamespaceConfig(name='python', description=self.instruction, tools=[])\n",
        "\n",
        "    def _make_response(self, output, channel=None):\n",
        "        msg = Message(author=Author(role=Role.TOOL, name='python'),\n",
        "                     content=[TextContent(text=output)]).with_recipient('assistant')\n",
        "        return msg.with_channel(channel) if channel else msg\n",
        "\n",
        "    def process_sync_plus(self, message):\n",
        "        self._ensure_session()\n",
        "        final_script = self._ensure_last_print(message.content[0].text)\n",
        "        with self._execution_lock:\n",
        "            try:\n",
        "                output = self._jupyter_session.execute(final_script)\n",
        "            except TimeoutError as exc:\n",
        "                output = f'[ERROR] {exc}'\n",
        "        return [self._make_response(output, channel=message.channel)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "UZ0_Q6RQD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Solver:\n",
        "    def __init__(self, cfg, port=8000):\n",
        "        self.cfg, self.port = cfg, port\n",
        "        self.base_url, self.api_key = f'http://0.0.0.0:{port}/v1', 'sk-local'\n",
        "        self.template, self.encoding = AIMO3Template(), load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
        "        self._preload_model_weights()\n",
        "        self.server_process = self._start_server()\n",
        "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
        "        self._wait_for_server()\n",
        "        self._initialize_kernels()\n",
        "        self.notebook_start_time, self.problems_remaining = time.time(), 50\n",
        "\n",
        "    def _preload_model_weights(self):\n",
        "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
        "        start, files, total = time.time(), [], 0\n",
        "        for root, _, fnames in os.walk(self.cfg.model_path):\n",
        "            for fn in fnames:\n",
        "                fp = os.path.join(root, fn)\n",
        "                if os.path.isfile(fp):\n",
        "                    files.append(fp)\n",
        "                    total += os.path.getsize(fp)\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as ex:\n",
        "            list(ex.map(lambda p: open(p, 'rb').read(), files))\n",
        "        print(f'Processed {len(files)} files ({total/1e9:.2f} GB) in {time.time()-start:.2f} seconds.\\n')\n",
        "\n",
        "    def _start_server(self):\n",
        "        cmd = [sys.executable, '-m', 'vllm.entrypoints.openai.api_server', '--seed', str(self.cfg.seed),\n",
        "               '--model', self.cfg.model_path, '--served-model-name', self.cfg.served_model_name,\n",
        "               '--tensor-parallel-size', '1', '--max-num-seqs', str(self.cfg.batch_size),\n",
        "               '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization), '--host', '0.0.0.0',\n",
        "               '--port', str(self.port), '--dtype', self.cfg.dtype, '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
        "               '--max-model-len', str(self.cfg.context_tokens), '--stream-interval', str(self.cfg.stream_interval),\n",
        "               '--async-scheduling', '--disable-log-stats', '--enable-prefix-caching']\n",
        "        self.log_file = open('vllm_server.log', 'w')\n",
        "        return subprocess.Popen(cmd, stdout=self.log_file, stderr=subprocess.STDOUT, start_new_session=True)\n",
        "\n",
        "    def _wait_for_server(self):\n",
        "        print('Waiting for vLLM server...')\n",
        "        start = time.time()\n",
        "        for _ in range(self.cfg.server_timeout):\n",
        "            if (rc := self.server_process.poll()) is not None:\n",
        "                self.log_file.flush()\n",
        "                raise RuntimeError(f'Server died with code {rc}. Full logs:\\n{open(\"vllm_server.log\").read()}\\n')\n",
        "            try:\n",
        "                self.client.models.list()\n",
        "                print(f'Server is ready (took {time.time()-start:.2f} seconds).\\n')\n",
        "                return\n",
        "            except Exception:\n",
        "                time.sleep(1)\n",
        "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
        "\n",
        "    def _initialize_kernels(self):\n",
        "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
        "        start = time.time()\n",
        "        self.sandbox_pool = queue.Queue()\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as ex:\n",
        "            for future in as_completed([ex.submit(lambda: AIMO3Sandbox(timeout=self.cfg.jupyter_timeout))\n",
        "                                       for _ in range(self.cfg.workers)]):\n",
        "                self.sandbox_pool.put(future.result())\n",
        "        print(f'Kernels initialized in {time.time()-start:.2f} seconds.\\n')\n",
        "\n",
        "    def _scan_for_answer(self, text):\n",
        "        for pattern in [r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}', r'final\\s+answer\\s+is\\s*([0-9,]+)']:\n",
        "            if matches := re.findall(pattern, text, re.IGNORECASE):\n",
        "                try:\n",
        "                    val = int(matches[-1].replace(',', ''))\n",
        "                    if 0 <= val <= 99999: return val\n",
        "                except ValueError: pass\n",
        "        return None\n",
        "\n",
        "    def _compute_mean_entropy(self, logprobs):\n",
        "        if not logprobs: return float('inf')\n",
        "        total, count = 0.0, 0\n",
        "        for top_lp in logprobs:\n",
        "            if isinstance(top_lp, dict) and top_lp:\n",
        "                ent = sum(-math.exp(lp)*math.log2(math.exp(lp)) for lp in top_lp.values() if math.exp(lp) > 0)\n",
        "                total += ent\n",
        "                count += 1\n",
        "        return total/count if count else float('inf')\n",
        "\n",
        "    def _process_attempt(self, problem, sys_prompt, idx, stop_evt, deadline):\n",
        "        if stop_evt.is_set() or time.time() > deadline:\n",
        "            return {'Attempt': idx+1, 'Answer': None, 'Python Calls': 0, 'Python Errors': 0,\n",
        "                   'Response Length': 0, 'Entropy': float('inf')}\n",
        "        local_tool, sandbox, py_calls, py_errs, total_toks, ans, logprobs = None, None, 0, 0, 0, None, []\n",
        "        seed = int(math.pow(self.cfg.seed + idx, 2))\n",
        "        try:\n",
        "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
        "            local_tool = AIMO3Tool(self.cfg.jupyter_timeout, self.cfg.tool_prompt, sandbox)\n",
        "            conv = Conversation.from_messages(self.template.apply_chat_template(\n",
        "                sys_prompt, problem, local_tool.tool_config))\n",
        "            for _ in range(self.cfg.turns):\n",
        "                if stop_evt.is_set() or time.time() > deadline: break\n",
        "                prompt_ids = self.encoding.render_conversation_for_completion(conv, Role.ASSISTANT)\n",
        "                if (max_toks := self.cfg.context_tokens - len(prompt_ids)) < self.cfg.buffer_tokens: break\n",
        "                stream = self.client.completions.create(model=self.cfg.served_model_name,\n",
        "                    temperature=self.cfg.temperature, logprobs=self.cfg.top_logprobs, max_tokens=max_toks,\n",
        "                    prompt=prompt_ids, seed=seed, stream=True, extra_body={\n",
        "                        'min_p': self.cfg.min_p, 'stop_token_ids': self.stop_token_ids, 'return_token_ids': True})\n",
        "                try:\n",
        "                    tok_buf, txt_chunks = [], []\n",
        "                    for chunk in stream:\n",
        "                        if stop_evt.is_set() or time.time() > deadline: break\n",
        "                        if new_toks := chunk.choices[0].token_ids:\n",
        "                            tok_buf.extend(new_toks)\n",
        "                            total_toks += len(new_toks)\n",
        "                            txt_chunks.append(chunk.choices[0].text)\n",
        "                            if (clp := chunk.choices[0].logprobs) and clp.top_logprobs:\n",
        "                                logprobs.extend(clp.top_logprobs)\n",
        "                        if '}' in chunk.choices[0].text and (ans := self._scan_for_answer(\n",
        "                            ''.join(txt_chunks[-self.cfg.search_tokens:]))):\n",
        "                            break\n",
        "                finally:\n",
        "                    stream.close()\n",
        "                if ans or not tok_buf: break\n",
        "                new_msgs = self.encoding.parse_messages_from_completion_tokens(tok_buf, Role.ASSISTANT)\n",
        "                conv.messages.extend(new_msgs)\n",
        "                last = new_msgs[-1]\n",
        "                if last.channel == 'final':\n",
        "                    ans = self._scan_for_answer(last.content[0].text)\n",
        "                    break\n",
        "                if last.recipient == 'python':\n",
        "                    py_calls += 1\n",
        "                    resp = local_tool.process_sync_plus(last)\n",
        "                    if any(x in (txt := resp[0].content[0].text) for x in ['[ERROR]', 'Traceback', 'Error:']):\n",
        "                        py_errs += 1\n",
        "                    conv.messages.extend(resp)\n",
        "        except Exception: py_errs += 1\n",
        "        finally:\n",
        "            if sandbox:\n",
        "                sandbox.reset()\n",
        "                self.sandbox_pool.put(sandbox)\n",
        "        return {'Attempt': idx+1, 'Response Length': total_toks, 'Python Calls': py_calls,\n",
        "               'Python Errors': py_errs, 'Entropy': self._compute_mean_entropy(logprobs), 'Answer': ans}\n",
        "\n",
        "    def _select_answer(self, results):\n",
        "        ans_weights, ans_votes = defaultdict(float), defaultdict(int)\n",
        "        for r in results:\n",
        "            if (a := r['Answer']) is not None:\n",
        "                ans_weights[a] += 1.0/max(r['Entropy'], 1e-9)\n",
        "                ans_votes[a] += 1\n",
        "        scored = sorted([{'answer': a, 'votes': ans_votes[a], 'score': w}\n",
        "                        for a, w in ans_weights.items()], key=lambda x: x['score'], reverse=True)\n",
        "        display(pd.DataFrame([(s['answer'], s['votes'], s['score']) for s in scored],\n",
        "                            columns=['Answer', 'Votes', 'Score']).round({'Score': 3}))\n",
        "        final = scored[0]['answer'] if scored else 0\n",
        "        print(f'\\nFinal Answer: {final}\\n')\n",
        "        return final\n",
        "\n",
        "    def solve_problem(self, problem):\n",
        "        print(f'\\nProblem: {problem}\\n')\n",
        "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
        "        time_left = self.cfg.notebook_limit - (time.time() - self.notebook_start_time)\n",
        "        budget = max(self.cfg.base_problem_timeout,\n",
        "                    min(time_left - max(0, self.problems_remaining-1)*self.cfg.base_problem_timeout,\n",
        "                        self.cfg.high_problem_timeout))\n",
        "        deadline = time.time() + budget\n",
        "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
        "        results, valid, stop_evt = [], [], threading.Event()\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as ex:\n",
        "            futures = [ex.submit(self._process_attempt, user_input, self.cfg.system_prompt, i, stop_evt, deadline)\n",
        "                      for i in range(self.cfg.attempts)]\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    if (r := future.result())['Answer'] is not None:\n",
        "                        valid.append(r['Answer'])\n",
        "                    results.append(r)\n",
        "                    if (cnts := Counter(valid).most_common(1)) and cnts[0][1] >= self.cfg.early_stop:\n",
        "                        stop_evt.set()\n",
        "                        for f in futures: f.cancel()\n",
        "                        break\n",
        "                except Exception as exc:\n",
        "                    print(f'Future failed: {exc}')\n",
        "        self.problems_remaining = max(0, self.problems_remaining - 1)\n",
        "        if results:\n",
        "            df = pd.DataFrame(results)\n",
        "            df['Entropy'] = df['Entropy'].round(3)\n",
        "            df['Answer'] = df['Answer'].astype('Int64')\n",
        "            display(df)\n",
        "        return self._select_answer(results) if valid else 0\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'server_process'):\n",
        "            self.server_process.terminate()\n",
        "            self.server_process.wait()\n",
        "        if hasattr(self, 'log_file'): self.log_file.close()\n",
        "        if hasattr(self, 'sandbox_pool'):\n",
        "            while not self.sandbox_pool.empty():\n",
        "                with contextlib.suppress(Exception): self.sandbox_pool.get_nowait().close()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xrNtbnqXD8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "solver = AIMO3Solver(CFG)"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "DJHBw472D8R_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
        "    gc.disable()\n",
        "    final_answer = solver.solve_problem(question.item(0))\n",
        "    gc.enable()\n",
        "    gc.collect()\n",
        "    return pl.DataFrame({'id': id_.item(0), 'answer': final_answer})"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "x8stGB9CD8SA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "TXo4--o4D8SA"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}