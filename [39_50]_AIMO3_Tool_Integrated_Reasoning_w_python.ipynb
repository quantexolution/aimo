{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaH100",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 289055161,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 510391,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 404485,
          "modelId": 422384
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "[39/50] AIMO3: Tool Integrated Reasoning w/ python",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantexolution/aimo/blob/main/%5B39_50%5D_AIMO3_Tool_Integrated_Reasoning_w_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "l61oey08FJ7U"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "andreasbis_aimo_3_utils_path = kagglehub.notebook_output_download('andreasbis/aimo-3-utils')\n",
        "danielhanchen_gpt_oss_120b_transformers_default_1_path = kagglehub.model_download('danielhanchen/gpt-oss-120b/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Gfx1j2IbFJ7Z"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "fsqJDZ1-FJ7a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "id": "atJthLawFJ7a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess"
      ],
      "metadata": {
        "trusted": true,
        "id": "ibf2yqIHFJ7b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def set_env(input_archive, temp_dir):\n",
        "\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
        "\n",
        "    subprocess.run([\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'pip',\n",
        "        'install',\n",
        "        '--no-index',\n",
        "        '--find-links',\n",
        "        f'{temp_dir}/wheels',\n",
        "        'unsloth',\n",
        "        'trl',\n",
        "        'vllm',\n",
        "        'openai_harmony'\n",
        "    ], check=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lt77u30hFJ7b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_env(\n",
        "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz',\n",
        "    temp_dir='/kaggle/tmp/setup'\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "8bnnnH5_FJ7b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "QkxyXG8ZFJ7c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
        "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
        "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'"
      ],
      "metadata": {
        "trusted": true,
        "id": "adN58OcWFJ7c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import contextlib\n",
        "from typing import Optional\n",
        "from jupyter_client import KernelManager\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from openai_harmony import (\n",
        "    HarmonyEncodingName,\n",
        "    load_harmony_encoding,\n",
        "    SystemContent,\n",
        "    ReasoningEffort,\n",
        "    ToolNamespaceConfig,\n",
        "    Author,\n",
        "    Message,\n",
        "    Role,\n",
        "    TextContent,\n",
        "    Conversation\n",
        ")\n",
        "\n",
        "from transformers import set_seed\n",
        "import kaggle_evaluation.aimo_3_inference_server"
      ],
      "metadata": {
        "trusted": true,
        "id": "1F93NxzgFJ7d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # MULTI-AGENT CONDITION MINING (MACM) INSPIRED PROMPTS\n",
        "    thinker_prompt = (\n",
        "        'You are a THINKER agent specialized in mathematical reasoning. '\n",
        "        'Your role is to:\\n'\n",
        "        '1. Extract all CONDITIONS from the problem\\n'\n",
        "        '2. Identify the OBJECTIVE clearly\\n'\n",
        "        '3. Iteratively mine NEW CONDITIONS by combining existing ones\\n'\n",
        "        '4. Use the format: \"Based on Condition A and Condition B, we can derive: C\"\\n'\n",
        "        '5. Think step-by-step and explore multiple reasoning paths'\n",
        "    )\n",
        "\n",
        "    judge_prompt = (\n",
        "        'You are a JUDGE agent that validates mathematical reasoning. '\n",
        "        'Your role is to:\\n'\n",
        "        '1. Verify the correctness of each derived condition\\n'\n",
        "        '2. Check logical consistency between steps\\n'\n",
        "        '3. Identify any errors or unjustified assumptions\\n'\n",
        "        '4. Use Python to verify numerical claims when possible\\n'\n",
        "        '5. Only accept conditions that are rigorously proven'\n",
        "    )\n",
        "\n",
        "    executor_prompt = (\n",
        "        'You are an EXECUTOR agent that performs calculations. '\n",
        "        'Your role is to:\\n'\n",
        "        '1. Execute final computations once sufficient conditions are gathered\\n'\n",
        "        '2. Use symbolic computation (sympy) for exact results\\n'\n",
        "        '3. Verify answers through multiple independent methods\\n'\n",
        "        '4. Check edge cases and boundary conditions\\n'\n",
        "        '5. Present the final answer in \\\\boxed{N} format'\n",
        "    )\n",
        "\n",
        "    # SELF-VERIFICATION SYSTEM PROMPT\n",
        "    verifier_prompt = (\n",
        "        'You are a VERIFIER agent that critiques mathematical solutions. '\n",
        "        'Your role is to:\\n'\n",
        "        '1. Read the proposed solution carefully\\n'\n",
        "        '2. Identify potential errors, gaps, or logical flaws\\n'\n",
        "        '3. Check if all steps are justified\\n'\n",
        "        '4. Verify numerical computations independently\\n'\n",
        "        '5. Rate the solution quality: CORRECT, MINOR_ISSUES, or MAJOR_ERRORS'\n",
        "    )\n",
        "\n",
        "    # INTEGRATED SYSTEM PROMPT (combining MACM philosophy with self-verification)\n",
        "    system_prompt = (\n",
        "        'You are an elite IMO gold medalist with expertise in advanced mathematics. '\n",
        "        'You will solve problems using a structured multi-agent approach:\\n\\n'\n",
        "        'PHASE 1 - CONDITION MINING (THINKER):\\n'\n",
        "        '- Extract all given conditions and the objective\\n'\n",
        "        '- Iteratively derive new conditions from existing ones\\n'\n",
        "        '- Build a comprehensive condition list\\n\\n'\n",
        "        'PHASE 2 - VALIDATION (JUDGE):\\n'\n",
        "        '- Verify each step for logical correctness\\n'\n",
        "        '- Use Python/symbolic computation to check claims\\n'\n",
        "        '- Ensure rigorous reasoning throughout\\n\\n'\n",
        "        'PHASE 3 - EXECUTION (EXECUTOR):\\n'\n",
        "        '- Compute final answer using verified conditions\\n'\n",
        "        '- Use multiple independent methods for verification\\n'\n",
        "        '- Check edge cases\\n\\n'\n",
        "        'PHASE 4 - SELF-VERIFICATION (VERIFIER):\\n'\n",
        "        '- Critique your own solution\\n'\n",
        "        '- Identify and fix any errors\\n'\n",
        "        '- Confirm answer meets all requirements\\n\\n'\n",
        "        'REQUIREMENTS:\\n'\n",
        "        '1. Final answer must be a single integer 0-99999 in \\\\boxed{N}\\n'\n",
        "        '2. Show complete reasoning for all phases\\n'\n",
        "        '3. Use code to verify numerical claims\\n'\n",
        "        '4. Self-correct any identified issues'\n",
        "    )\n",
        "\n",
        "    # ENHANCED TOOL PROMPT with verification emphasis\n",
        "    tool_prompt = (\n",
        "        'Execute Python in a Jupyter environment. CRITICAL REQUIREMENTS:\\n'\n",
        "        '1. ALWAYS use print() to display results\\n'\n",
        "        '2. Use sympy for symbolic/exact computation\\n'\n",
        "        '3. Verify results with multiple methods when possible\\n'\n",
        "        '4. Check edge cases and special values\\n'\n",
        "        '5. Use high-precision arithmetic (Decimal, Fraction) when needed'\n",
        "    )\n",
        "\n",
        "    # PREFERENCE PROMPT with MACM and verification emphasis\n",
        "    preference_prompt = (\n",
        "        'SOLUTION METHODOLOGY:\\n'\n",
        "        '1. CONDITION EXTRACTION: List all given facts explicitly\\n'\n",
        "        '2. OBJECTIVE IDENTIFICATION: State what needs to be found\\n'\n",
        "        '3. CONDITION MINING: Iteratively combine conditions to derive new insights\\n'\n",
        "        '4. VALIDATION: Verify each step with code\\n'\n",
        "        '5. MULTI-METHOD SOLVING: Use 2-3 independent approaches\\n'\n",
        "        '6. SELF-VERIFICATION: Critique and refine your solution\\n'\n",
        "        '7. EDGE CASE TESTING: Test boundary conditions\\n\\n'\n",
        "        'Use sympy, math, numpy, fractions, and itertools as needed.'\n",
        "    )\n",
        "\n",
        "    served_model_name = 'gpt-oss'\n",
        "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
        "\n",
        "    kv_cache_dtype = 'fp8_e4m3'\n",
        "    dtype = 'auto'\n",
        "\n",
        "    high_problem_timeout = 900\n",
        "    base_problem_timeout = 300\n",
        "\n",
        "    notebook_limit = 17400\n",
        "    server_timeout = 180\n",
        "    session_timeout = 960\n",
        "    jupyter_timeout = 10\n",
        "    sandbox_timeout = 5\n",
        "\n",
        "    stream_interval = 200\n",
        "    context_tokens = 65536\n",
        "    search_tokens = 1024\n",
        "    buffer_tokens = 512\n",
        "    batch_size = 256\n",
        "\n",
        "    early_stop = 4\n",
        "    attempts = 8\n",
        "    workers = 16\n",
        "    turns = 128\n",
        "    seed = 42\n",
        "\n",
        "    gpu_memory_utilization = 0.96\n",
        "    temperature = 1.0\n",
        "    min_p = 0.05\n",
        "\n",
        "    # Quality thresholds\n",
        "    min_votes_for_confidence = 4\n",
        "    require_verification_consensus = True"
      ],
      "metadata": {
        "trusted": true,
        "id": "MJ7b5Jc7FJ7d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TdrAE4vUFJ7d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Template:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
        "        return (\n",
        "            SystemContent.new()\n",
        "            .with_model_identity(system_prompt)\n",
        "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
        "            .with_tools(tool_config)\n",
        "        )\n",
        "\n",
        "    def apply_chat_template(\n",
        "        self, system_prompt: str, user_prompt: str,\n",
        "        tool_config: ToolNamespaceConfig\n",
        "    ) -> list[Message]:\n",
        "        system_content = self.get_system_content(system_prompt, tool_config)\n",
        "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
        "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
        "        return [system_message, user_message]"
      ],
      "metadata": {
        "trusted": true,
        "id": "QsFNCa8IFJ7e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Sandbox:\n",
        "    _port_lock = threading.Lock()\n",
        "    _next_port = 50000\n",
        "\n",
        "    @classmethod\n",
        "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
        "        with cls._port_lock:\n",
        "            ports = list(range(cls._next_port, cls._next_port + count))\n",
        "            cls._next_port += count\n",
        "            return ports\n",
        "\n",
        "    def __init__(self, timeout: float):\n",
        "        self._default_timeout = timeout\n",
        "        self._owns_kernel = False\n",
        "        self._client = None\n",
        "        self._km = None\n",
        "\n",
        "        ports = self._get_next_ports(5)\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
        "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
        "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
        "        env['PYTHONWARNINGS'] = 'ignore'\n",
        "        env['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "        self._km = KernelManager()\n",
        "        self._km.shell_port = ports[0]\n",
        "        self._km.iopub_port = ports[1]\n",
        "        self._km.stdin_port = ports[2]\n",
        "        self._km.hb_port = ports[3]\n",
        "        self._km.control_port = ports[4]\n",
        "\n",
        "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
        "\n",
        "        self._client = self._km.blocking_client()\n",
        "        self._client.start_channels()\n",
        "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
        "        self._owns_kernel = True\n",
        "\n",
        "        # Enhanced imports for verification and symbolic reasoning\n",
        "        self.execute(\n",
        "            'import math\\n'\n",
        "            'import sympy\\n'\n",
        "            'from sympy import *\\n'\n",
        "            'import itertools\\n'\n",
        "            'import collections\\n'\n",
        "            'import numpy as np\\n'\n",
        "            'from fractions import Fraction\\n'\n",
        "            'from decimal import Decimal, getcontext\\n'\n",
        "            'getcontext().prec = 50\\n'\n",
        "            'import functools\\n'\n",
        "            'import operator\\n'\n",
        "        )\n",
        "\n",
        "    def _format_error(self, traceback: list[str]) -> str:\n",
        "        clean_lines = []\n",
        "        for frame in traceback:\n",
        "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
        "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
        "                continue\n",
        "            clean_lines.append(clean_frame)\n",
        "        return ''.join(clean_lines)\n",
        "\n",
        "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
        "        client = self._client\n",
        "        effective_timeout = timeout or self._default_timeout\n",
        "\n",
        "        msg_id = client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n",
        "\n",
        "        stdout_parts = []\n",
        "        stderr_parts = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        while True:\n",
        "            elapsed = time.time() - start_time\n",
        "            if elapsed > effective_timeout:\n",
        "                self._km.interrupt_kernel()\n",
        "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
        "\n",
        "            try:\n",
        "                msg = client.get_iopub_msg(timeout=1.0)\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
        "                continue\n",
        "\n",
        "            msg_type = msg.get('msg_type')\n",
        "            content = msg.get('content', {})\n",
        "\n",
        "            if msg_type == 'stream':\n",
        "                text = content.get('text', '')\n",
        "                if content.get('name') == 'stdout':\n",
        "                    stdout_parts.append(text)\n",
        "                else:\n",
        "                    stderr_parts.append(text)\n",
        "\n",
        "            elif msg_type == 'error':\n",
        "                traceback_list = content.get('traceback', [])\n",
        "                stderr_parts.append(self._format_error(traceback_list))\n",
        "\n",
        "            elif msg_type in {'execute_result', 'display_data'}:\n",
        "                data = content.get('data', {})\n",
        "                text = data.get('text/plain')\n",
        "                if text:\n",
        "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
        "\n",
        "            elif msg_type == 'status':\n",
        "                if content.get('execution_state') == 'idle':\n",
        "                    break\n",
        "\n",
        "        stdout = ''.join(stdout_parts)\n",
        "        stderr = ''.join(stderr_parts)\n",
        "\n",
        "        if stderr:\n",
        "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
        "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
        "\n",
        "    def close(self):\n",
        "        with contextlib.suppress(Exception):\n",
        "            if self._client:\n",
        "                self._client.stop_channels()\n",
        "\n",
        "        if self._owns_kernel and self._km is not None:\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.shutdown_kernel(now=True)\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.cleanup_resources()\n",
        "\n",
        "    def reset(self):\n",
        "        self.execute('%reset -f')\n",
        "        self.execute('import gc; gc.collect()')\n",
        "\n",
        "        self.execute(\n",
        "            'import math\\n'\n",
        "            'import sympy\\n'\n",
        "            'from sympy import *\\n'\n",
        "            'import itertools\\n'\n",
        "            'import collections\\n'\n",
        "            'import numpy as np\\n'\n",
        "            'from fractions import Fraction\\n'\n",
        "            'from decimal import Decimal, getcontext\\n'\n",
        "            'getcontext().prec = 50\\n'\n",
        "            'import functools\\n'\n",
        "            'import operator\\n'\n",
        "        )\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SCzBSRtlFJ7e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AIMO3Tool:\n",
        "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
        "        self._local_jupyter_timeout = local_jupyter_timeout\n",
        "        self._tool_prompt = tool_prompt\n",
        "        self._jupyter_session = sandbox\n",
        "        self._owns_session = sandbox is None\n",
        "        self._execution_lock = threading.Lock()\n",
        "        self._init_lock = threading.Lock()\n",
        "\n",
        "    def _ensure_session(self):\n",
        "        if self._jupyter_session is None:\n",
        "            with self._init_lock:\n",
        "                if self._jupyter_session is None:\n",
        "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
        "\n",
        "    def _ensure_last_print(self, code: str) -> str:\n",
        "        lines = code.strip().split('\\n')\n",
        "        if not lines:\n",
        "            return code\n",
        "\n",
        "        last_line = lines[-1].strip()\n",
        "\n",
        "        if any(keyword in last_line for keyword in ['print', 'import']) or not last_line or last_line.startswith('#'):\n",
        "            return code\n",
        "\n",
        "        lines[-1] = 'print(' + last_line + ')'\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    @property\n",
        "    def instruction(self) -> str:\n",
        "        return self._tool_prompt\n",
        "\n",
        "    @property\n",
        "    def tool_config(self) -> ToolNamespaceConfig:\n",
        "        return ToolNamespaceConfig(name='python', description=self.instruction, tools=[])\n",
        "\n",
        "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
        "        content = TextContent(text=output)\n",
        "        author = Author(role=Role.TOOL, name='python')\n",
        "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
        "        if channel:\n",
        "            message = message.with_channel(channel)\n",
        "        return message\n",
        "\n",
        "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
        "        self._ensure_session()\n",
        "        raw_script = message.content[0].text\n",
        "        final_script = self._ensure_last_print(raw_script)\n",
        "\n",
        "        with self._execution_lock:\n",
        "            try:\n",
        "                output = self._jupyter_session.execute(final_script)\n",
        "            except TimeoutError as exc:\n",
        "                output = f'[ERROR] {exc}'\n",
        "\n",
        "        return [self._make_response(output, channel=message.channel)]\n",
        "\n",
        "    def close(self):\n",
        "        if self._jupyter_session is not None:\n",
        "            if self._owns_session:\n",
        "                self._jupyter_session.close()\n",
        "            self._jupyter_session = None\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "id": "V3OlEnVfFJ7e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Solver:\n",
        "    def __init__(self, cfg, port: int = 8000):\n",
        "        self.cfg = cfg\n",
        "        self.port = port\n",
        "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
        "        self.api_key = 'sk-local'\n",
        "        self.template = AIMO3Template()\n",
        "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
        "\n",
        "        self._preload_model_weights()\n",
        "        self.server_process = self._start_server()\n",
        "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
        "\n",
        "        self._wait_for_server()\n",
        "        self._initialize_kernels()\n",
        "\n",
        "        self.notebook_start_time = time.time()\n",
        "        self.problems_remaining = 50\n",
        "\n",
        "    def _preload_model_weights(self) -> None:\n",
        "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        files_to_load = []\n",
        "        total_size = 0\n",
        "\n",
        "        for root, _, files in os.walk(self.cfg.model_path):\n",
        "            for file_name in files:\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    files_to_load.append(file_path)\n",
        "                    total_size += os.path.getsize(file_path)\n",
        "\n",
        "        def _read_file(path: str) -> None:\n",
        "            with open(path, 'rb') as file_object:\n",
        "                while file_object.read(1024 * 1024 * 1024):\n",
        "                    pass\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            list(executor.map(_read_file, files_to_load))\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
        "\n",
        "    def _start_server(self) -> subprocess.Popen:\n",
        "        cmd = [\n",
        "            sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
        "            '--seed', str(self.cfg.seed),\n",
        "            '--model', self.cfg.model_path,\n",
        "            '--served-model-name', self.cfg.served_model_name,\n",
        "            '--tensor-parallel-size', '1',\n",
        "            '--max-num-seqs', str(self.cfg.batch_size),\n",
        "            '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization),\n",
        "            '--host', '0.0.0.0',\n",
        "            '--port', str(self.port),\n",
        "            '--dtype', self.cfg.dtype,\n",
        "            '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
        "            '--max-model-len', str(self.cfg.context_tokens),\n",
        "            '--stream-interval', str(self.cfg.stream_interval),\n",
        "            '--async-scheduling',\n",
        "            '--enable-prefix-caching'\n",
        "        ]\n",
        "\n",
        "        self.log_file = open('vllm_server.log', 'w')\n",
        "        return subprocess.Popen(cmd, stdout=self.log_file, stderr=subprocess.STDOUT, start_new_session=True)\n",
        "\n",
        "    def _wait_for_server(self):\n",
        "        print('Waiting for vLLM server...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        for _ in range(self.cfg.server_timeout):\n",
        "            return_code = self.server_process.poll()\n",
        "            if return_code is not None:\n",
        "                self.log_file.flush()\n",
        "                with open('vllm_server.log', 'r') as log_file:\n",
        "                    logs = log_file.read()\n",
        "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
        "\n",
        "            try:\n",
        "                self.client.models.list()\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
        "                return\n",
        "            except Exception:\n",
        "                time.sleep(1)\n",
        "\n",
        "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
        "\n",
        "    def _initialize_kernels(self) -> None:\n",
        "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.sandbox_pool = queue.Queue()\n",
        "\n",
        "        def _create_sandbox():\n",
        "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
        "            for future in as_completed(futures):\n",
        "                self.sandbox_pool.put(future.result())\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
        "\n",
        "    def _scan_for_answer(self, text: str) -> int | None:\n",
        "        patterns = [\n",
        "            r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}',\n",
        "            r'\\\\boxed\\{([0-9,]+)\\}',\n",
        "            r'answer\\s*is\\s*\\\\boxed\\{([0-9,]+)\\}',\n",
        "            r'final\\s+answer:\\s*\\\\boxed\\{([0-9,]+)\\}',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                try:\n",
        "                    clean_value = matches[-1].replace(',', '').strip()\n",
        "                    value = int(clean_value)\n",
        "                    if 0 <= value <= 99999:\n",
        "                        return value\n",
        "                except ValueError:\n",
        "                    continue\n",
        "        return None\n",
        "\n",
        "    def _process_attempt(\n",
        "        self, problem: str, system_prompt: str, attempt_index: int,\n",
        "        stop_event: threading.Event, deadline: float\n",
        "    ) -> dict:\n",
        "        if stop_event.is_set() or time.time() > deadline:\n",
        "            return {\n",
        "                'Attempt': attempt_index + 1,\n",
        "                'Answer': None,\n",
        "                'Python Calls': 0,\n",
        "                'Python Errors': 0,\n",
        "                'Response Length': 0,\n",
        "                'Verification Score': 0\n",
        "            }\n",
        "\n",
        "        local_tool = None\n",
        "        sandbox = None\n",
        "        python_calls = 0\n",
        "        python_errors = 0\n",
        "        total_tokens = 0\n",
        "        final_answer = None\n",
        "        verification_score = 0\n",
        "\n",
        "        attempt_seed = self.cfg.seed * 1000 + attempt_index * 137\n",
        "\n",
        "        try:\n",
        "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
        "\n",
        "            local_tool = AIMO3Tool(\n",
        "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
        "                tool_prompt=self.cfg.tool_prompt,\n",
        "                sandbox=sandbox\n",
        "            )\n",
        "\n",
        "            encoding = self.encoding\n",
        "            messages = self.template.apply_chat_template(system_prompt, problem, local_tool.tool_config)\n",
        "\n",
        "            conversation = Conversation.from_messages(messages)\n",
        "\n",
        "            for turn_idx in range(self.cfg.turns):\n",
        "                if stop_event.is_set() or time.time() > deadline:\n",
        "                    break\n",
        "\n",
        "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
        "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
        "\n",
        "                if max_tokens < self.cfg.buffer_tokens:\n",
        "                    break\n",
        "\n",
        "                stream = self.client.completions.create(\n",
        "                    model=self.cfg.served_model_name,\n",
        "                    temperature=self.cfg.temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    prompt=prompt_ids,\n",
        "                    seed=attempt_seed,\n",
        "                    stream=True,\n",
        "                    extra_body={'min_p': self.cfg.min_p, 'stop_token_ids': self.stop_token_ids, 'return_token_ids': True}\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    token_buffer = []\n",
        "                    text_chunks = []\n",
        "\n",
        "                    for chunk in stream:\n",
        "                        if stop_event.is_set() or time.time() > deadline:\n",
        "                            break\n",
        "\n",
        "                        new_tokens = chunk.choices[0].token_ids\n",
        "                        new_text = chunk.choices[0].text\n",
        "\n",
        "                        if new_tokens:\n",
        "                            token_buffer.extend(new_tokens)\n",
        "                            total_tokens += len(new_tokens)\n",
        "                            text_chunks.append(new_text)\n",
        "\n",
        "                        if '}' in new_text or 'boxed' in new_text.lower():\n",
        "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
        "                            answer = self._scan_for_answer(search_text)\n",
        "                            if answer is not None:\n",
        "                                final_answer = answer\n",
        "                                # Award points for using verification keywords\n",
        "                                full_text = ''.join(text_chunks)\n",
        "                                if 'verif' in full_text.lower():\n",
        "                                    verification_score += 1\n",
        "                                if 'check' in full_text.lower():\n",
        "                                    verification_score += 1\n",
        "                                break\n",
        "\n",
        "                finally:\n",
        "                    stream.close()\n",
        "\n",
        "                if final_answer is not None:\n",
        "                    break\n",
        "\n",
        "                if not token_buffer:\n",
        "                    break\n",
        "\n",
        "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
        "                conversation.messages.extend(new_messages)\n",
        "                last_message = new_messages[-1]\n",
        "\n",
        "                if last_message.channel == 'final':\n",
        "                    answer_text = last_message.content[0].text\n",
        "                    final_answer = self._scan_for_answer(answer_text)\n",
        "                    break\n",
        "\n",
        "                if last_message.recipient == 'python':\n",
        "                    python_calls += 1\n",
        "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
        "                    response_text = tool_responses[0].content[0].text\n",
        "\n",
        "                    if any(err in response_text for err in ['[ERROR]', 'Traceback', 'Error:', 'Exception']):\n",
        "                        python_errors += 1\n",
        "                    else:\n",
        "                        verification_score += 1  # Successful code execution\n",
        "\n",
        "                    conversation.messages.extend(tool_responses)\n",
        "\n",
        "        except Exception as exc:\n",
        "            python_errors += 1\n",
        "\n",
        "        finally:\n",
        "            if local_tool is not None:\n",
        "                local_tool.close()\n",
        "            if sandbox is not None:\n",
        "                sandbox.reset()\n",
        "                self.sandbox_pool.put(sandbox)\n",
        "\n",
        "        return {\n",
        "            'Attempt': attempt_index + 1,\n",
        "            'Response Length': total_tokens,\n",
        "            'Python Calls': python_calls,\n",
        "            'Python Errors': python_errors,\n",
        "            'Answer': final_answer,\n",
        "            'Verification Score': verification_score\n",
        "        }\n",
        "\n",
        "    def _select_answer(self, detailed_results: list) -> int:\n",
        "        # Enhanced selection with verification scoring\n",
        "        stats = defaultdict(lambda: {\n",
        "            'votes': 0, 'calls': 0, 'tokens': 0, 'errors': 0, 'verification': 0\n",
        "        })\n",
        "\n",
        "        for result in detailed_results:\n",
        "            answer = result['Answer']\n",
        "            if answer is not None:\n",
        "                stats[answer]['votes'] += 1\n",
        "                stats[answer]['calls'] += result['Python Calls']\n",
        "                stats[answer]['tokens'] += result['Response Length']\n",
        "                stats[answer]['errors'] += result['Python Errors']\n",
        "                stats[answer]['verification'] += result['Verification Score']\n",
        "\n",
        "        sorted_stats = sorted(\n",
        "            stats.items(),\n",
        "            key=lambda item: (\n",
        "                item[1]['votes'],              # Primary: votes\n",
        "                item[1]['verification'],       # Secondary: verification quality\n",
        "                item[1]['calls'],              # Tertiary: Python usage\n",
        "                -item[1]['errors'],            # Quaternary: fewer errors\n",
        "                item[1]['tokens']              # Quinary: reasoning depth\n",
        "            ),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        vote_data = []\n",
        "        for answer, data in sorted_stats[:5]:\n",
        "            vote_data.append((\n",
        "                answer, data['votes'], data['calls'],\n",
        "                data['errors'], data['verification']\n",
        "            ))\n",
        "\n",
        "        vote_dataframe = pd.DataFrame(\n",
        "            vote_data,\n",
        "            columns=['Answer', 'Votes', 'Python Calls', 'Errors', 'Verification']\n",
        "        )\n",
        "        display(vote_dataframe)\n",
        "\n",
        "        final_answer = sorted_stats[0][0]\n",
        "        final_stats = sorted_stats[0][1]\n",
        "\n",
        "        print(f'\\nFinal Result: {final_answer} | Votes: {final_stats[\"votes\"]} | '\n",
        "              f'Verification: {final_stats[\"verification\"]} | '\n",
        "              f'Calls: {final_stats[\"calls\"]} | Errors: {final_stats[\"errors\"]}\\n')\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    def solve_problem(self, problem: str) -> int:\n",
        "        print(f'\\nProblem: {problem}\\n')\n",
        "\n",
        "        user_input = (\n",
        "            f'{problem}\\n\\n'\n",
        "            f'{self.cfg.preference_prompt}\\n\\n'\n",
        "            'Remember: Use the multi-agent approach (Thinker→Judge→Executor→Verifier) '\n",
        "            'and provide your final answer as \\\\boxed{{integer}}.'\n",
        "        )\n",
        "\n",
        "        elapsed_global = time.time() - self.notebook_start_time\n",
        "        time_left = self.cfg.notebook_limit - elapsed_global\n",
        "        problems_left_others = max(0, self.problems_remaining - 1)\n",
        "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
        "\n",
        "        budget = time_left - reserved_time\n",
        "        budget = min(budget, self.cfg.high_problem_timeout)\n",
        "        budget = max(budget, self.cfg.base_problem_timeout)\n",
        "\n",
        "        deadline = time.time() + budget\n",
        "\n",
        "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
        "\n",
        "        # Multi-agent approach: use different system prompts\n",
        "        agent_prompts = [\n",
        "            self.cfg.system_prompt,  # Integrated approach\n",
        "            self.cfg.thinker_prompt,\n",
        "            self.cfg.judge_prompt,\n",
        "            self.cfg.executor_prompt,\n",
        "            self.cfg.verifier_prompt,\n",
        "        ]\n",
        "\n",
        "        tasks = []\n",
        "        for attempt_index in range(self.cfg.attempts):\n",
        "            # Rotate through agent prompts for diversity\n",
        "            prompt = agent_prompts[attempt_index % len(agent_prompts)]\n",
        "            tasks.append((prompt, attempt_index))\n",
        "\n",
        "        detailed_results = []\n",
        "        valid_answers = []\n",
        "\n",
        "        stop_event = threading.Event()\n",
        "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
        "\n",
        "        try:\n",
        "            futures = []\n",
        "            for (system_prompt, attempt_index) in tasks:\n",
        "                future = executor.submit(\n",
        "                    self._process_attempt,\n",
        "                    user_input,\n",
        "                    system_prompt,\n",
        "                    attempt_index,\n",
        "                    stop_event,\n",
        "                    deadline\n",
        "                )\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    detailed_results.append(result)\n",
        "\n",
        "                    if result['Answer'] is not None:\n",
        "                        valid_answers.append(result['Answer'])\n",
        "\n",
        "                    counts = Counter(valid_answers).most_common(1)\n",
        "\n",
        "                    # Enhanced early stopping with verification\n",
        "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
        "                        # Check if we have verification consensus\n",
        "                        answer = counts[0][0]\n",
        "                        verified_count = sum(\n",
        "                            1 for r in detailed_results\n",
        "                            if r['Answer'] == answer and r['Verification Score'] > 0\n",
        "                        )\n",
        "\n",
        "                        if verified_count >= self.cfg.min_votes_for_confidence:\n",
        "                            stop_event.set()\n",
        "                            for f in futures:\n",
        "                                f.cancel()\n",
        "                            break\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f'Future failed: {exc}')\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            executor.shutdown(wait=False, cancel_futures=True)\n",
        "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
        "\n",
        "        if detailed_results:\n",
        "            results_dataframe = pd.DataFrame(detailed_results)\n",
        "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
        "            display(results_dataframe)\n",
        "\n",
        "        if not valid_answers:\n",
        "            print('\\nResult: 0 (No valid answers found)\\n')\n",
        "            return 0\n",
        "\n",
        "        return self._select_answer(detailed_results)\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'server_process'):\n",
        "            self.server_process.terminate()\n",
        "            self.server_process.wait()\n",
        "\n",
        "        if hasattr(self, 'log_file'):\n",
        "            self.log_file.close()\n",
        "\n",
        "        if hasattr(self, 'sandbox_pool'):\n",
        "            while not self.sandbox_pool.empty():\n",
        "                try:\n",
        "                    sb = self.sandbox_pool.get_nowait()\n",
        "                    sb.close()\n",
        "                except Exception:\n",
        "                    pass"
      ],
      "metadata": {
        "trusted": true,
        "id": "pQOxKBLPFJ7f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "solver = AIMO3Solver(CFG)"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "k8eERM1KFJ7g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
        "\n",
        "    id_value = id_.item(0)\n",
        "    question_text = question.item(0)\n",
        "\n",
        "    final_answer = solver.solve_problem(question_text)\n",
        "\n",
        "    return pl.DataFrame({'id': id_value, 'answer': final_answer})"
      ],
      "metadata": {
        "trusted": true,
        "id": "-gqGUjknFJ7g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "\n",
        "else:\n",
        "    inference_server.run_local_gateway(\n",
        "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "ncoNMVkpFJ7g"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}