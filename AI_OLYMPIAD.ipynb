{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaH100",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 289055161,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 510391,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 404485,
          "modelId": 422384
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "AI_OLYMPIAD",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantexolution/aimo/blob/main/AI_OLYMPIAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "UuTDuNyTDxBv"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "andreasbis_aimo_3_utils_path = kagglehub.notebook_output_download('andreasbis/aimo-3-utils')\n",
        "danielhanchen_gpt_oss_120b_transformers_default_1_path = kagglehub.model_download('danielhanchen/gpt-oss-120b/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "wYcFwxXuDxB0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:21:09.641594Z",
          "iopub.execute_input": "2026-01-26T06:21:09.641705Z",
          "iopub.status.idle": "2026-01-26T06:22:18.886569Z",
          "shell.execute_reply.started": "2026-01-26T06:21:09.64169Z",
          "shell.execute_reply": "2026-01-26T06:22:18.886097Z"
        },
        "id": "67A-ExL3DxB1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:22:18.88765Z",
          "iopub.execute_input": "2026-01-26T06:22:18.887818Z",
          "iopub.status.idle": "2026-01-26T06:22:18.890259Z",
          "shell.execute_reply.started": "2026-01-26T06:22:18.8878Z",
          "shell.execute_reply": "2026-01-26T06:22:18.889915Z"
        },
        "id": "VBPedL8uDxB2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:22:18.890745Z",
          "iopub.execute_input": "2026-01-26T06:22:18.890862Z",
          "iopub.status.idle": "2026-01-26T06:22:22.939785Z",
          "shell.execute_reply.started": "2026-01-26T06:22:18.890851Z",
          "shell.execute_reply": "2026-01-26T06:22:22.93934Z"
        },
        "id": "-xDShRTHDxB2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def set_env(input_archive, temp_dir):\n",
        "\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
        "\n",
        "    subprocess.run([\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'pip',\n",
        "        'install',\n",
        "        '--no-index',\n",
        "        '--find-links',\n",
        "        f'{temp_dir}/wheels',\n",
        "        'unsloth',\n",
        "        'trl',\n",
        "        'vllm',\n",
        "        'openai_harmony'\n",
        "    ], check=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:22:22.94201Z",
          "iopub.execute_input": "2026-01-26T06:22:22.94213Z",
          "iopub.status.idle": "2026-01-26T06:22:22.951877Z",
          "shell.execute_reply.started": "2026-01-26T06:22:22.942116Z",
          "shell.execute_reply": "2026-01-26T06:22:22.951498Z"
        },
        "id": "40YIz7PNDxB3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_env(\n",
        "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz',\n",
        "    temp_dir='/kaggle/tmp/setup'\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:27:49.114739Z",
          "iopub.execute_input": "2026-01-26T06:27:49.115059Z",
          "iopub.status.idle": "2026-01-26T06:31:56.886311Z",
          "shell.execute_reply.started": "2026-01-26T06:27:49.115044Z",
          "shell.execute_reply": "2026-01-26T06:31:56.885898Z"
        },
        "id": "vNMUZ51rDxB3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:31:56.887072Z",
          "iopub.execute_input": "2026-01-26T06:31:56.8872Z",
          "iopub.status.idle": "2026-01-26T06:31:56.901082Z",
          "shell.execute_reply.started": "2026-01-26T06:31:56.887188Z",
          "shell.execute_reply": "2026-01-26T06:31:56.899524Z"
        },
        "id": "4MmndkB5DxB3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
        "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
        "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:31:56.901723Z",
          "iopub.execute_input": "2026-01-26T06:31:56.901866Z",
          "iopub.status.idle": "2026-01-26T06:31:57.84856Z",
          "shell.execute_reply.started": "2026-01-26T06:31:56.901853Z",
          "shell.execute_reply": "2026-01-26T06:31:57.848153Z"
        },
        "id": "Et0h_Mh4DxB4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import contextlib\n",
        "from typing import Optional\n",
        "from jupyter_client import KernelManager\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from openai_harmony import (\n",
        "    HarmonyEncodingName,\n",
        "    load_harmony_encoding,\n",
        "    SystemContent,\n",
        "    ReasoningEffort,\n",
        "    ToolNamespaceConfig,\n",
        "    Author,\n",
        "    Message,\n",
        "    Role,\n",
        "    TextContent,\n",
        "    Conversation\n",
        ")\n",
        "\n",
        "from transformers import set_seed\n",
        "import kaggle_evaluation.aimo_3_inference_server"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:31:57.84943Z",
          "iopub.execute_input": "2026-01-26T06:31:57.849559Z",
          "iopub.status.idle": "2026-01-26T06:32:04.914773Z",
          "shell.execute_reply.started": "2026-01-26T06:31:57.849547Z",
          "shell.execute_reply": "2026-01-26T06:32:04.914337Z"
        },
        "id": "TXNpmlqbDxB4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "\n",
        "    system_prompt = (\n",
        "        'You are an elite mathematical problem solver with expertise at the International '\n",
        "        'Mathematical Olympiad (IMO) level. Your goal is to find the correct answer through '\n",
        "        'rigorous mathematical reasoning.\\n\\n'\n",
        "\n",
        "        '# Problem-Solving Approach:\\n'\n",
        "        '1. UNDERSTAND: Carefully read and rephrase the problem in your own words. '\n",
        "        'Identify what is given, what needs to be found, and any constraints.\\n'\n",
        "        '2. EXPLORE: Consider multiple solution strategies. Think about relevant theorems, '\n",
        "        'techniques, patterns, or analogous problems. Don\\'t commit to one approach immediately.\\n'\n",
        "        '3. PLAN: Select the most promising approach and outline key steps before executing.\\n'\n",
        "        '4. EXECUTE: Work through your solution methodically. Show all reasoning steps clearly.\\n'\n",
        "        '5. VERIFY: Check your answer by substituting back, testing edge cases, or using '\n",
        "        'alternative methods. Ensure logical consistency throughout.\\n\\n'\n",
        "        '# Mathematical Reasoning Principles:\\n'\n",
        "        '- Break complex problems into smaller, manageable sub-problems\\n'\n",
        "        '- Look for patterns, symmetries, and special cases that provide insight\\n'\n",
        "        '- Use concrete examples to build intuition before generalizing\\n'\n",
        "        '- Consider extreme cases and boundary conditions\\n'\n",
        "        '- If stuck, try working backwards from the desired result\\n'\n",
        "        '- Be willing to restart with a different approach if needed\\n\\n'\n",
        "\n",
        "        '# Verification Requirements:\\n'\n",
        "        '- Cross-check arithmetic and algebraic manipulations\\n'\n",
        "        '- Verify that your solution satisfies all problem constraints\\n'\n",
        "        '- Test your answer with simple cases or special values when possible\\n'\n",
        "        '- Ensure dimensional consistency and reasonableness of the result\\n\\n'\n",
        "\n",
        "        '# Output Format:\\n'\n",
        "        'The final answer must be a non-negative integer between 0 and 99999.\\n'\n",
        "        'Place your final numerical answer inside \\\\boxed{}, e.g., \\\\boxed{42}\\n\\n'\n",
        "\n",
        "        'Think step-by-step and show your complete reasoning process. Quality of reasoning '\n",
        "        'is as important as the final answer.'\n",
        "    )\n",
        "    tool_prompt = (\n",
        "        'Use this tool to execute Python code for:\\n'\n",
        "        '- Complex calculations that would be error-prone by hand\\n'\n",
        "        '- Numerical verification of analytical results\\n'\n",
        "        '- Generating examples or testing conjectures\\n'\n",
        "        '- Visualizing problem structure when helpful\\n'\n",
        "        '- Brute-force verification for small cases\\n\\n'\n",
        "\n",
        "        'The environment is a stateful Jupyter notebook. Code persists between executions.\\n'\n",
        "        'Always use print() to display results. Write clear, well-commented code.\\n\\n'\n",
        "\n",
        "        'Remember: Code should support your mathematical reasoning, not replace it. '\n",
        "        'Explain what you\\'re computing and why before running code.'\n",
        "    )\n",
        "    preference_prompt = (\n",
        "        'You have access to `math`, `numpy`, and `sympy` for:\\n\\n'\n",
        "\n",
        "        '# Symbolic Computation (sympy):\\n'\n",
        "        '- Algebraic manipulation and simplification\\n'\n",
        "        '- Solving equations and systems of equations\\n'\n",
        "        '- Symbolic differentiation and integration\\n'\n",
        "        '- Number theory functions (primes, divisors, modular arithmetic)\\n'\n",
        "        '- Polynomial operations and factorization\\n'\n",
        "        '- Working with mathematical expressions symbolically\\n\\n'\n",
        "\n",
        "        '# Numerical Computation (numpy):\\n'\n",
        "        '- Array operations and linear algebra\\n'\n",
        "        '- Efficient numerical calculations for large datasets\\n'\n",
        "        '- Matrix operations and eigenvalue problems\\n'\n",
        "        '- Statistical computations\\n\\n'\n",
        "        '# Mathematical Functions (math):\\n'\n",
        "        '- Standard mathematical functions (trig, log, exp)\\n'\n",
        "        '- Constants like pi and e\\n'\n",
        "        '- Basic operations for single values\\n\\n'\n",
        "\n",
        "        'Best Practices:\\n'\n",
        "        '- Use sympy for exact symbolic answers when possible\\n'\n",
        "        '- Use numpy for numerical verification and large-scale computation\\n'\n",
        "        '- Combine symbolic and numerical approaches: derive symbolically, verify numerically\\n'\n",
        "        '- Document your computational strategy clearly\\n'\n",
        "        '- Validate computational results against known cases or theoretical bounds'\n",
        "    )\n",
        "    served_model_name = 'gpt-oss'\n",
        "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
        "\n",
        "    kv_cache_dtype = 'fp8_e4m3'\n",
        "    dtype = 'auto'\n",
        "\n",
        "    high_problem_timeout = 900\n",
        "    base_problem_timeout = 300\n",
        "\n",
        "    notebook_limit = 17400\n",
        "    server_timeout = 180\n",
        "\n",
        "    session_timeout = 960\n",
        "    jupyter_timeout = 6\n",
        "    sandbox_timeout = 3\n",
        "\n",
        "    stream_interval = 200\n",
        "    context_tokens = 65536\n",
        "    buffer_tokens = 512\n",
        "    search_tokens = 32\n",
        "    top_logprobs = 5\n",
        "    batch_size = 256\n",
        "    early_stop = 4\n",
        "    attempts = 8\n",
        "    workers = 16\n",
        "    turns = 128\n",
        "    seed = 42\n",
        "\n",
        "    gpu_memory_utilization = 0.96\n",
        "    temperature = 1.0\n",
        "    min_p = 0.02"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:32:04.91534Z",
          "iopub.execute_input": "2026-01-26T06:32:04.91557Z",
          "iopub.status.idle": "2026-01-26T06:32:04.920188Z",
          "shell.execute_reply.started": "2026-01-26T06:32:04.915557Z",
          "shell.execute_reply": "2026-01-26T06:32:04.91979Z"
        },
        "id": "l7Myw0mKDxB5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:32:04.920705Z",
          "iopub.execute_input": "2026-01-26T06:32:04.920833Z",
          "iopub.status.idle": "2026-01-26T06:32:04.937154Z",
          "shell.execute_reply.started": "2026-01-26T06:32:04.920821Z",
          "shell.execute_reply": "2026-01-26T06:32:04.9368Z"
        },
        "id": "OABSyvhODxB5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Template:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
        "\n",
        "        return (\n",
        "            SystemContent.new()\n",
        "            .with_model_identity(system_prompt)\n",
        "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
        "            .with_tools(tool_config)\n",
        "        )\n",
        "\n",
        "    def apply_chat_template(\n",
        "        self,\n",
        "        system_prompt: str,\n",
        "        user_prompt: str,\n",
        "        tool_config: ToolNamespaceConfig\n",
        "    ) -> list[Message]:\n",
        "        system_content = self.get_system_content(system_prompt, tool_config)\n",
        "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
        "\n",
        "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
        "\n",
        "        return [system_message, user_message]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:32:39.633038Z",
          "iopub.execute_input": "2026-01-26T06:32:39.633489Z",
          "iopub.status.idle": "2026-01-26T06:32:39.637042Z",
          "shell.execute_reply.started": "2026-01-26T06:32:39.633473Z",
          "shell.execute_reply": "2026-01-26T06:32:39.63663Z"
        },
        "id": "yKy1k5C5DxB5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Sandbox:\n",
        "\n",
        "    _port_lock = threading.Lock()\n",
        "    _next_port = 50000\n",
        "\n",
        "    @classmethod\n",
        "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
        "\n",
        "        with cls._port_lock:\n",
        "            ports = list(range(cls._next_port, cls._next_port + count))\n",
        "            cls._next_port += count\n",
        "\n",
        "            return ports\n",
        "\n",
        "    def __init__(self, timeout: float):\n",
        "\n",
        "        self._default_timeout = timeout\n",
        "        self._owns_kernel = False\n",
        "        self._client = None\n",
        "        self._km = None\n",
        "        ports = self._get_next_ports(5)\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
        "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
        "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
        "        env['PYTHONWARNINGS'] = 'ignore'\n",
        "        env['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "        self._km = KernelManager()\n",
        "        self._km.shell_port = ports[0]\n",
        "        self._km.iopub_port = ports[1]\n",
        "        self._km.stdin_port = ports[2]\n",
        "        self._km.hb_port = ports[3]\n",
        "        self._km.control_port = ports[4]\n",
        "\n",
        "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
        "        self._client = self._km.blocking_client()\n",
        "        self._client.start_channels()\n",
        "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
        "        self._owns_kernel = True\n",
        "\n",
        "        self.execute(\n",
        "            'import math\\n'\n",
        "            'import numpy\\n'\n",
        "            'import sympy\\n'\n",
        "            'import itertools\\n'\n",
        "            'import collections\\n'\n",
        "            'import mpmath\\n'\n",
        "            'mpmath.mp.dps = 64\\n'\n",
        "        )\n",
        "    def _format_error(self, traceback: list[str]) -> str:\n",
        "\n",
        "        clean_lines = []\n",
        "\n",
        "        for frame in traceback:\n",
        "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
        "\n",
        "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
        "                continue\n",
        "\n",
        "            clean_lines.append(clean_frame)\n",
        "\n",
        "        return ''.join(clean_lines)\n",
        "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
        "\n",
        "        client = self._client\n",
        "        effective_timeout = timeout or self._default_timeout\n",
        "\n",
        "        msg_id = client.execute(\n",
        "            code,\n",
        "            store_history=True,\n",
        "            allow_stdin=False,\n",
        "            stop_on_error=False\n",
        "        )\n",
        "\n",
        "        stdout_parts = []\n",
        "        stderr_parts = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        while True:\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            if elapsed > effective_timeout:\n",
        "                self._km.interrupt_kernel()\n",
        "\n",
        "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
        "\n",
        "            try:\n",
        "                msg = client.get_iopub_msg(timeout=1.0)\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
        "                continue\n",
        "\n",
        "            msg_type = msg.get('msg_type')\n",
        "            content = msg.get('content', {})\n",
        "\n",
        "            if msg_type == 'stream':\n",
        "                text = content.get('text', '')\n",
        "\n",
        "                if content.get('name') == 'stdout':\n",
        "                    stdout_parts.append(text)\n",
        "\n",
        "                else:\n",
        "                    stderr_parts.append(text)\n",
        "\n",
        "            elif msg_type == 'error':\n",
        "                traceback_list = content.get('traceback', [])\n",
        "\n",
        "                stderr_parts.append(self._format_error(traceback_list))\n",
        "\n",
        "            elif msg_type in {'execute_result', 'display_data'}:\n",
        "                data = content.get('data', {})\n",
        "                text = data.get('text/plain')\n",
        "\n",
        "                if text:\n",
        "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
        "\n",
        "            elif msg_type == 'status':\n",
        "               if content.get('execution_state') == 'idle':\n",
        "                    break\n",
        "\n",
        "        stdout = ''.join(stdout_parts)\n",
        "        stderr = ''.join(stderr_parts)\n",
        "\n",
        "        if stderr:\n",
        "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
        "\n",
        "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
        "\n",
        "    def close(self):\n",
        "\n",
        "        with contextlib.suppress(Exception):\n",
        "            if self._client:\n",
        "                self._client.stop_channels()\n",
        "\n",
        "        if self._owns_kernel and self._km is not None:\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.shutdown_kernel(now=True)\n",
        "\n",
        "            with contextlib.suppress(Exception):\n",
        "               self._km.cleanup_resources()\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.execute(\n",
        "            '%reset -f\\n'\n",
        "            'import math\\n'\n",
        "            'import numpy\\n'\n",
        "            'import sympy\\n'\n",
        "            'import itertools\\n'\n",
        "            'import collections\\n'\n",
        "            'import mpmath\\n'\n",
        "            'mpmath.mp.dps = 64\\n'\n",
        "        )\n",
        "    def __del__(self):\n",
        "\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:34:31.107475Z",
          "iopub.execute_input": "2026-01-26T06:34:31.10806Z",
          "iopub.status.idle": "2026-01-26T06:34:31.117753Z",
          "shell.execute_reply.started": "2026-01-26T06:34:31.108042Z",
          "shell.execute_reply": "2026-01-26T06:34:31.11737Z"
        },
        "id": "Dlr1b5qzDxB5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Tool:\n",
        "\n",
        "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
        "\n",
        "        self._local_jupyter_timeout = local_jupyter_timeout\n",
        "        self._tool_prompt = tool_prompt\n",
        "        self._jupyter_session = sandbox\n",
        "\n",
        "        self._owns_session = sandbox is None\n",
        "\n",
        "        self._execution_lock = threading.Lock()\n",
        "        self._init_lock = threading.Lock()\n",
        "\n",
        "    def _ensure_session(self):\n",
        "\n",
        "        if self._jupyter_session is None:\n",
        "            with self._init_lock:\n",
        "                if self._jupyter_session is None:\n",
        "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
        "    def _ensure_last_print(self, code: str) -> str:\n",
        "\n",
        "        lines = code.strip().split('\\n')\n",
        "\n",
        "        if not lines:\n",
        "            return code\n",
        "\n",
        "        last_line = lines[-1].strip()\n",
        "\n",
        "        if 'print' in last_line or 'import' in last_line:\n",
        "            return code\n",
        "\n",
        "        if not last_line:\n",
        "            return code\n",
        "\n",
        "        if last_line.startswith('#'):\n",
        "            return code\n",
        "\n",
        "        lines[-1] = 'print(' + last_line + ')'\n",
        "\n",
        "        return '\\n'.join(lines)\n",
        "    @property\n",
        "    def instruction(self) -> str:\n",
        "\n",
        "        return self._tool_prompt\n",
        "\n",
        "    @property\n",
        "    def tool_config(self) -> ToolNamespaceConfig:\n",
        "\n",
        "        return ToolNamespaceConfig(\n",
        "            name='python',\n",
        "            description=self.instruction,\n",
        "            tools=[]\n",
        "        )\n",
        "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
        "\n",
        "        content = TextContent(text=output)\n",
        "        author = Author(role=Role.TOOL, name='python')\n",
        "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
        "\n",
        "        if channel:\n",
        "            message = message.with_channel(channel)\n",
        "\n",
        "        return message\n",
        "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
        "\n",
        "        self._ensure_session()\n",
        "        raw_script = message.content[0].text\n",
        "        final_script = self._ensure_last_print(raw_script)\n",
        "\n",
        "        with self._execution_lock:\n",
        "            try:\n",
        "                output = self._jupyter_session.execute(final_script)\n",
        "\n",
        "            except TimeoutError as exc:\n",
        "                output = f'[ERROR] {exc}'\n",
        "\n",
        "        return [self._make_response(output, channel=message.channel)]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:35:55.673299Z",
          "iopub.execute_input": "2026-01-26T06:35:55.673696Z",
          "iopub.status.idle": "2026-01-26T06:35:55.679684Z",
          "shell.execute_reply.started": "2026-01-26T06:35:55.673673Z",
          "shell.execute_reply": "2026-01-26T06:35:55.679289Z"
        },
        "id": "g4aqg3XsDxB6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Solver:\n",
        "\n",
        "    def __init__(self, cfg, port: int = 8000):\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.port = port\n",
        "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
        "        self.api_key = 'sk-local'\n",
        "        self.template = AIMO3Template()\n",
        "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
        "\n",
        "        self._preload_model_weights()\n",
        "\n",
        "        self.server_process = self._start_server()\n",
        "\n",
        "        self.client = OpenAI(\n",
        "            base_url=self.base_url,\n",
        "            api_key=self.api_key,\n",
        "            timeout=self.cfg.session_timeout\n",
        "        )\n",
        "\n",
        "        self._wait_for_server()\n",
        "        self._initialize_kernels()\n",
        "\n",
        "        self.notebook_start_time = time.time()\n",
        "        self.problems_remaining = 50\n",
        "    def _preload_model_weights(self) -> None:\n",
        "\n",
        "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        files_to_load = []\n",
        "        total_size = 0\n",
        "\n",
        "        for root, _, files in os.walk(self.cfg.model_path):\n",
        "            for file_name in files:\n",
        "                file_path = os.path.join(root, file_name)\n",
        "\n",
        "                if os.path.isfile(file_path):\n",
        "                    files_to_load.append(file_path)\n",
        "                    total_size += os.path.getsize(file_path)\n",
        "        def _read_file(path: str) -> None:\n",
        "\n",
        "            with open(path, 'rb') as file_object:\n",
        "                while file_object.read(1024 * 1024 * 1024):\n",
        "                    pass\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            list(executor.map(_read_file, files_to_load))\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
        "    def _start_server(self) -> subprocess.Popen:\n",
        "\n",
        "        cmd = [\n",
        "            sys.executable,\n",
        "            '-m',\n",
        "            'vllm.entrypoints.openai.api_server',\n",
        "            '--seed',\n",
        "            str(self.cfg.seed),\n",
        "            '--model',\n",
        "            self.cfg.model_path,\n",
        "            '--served-model-name',\n",
        "            self.cfg.served_model_name,\n",
        "            '--tensor-parallel-size',\n",
        "            '1',\n",
        "            '--max-num-seqs',\n",
        "            str(self.cfg.batch_size),\n",
        "            '--gpu-memory-utilization',\n",
        "            str(self.cfg.gpu_memory_utilization),\n",
        "            '--host',\n",
        "            '0.0.0.0',\n",
        "            '--port',\n",
        "            str(self.port),\n",
        "            '--dtype',\n",
        "            self.cfg.dtype,\n",
        "            '--kv-cache-dtype',\n",
        "            self.cfg.kv_cache_dtype,\n",
        "            '--max-model-len',\n",
        "            str(self.cfg.context_tokens),\n",
        "            '--stream-interval',\n",
        "            str(self.cfg.stream_interval),\n",
        "            '--async-scheduling',\n",
        "            '--disable-log-stats',\n",
        "            '--enable-prefix-caching'\n",
        "        ]\n",
        "\n",
        "        self.log_file = open('vllm_server.log', 'w')\n",
        "\n",
        "        return subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=self.log_file,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            start_new_session=True\n",
        "        )\n",
        "    def _wait_for_server(self):\n",
        "\n",
        "        print('Waiting for vLLM server...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        for _ in range(self.cfg.server_timeout):\n",
        "            return_code = self.server_process.poll()\n",
        "\n",
        "            if return_code is not None:\n",
        "                self.log_file.flush()\n",
        "\n",
        "                with open('vllm_server.log', 'r') as log_file:\n",
        "                    logs = log_file.read()\n",
        "\n",
        "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
        "            try:\n",
        "                self.client.models.list()\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
        "\n",
        "                return\n",
        "\n",
        "            except Exception:\n",
        "                time.sleep(1)\n",
        "\n",
        "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
        "    def _initialize_kernels(self) -> None:\n",
        "\n",
        "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.sandbox_pool = queue.Queue()\n",
        "\n",
        "        def _create_sandbox():\n",
        "\n",
        "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                self.sandbox_pool.put(future.result())\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
        "    def _scan_for_answer(self, text: str) -> int | None:\n",
        "\n",
        "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        if matches:\n",
        "            try:\n",
        "                clean_value = matches[-1].replace(',', '')\n",
        "                value = int(clean_value)\n",
        "\n",
        "                if 0 <= value <= 99999:\n",
        "                    return value\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "\n",
        "        if matches:\n",
        "            try:\n",
        "                clean_value = matches[-1].replace(',', '')\n",
        "                value = int(clean_value)\n",
        "\n",
        "                if 0 <= value <= 99999:\n",
        "                    return value\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        return None\n",
        "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
        "\n",
        "        if not logprobs_buffer:\n",
        "            return float('inf')\n",
        "\n",
        "        total_entropy = 0.0\n",
        "        token_count = 0\n",
        "\n",
        "        for top_logprobs_dict in logprobs_buffer:\n",
        "\n",
        "            if not isinstance(top_logprobs_dict, dict):\n",
        "                continue\n",
        "\n",
        "            if not top_logprobs_dict:\n",
        "                continue\n",
        "\n",
        "            token_entropy = 0.0\n",
        "\n",
        "            for token_str, log_prob in top_logprobs_dict.items():\n",
        "                prob = math.exp(log_prob)\n",
        "\n",
        "                if prob > 0:\n",
        "                    token_entropy -= prob * math.log2(prob)\n",
        "\n",
        "            total_entropy += token_entropy\n",
        "            token_count += 1\n",
        "\n",
        "        if token_count == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        return total_entropy / token_count\n",
        "    def _process_attempt(\n",
        "        self,\n",
        "        problem: str,\n",
        "        system_prompt: str,\n",
        "        attempt_index: int,\n",
        "        stop_event: threading.Event,\n",
        "        deadline: float\n",
        "    ) -> dict:\n",
        "\n",
        "        if stop_event.is_set() or time.time() > deadline:\n",
        "            return {\n",
        "                'Attempt': attempt_index + 1,\n",
        "                'Answer': None,\n",
        "                'Python Calls': 0,\n",
        "                'Python Errors': 0,\n",
        "                'Response Length': 0,\n",
        "                'Entropy': float('inf')\n",
        "            }\n",
        "        local_tool = None\n",
        "        sandbox = None\n",
        "        python_calls = 0\n",
        "        python_errors = 0\n",
        "        total_tokens = 0\n",
        "        final_answer = None\n",
        "\n",
        "        logprobs_buffer = []\n",
        "\n",
        "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
        "        try:\n",
        "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
        "\n",
        "            local_tool = AIMO3Tool(\n",
        "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
        "                tool_prompt=self.cfg.tool_prompt,\n",
        "                sandbox=sandbox\n",
        "            )\n",
        "\n",
        "            encoding = self.encoding\n",
        "            messages = self.template.apply_chat_template(\n",
        "                system_prompt,\n",
        "                problem,\n",
        "                local_tool.tool_config\n",
        "            )\n",
        "\n",
        "            conversation = Conversation.from_messages(messages)\n",
        "            for _ in range(self.cfg.turns):\n",
        "                if stop_event.is_set() or time.time() > deadline:\n",
        "                    break\n",
        "\n",
        "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
        "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
        "\n",
        "                if max_tokens < self.cfg.buffer_tokens:\n",
        "                    break\n",
        "\n",
        "                stream = self.client.completions.create(\n",
        "                    model=self.cfg.served_model_name,\n",
        "                    temperature=self.cfg.temperature,\n",
        "                    logprobs=self.cfg.top_logprobs,\n",
        "                    max_tokens=max_tokens,\n",
        "                    prompt=prompt_ids,\n",
        "                    seed=attempt_seed,\n",
        "                    stream=True,\n",
        "                    extra_body={\n",
        "                        'min_p': self.cfg.min_p,\n",
        "                        'stop_token_ids': self.stop_token_ids,\n",
        "                        'return_token_ids': True\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    token_buffer = []\n",
        "                    text_chunks = []\n",
        "\n",
        "                    for chunk in stream:\n",
        "                        if stop_event.is_set() or time.time() > deadline:\n",
        "                            break\n",
        "\n",
        "                        new_tokens = chunk.choices[0].token_ids\n",
        "                        new_text = chunk.choices[0].text\n",
        "\n",
        "                        if new_tokens:\n",
        "                            token_buffer.extend(new_tokens)\n",
        "                            total_tokens += len(new_tokens)\n",
        "                            text_chunks.append(new_text)\n",
        "                            chunk_logprobs = chunk.choices[0].logprobs\n",
        "\n",
        "                            if chunk_logprobs is not None:\n",
        "                                if chunk_logprobs.top_logprobs:\n",
        "                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n",
        "\n",
        "                        if '}' in new_text:\n",
        "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
        "                            answer = self._scan_for_answer(search_text)\n",
        "\n",
        "                            if answer is not None:\n",
        "                                final_answer = answer\n",
        "                                break\n",
        "\n",
        "                finally:\n",
        "                    stream.close()\n",
        "\n",
        "                if final_answer is not None:\n",
        "                    break\n",
        "\n",
        "                if not token_buffer:\n",
        "                    break\n",
        "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
        "                conversation.messages.extend(new_messages)\n",
        "                last_message = new_messages[-1]\n",
        "\n",
        "                if last_message.channel == 'final':\n",
        "                    answer_text = last_message.content[0].text\n",
        "                    final_answer = self._scan_for_answer(answer_text)\n",
        "                    break\n",
        "\n",
        "                if last_message.recipient == 'python':\n",
        "                    python_calls += 1\n",
        "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
        "\n",
        "                    response_text = tool_responses[0].content[0].text\n",
        "\n",
        "                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n",
        "                        python_errors += 1\n",
        "\n",
        "                    conversation.messages.extend(tool_responses)\n",
        "        except Exception as exc:\n",
        "            python_errors += 1\n",
        "\n",
        "        finally:\n",
        "            if sandbox is not None:\n",
        "                sandbox.reset()\n",
        "                self.sandbox_pool.put(sandbox)\n",
        "\n",
        "        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n",
        "\n",
        "        return {\n",
        "            'Attempt': attempt_index + 1,\n",
        "            'Response Length': total_tokens,\n",
        "            'Python Calls': python_calls,\n",
        "            'Python Errors': python_errors,\n",
        "            'Entropy': mean_entropy,\n",
        "            'Answer': final_answer\n",
        "        }\n",
        "    def _select_answer(self, detailed_results: list) -> int:\n",
        "\n",
        "        answer_weights = defaultdict(float)\n",
        "        answer_votes = defaultdict(int)\n",
        "\n",
        "        for result in detailed_results:\n",
        "            answer = result['Answer']\n",
        "            entropy = result['Entropy']\n",
        "\n",
        "            if answer is not None:\n",
        "                weight = 1.0 / max(entropy, 1e-9)\n",
        "\n",
        "                answer_weights[answer] += weight\n",
        "                answer_votes[answer] += 1\n",
        "\n",
        "        scored_answers = []\n",
        "        for answer, total_weight in answer_weights.items():\n",
        "            scored_answers.append({\n",
        "                'answer': answer,\n",
        "                'votes': answer_votes[answer],\n",
        "                'score': total_weight\n",
        "            })\n",
        "\n",
        "        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        vote_data = []\n",
        "\n",
        "        for item in scored_answers:\n",
        "            vote_data.append((\n",
        "                item['answer'],\n",
        "                item['votes'],\n",
        "                item['score']\n",
        "            ))\n",
        "        vote_dataframe = pd.DataFrame(\n",
        "            vote_data,\n",
        "            columns=['Answer', 'Votes', 'Score']\n",
        "        )\n",
        "\n",
        "        vote_dataframe = vote_dataframe.round({'Score': 3})\n",
        "        display(vote_dataframe)\n",
        "\n",
        "        if not scored_answers:\n",
        "            print('\\nFinal Answer: 0\\n')\n",
        "            return 0\n",
        "\n",
        "        final_answer = scored_answers[0]['answer']\n",
        "        print(f'\\nFinal Answer: {final_answer}\\n')\n",
        "\n",
        "        return final_answer\n",
        "    def solve_problem(self, problem: str) -> int:\n",
        "\n",
        "        print(f'\\nProblem: {problem}\\n')\n",
        "\n",
        "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
        "\n",
        "        elapsed_global = time.time() - self.notebook_start_time\n",
        "        time_left = self.cfg.notebook_limit - elapsed_global\n",
        "        problems_left_others = max(0, self.problems_remaining - 1)\n",
        "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
        "\n",
        "        budget = time_left - reserved_time\n",
        "        budget = min(budget, self.cfg.high_problem_timeout)\n",
        "        budget = max(budget, self.cfg.base_problem_timeout)\n",
        "\n",
        "        deadline = time.time() + budget\n",
        "\n",
        "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
        "\n",
        "        tasks = []\n",
        "        for attempt_index in range(self.cfg.attempts):\n",
        "            tasks.append((self.cfg.system_prompt, attempt_index))\n",
        "\n",
        "        detailed_results = []\n",
        "        valid_answers = []\n",
        "\n",
        "        stop_event = threading.Event()\n",
        "\n",
        "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
        "\n",
        "        try:\n",
        "            futures = []\n",
        "\n",
        "            for (system_prompt, attempt_index) in tasks:\n",
        "                future = executor.submit(\n",
        "                    self._process_attempt,\n",
        "                    user_input,\n",
        "                    system_prompt,\n",
        "                    attempt_index,\n",
        "                    stop_event,\n",
        "                    deadline\n",
        "                )\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    detailed_results.append(result)\n",
        "\n",
        "                    if result['Answer'] is not None:\n",
        "                        valid_answers.append(result['Answer'])\n",
        "\n",
        "                    counts = Counter(valid_answers).most_common(1)\n",
        "\n",
        "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
        "                        stop_event.set()\n",
        "\n",
        "                        for f in futures:\n",
        "                            f.cancel()\n",
        "\n",
        "                        break\n",
        "                except Exception as exc:\n",
        "                    print(f'Future failed: {exc}')\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            stop_event.set()\n",
        "            executor.shutdown(wait=True, cancel_futures=True)\n",
        "\n",
        "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
        "\n",
        "        if detailed_results:\n",
        "            results_dataframe = pd.DataFrame(detailed_results)\n",
        "            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n",
        "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
        "\n",
        "            display(results_dataframe)\n",
        "\n",
        "        if not valid_answers:\n",
        "            print('\\nResult: 0\\n')\n",
        "\n",
        "            return 0\n",
        "        return self._select_answer(detailed_results)\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        if hasattr(self, 'server_process'):\n",
        "            self.server_process.terminate()\n",
        "            self.server_process.wait()\n",
        "\n",
        "        if hasattr(self, 'log_file'):\n",
        "            self.log_file.close()\n",
        "\n",
        "        if hasattr(self, 'sandbox_pool'):\n",
        "            while not self.sandbox_pool.empty():\n",
        "                try:\n",
        "                    sb = self.sandbox_pool.get_nowait()\n",
        "                    sb.close()\n",
        "\n",
        "                except Exception:\n",
        "                    pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:43:29.060276Z",
          "iopub.execute_input": "2026-01-26T06:43:29.060887Z",
          "iopub.status.idle": "2026-01-26T06:43:29.084476Z",
          "shell.execute_reply.started": "2026-01-26T06:43:29.060871Z",
          "shell.execute_reply": "2026-01-26T06:43:29.084082Z"
        },
        "id": "0DtPqRZ7DxB7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "solver = AIMO3Solver(CFG)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:43:39.24291Z",
          "iopub.execute_input": "2026-01-26T06:43:39.243504Z",
          "iopub.status.idle": "2026-01-26T06:47:19.250339Z",
          "shell.execute_reply.started": "2026-01-26T06:43:39.243487Z",
          "shell.execute_reply": "2026-01-26T06:47:19.249897Z"
        },
        "id": "mlN5VJa1DxB8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
        "\n",
        "    id_value = id_.item(0)\n",
        "    question_text = question.item(0)\n",
        "\n",
        "    gc.disable()\n",
        "\n",
        "    final_answer = solver.solve_problem(question_text)\n",
        "\n",
        "    gc.enable()\n",
        "    gc.collect()\n",
        "\n",
        "    return pl.DataFrame({'id': id_value, 'answer': final_answer})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:47:19.251138Z",
          "iopub.execute_input": "2026-01-26T06:47:19.251292Z",
          "iopub.status.idle": "2026-01-26T06:47:19.25441Z",
          "shell.execute_reply.started": "2026-01-26T06:47:19.251278Z",
          "shell.execute_reply": "2026-01-26T06:47:19.254006Z"
        },
        "id": "Mc3kYld4DxB8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "\n",
        "else:\n",
        "    inference_server.run_local_gateway(\n",
        "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T06:47:19.254941Z",
          "iopub.execute_input": "2026-01-26T06:47:19.255086Z",
          "iopub.status.idle": "2026-01-26T06:47:40.937315Z",
          "shell.execute_reply.started": "2026-01-26T06:47:19.255073Z",
          "shell.execute_reply": "2026-01-26T06:47:40.936859Z"
        },
        "id": "yYHapxnTDxB8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "dzzHrajcDxB8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}