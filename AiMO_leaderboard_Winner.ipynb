{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "kaggle": {
      "accelerator": "nvidiaH100",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 281315401,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 499291,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 396608,
          "modelId": 322000
        },
        {
          "sourceId": 510391,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 404485,
          "modelId": 422384
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "AiMO leaderboard Winner ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantexolution/aimo/blob/main/AiMO_leaderboard_Winner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "hbtxkiWEElfP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "huikang_pip_install_aimo3_1_path = kagglehub.utility_script_install('huikang/pip-install-aimo3-1')\n",
        "qwen_lm_qwen_3_transformers_30b_a3b_thinking_2507_fp8_1_path = kagglehub.model_download('qwen-lm/qwen-3/Transformers/30b-a3b-thinking-2507-fp8/1')\n",
        "danielhanchen_gpt_oss_120b_transformers_default_1_path = kagglehub.model_download('danielhanchen/gpt-oss-120b/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "b8kYXhGWElfQ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "start_time = time.time()\n",
        "final_cutoff_time = start_time + (4 * 60 + 58) * 60  # 4h 55m\n",
        "\n",
        "TOTAL_TIME = 4 * 60 * 60 + 58 * 60  # 4h 55m\n",
        "NUM_QUESTIONS = 50\n",
        "BUFFER_TIME = 60"
      ],
      "metadata": {
        "id": "RVy4m8T8ElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "uninstall_proc = subprocess.Popen(\n",
        "    [\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")"
      ],
      "metadata": {
        "id": "Jk6jWBxPElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null"
      ],
      "metadata": {
        "id": "U-Gi3UDkElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def cache_model(path, exts=(\".bin\", \".pt\", \".safetensors\"), num_workers=None, chunk_mb=256):\n",
        "    \"\"\"Pre-read model weight files into OS page cache.\"\"\"\n",
        "    import os\n",
        "    import multiprocessing\n",
        "    import time\n",
        "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "    def warmup_file(fpath):\n",
        "        chunk_size = chunk_mb * 1024 * 1024\n",
        "        total = 0\n",
        "        with open(fpath, \"rb\") as f:\n",
        "            while True:\n",
        "                data = f.read(chunk_size)\n",
        "                if not data:\n",
        "                    break\n",
        "                total += len(data)\n",
        "        return fpath, total\n",
        "\n",
        "    if os.path.isdir(path):\n",
        "        files = [\n",
        "            os.path.join(root, name)\n",
        "            for root, _, names in os.walk(path)\n",
        "            for name in names\n",
        "            if name.endswith(exts)\n",
        "        ]\n",
        "        files.sort()\n",
        "    else:\n",
        "        files = [path]\n",
        "\n",
        "    if not files:\n",
        "        raise ValueError(f\"No model files found under: {path}\")\n",
        "\n",
        "    if num_workers is None:\n",
        "        try:\n",
        "            num_workers = min(multiprocessing.cpu_count(), 8)\n",
        "        except Exception:\n",
        "            num_workers = 4\n",
        "\n",
        "    print(f\"[cache_model] {len(files)} file(s), {num_workers} worker(s)\")\n",
        "    t0 = time.time()\n",
        "    total_bytes = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as pool:\n",
        "        futures = {pool.submit(warmup_file, f): f for f in files}\n",
        "        for i, fut in enumerate(as_completed(futures), 1):\n",
        "            fpath, n = fut.result()\n",
        "            total_bytes += n\n",
        "            print(f\"[{i}/{len(files)}] cached {os.path.basename(fpath)}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    gb = total_bytes / 1024**3\n",
        "    print(f\"[cache_model] total read ‚âà {gb:.2f} GB in {elapsed:.2f}s\")\n",
        "    return total_bytes\n",
        "\n",
        "\n",
        "cache_model(\"/kaggle/input/gpt-oss-120b/transformers/default/1\", num_workers=16, chunk_mb=1024)"
      ],
      "metadata": {
        "id": "stBWA4yuElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Copy vLLM compile cache if available\n",
        "import os\n",
        "if os.path.exists(\"/kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache\"):\n",
        "    !mkdir -p /root/.cache/vllm/\n",
        "    !cp -r /kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache /root/.cache/vllm/"
      ],
      "metadata": {
        "id": "BeZX832WElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "uninstall_proc.wait()"
      ],
      "metadata": {
        "id": "2l5r24k8ElfR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"])"
      ],
      "metadata": {
        "id": "ttr5EQnvElfS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
        "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\""
      ],
      "metadata": {
        "id": "uKp_EzKaElfS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Tool with Jupyter Kernel"
      ],
      "metadata": {
        "id": "hOcTDzkCElfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile local_python_tool.py\n",
        "\"\"\"Python tool using Jupyter kernel for stateful execution.\"\"\"\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import AsyncIterator, Any\n",
        "from uuid import UUID, uuid4\n",
        "\n",
        "from openai_harmony import (\n",
        "    Author,\n",
        "    Content,\n",
        "    Message,\n",
        "    Role,\n",
        "    TextContent,\n",
        "    ToolNamespaceConfig,\n",
        ")\n",
        "\n",
        "\n",
        "def add_libs(code: str) -> str:\n",
        "    \"\"\"Add common math libraries to code.\"\"\"\n",
        "    return \"import math\\nimport numpy as np\\nimport sympy as sp\\nfrom sympy import *\\n\" + code\n",
        "\n",
        "\n",
        "def ensure_last_print(code: str) -> str:\n",
        "    \"\"\"Ensure the last expression is printed.\"\"\"\n",
        "    lines = code.strip().split(\"\\n\")\n",
        "    if lines and \"print(\" not in lines[-1] and \"import\" not in lines[-1]:\n",
        "        if \"#\" in lines[-1]:\n",
        "            lines[-1] = lines[-1].split(\"#\")[0]\n",
        "        lines[-1] = \"print(\" + lines[-1] + \")\"\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "class LocalJupyterSession:\n",
        "    \"\"\"Stateful Jupyter kernel session for code execution.\"\"\"\n",
        "\n",
        "    # Class-level lock and port counter to avoid port conflicts\n",
        "    _port_lock = threading.Lock()\n",
        "    _next_port = 50000\n",
        "    _max_port = 65535  # Maximum valid port number\n",
        "\n",
        "    @classmethod\n",
        "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
        "        \"\"\"Get next available ports for kernel connection.\"\"\"\n",
        "        import socket\n",
        "        with cls._port_lock:\n",
        "            ports = []\n",
        "            attempts = 0\n",
        "            max_attempts = 100  # Prevent infinite loop\n",
        "\n",
        "            while len(ports) < count and attempts < max_attempts:\n",
        "                start_port = cls._next_port\n",
        "                # Check if port range is available\n",
        "                available = True\n",
        "                for i in range(count):\n",
        "                    port = start_port + i\n",
        "                    if port > cls._max_port:\n",
        "                        # Wrap around to beginning of port range\n",
        "                        start_port = 50000\n",
        "                        port = start_port + i\n",
        "\n",
        "                    # Quick check if port is in use\n",
        "                    try:\n",
        "                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                            s.settimeout(0.1)\n",
        "                            result = s.connect_ex(('127.0.0.1', port))\n",
        "                            if result == 0:\n",
        "                                available = False\n",
        "                                break\n",
        "                    except Exception:\n",
        "                        # If check fails, assume port might be in use\n",
        "                        available = False\n",
        "                        break\n",
        "\n",
        "                if available:\n",
        "                    ports = list(range(start_port, start_port + count))\n",
        "                    cls._next_port = start_port + count\n",
        "                    if cls._next_port > cls._max_port:\n",
        "                        cls._next_port = 50000\n",
        "                    break\n",
        "                else:\n",
        "                    # Try next range\n",
        "                    cls._next_port += count\n",
        "                    if cls._next_port > cls._max_port:\n",
        "                        cls._next_port = 50000\n",
        "                    attempts += 1\n",
        "\n",
        "            if len(ports) < count:\n",
        "                # Fallback: just return sequential ports without checking\n",
        "                ports = list(range(cls._next_port, cls._next_port + count))\n",
        "                cls._next_port += count\n",
        "                if cls._next_port > cls._max_port:\n",
        "                    cls._next_port = 50000\n",
        "\n",
        "            return ports\n",
        "\n",
        "    def __init__(self, connection_file: str | None = None, *, timeout: float = 120.0):\n",
        "        try:\n",
        "            from jupyter_client import BlockingKernelClient, KernelManager\n",
        "        except ImportError as exc:\n",
        "            raise RuntimeError(\"jupyter_client package required\") from exc\n",
        "\n",
        "        self._default_timeout = timeout\n",
        "        self._owns_kernel = False\n",
        "        self._client: BlockingKernelClient\n",
        "        self._km: KernelManager | None = None\n",
        "\n",
        "        if connection_file:\n",
        "            from pathlib import Path\n",
        "            connection_path = Path(connection_file).expanduser()\n",
        "            if not connection_path.exists():\n",
        "                raise FileNotFoundError(f\"Connection file not found: {connection_path}\")\n",
        "            client = BlockingKernelClient()\n",
        "            client.load_connection_file(str(connection_path))\n",
        "            client.start_channels()\n",
        "            client.wait_for_ready(timeout=self._default_timeout)\n",
        "            self._client = client\n",
        "        else:\n",
        "            # Allocate unique ports to avoid conflicts when running multiple kernels\n",
        "            ports = self._get_next_ports(5)\n",
        "            km = None\n",
        "            max_retries = 3\n",
        "            for retry in range(max_retries):\n",
        "                try:\n",
        "                    km = KernelManager()\n",
        "                    km.shell_port = ports[0]\n",
        "                    km.iopub_port = ports[1]\n",
        "                    km.stdin_port = ports[2]\n",
        "                    km.hb_port = ports[3]\n",
        "                    km.control_port = ports[4]\n",
        "                    km.start_kernel()\n",
        "                    client = km.blocking_client()\n",
        "                    client.start_channels()\n",
        "                    client.wait_for_ready(timeout=self._default_timeout)\n",
        "                    self._client = client\n",
        "                    self._km = km\n",
        "                    self._owns_kernel = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if retry < max_retries - 1:\n",
        "                        # Try different ports\n",
        "                        ports = self._get_next_ports(5)\n",
        "                        if km is not None:\n",
        "                            try:\n",
        "                                km.shutdown_kernel(now=True)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    else:\n",
        "                        # Last retry failed, raise the exception\n",
        "                        raise RuntimeError(f\"Failed to start kernel after {max_retries} retries: {e}\") from e\n",
        "\n",
        "    def execute(self, code: str, *, timeout: float | None = None) -> str:\n",
        "        \"\"\"Execute code and return combined stdout/stderr.\n",
        "        timeout: WALL-CLOCK seconds limit for this execution.\n",
        "        \"\"\"\n",
        "        import time\n",
        "        import queue as _queue\n",
        "\n",
        "        client = self._client\n",
        "        effective_timeout = float(timeout or self._default_timeout)\n",
        "\n",
        "        msg_id = client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n",
        "\n",
        "        stdout_parts: list[str] = []\n",
        "        stderr_parts: list[str] = []\n",
        "\n",
        "        # Track if we've seen a timeout/interrupt to filter IPython internal errors\n",
        "        _timeout_triggered = False\n",
        "\n",
        "        start = time.time()\n",
        "        poll = 0.5  # seconds: small polling interval so we can enforce wall-clock timeout\n",
        "\n",
        "        def _timed_out() -> bool:\n",
        "            return (time.time() - start) >= effective_timeout\n",
        "\n",
        "        # iopub loop\n",
        "        max_timeout_grace = 1.0  # Give kernel 1 seconds to clean up after interrupt\n",
        "        timeout_grace_start = None\n",
        "\n",
        "        while True:\n",
        "            if _timed_out():\n",
        "                if not _timeout_triggered:\n",
        "                    _timeout_triggered = True\n",
        "                    timeout_grace_start = time.time()\n",
        "                    # interrupt the kernel to stop runaway execution\n",
        "                    try:\n",
        "                        # BlockingKernelClient usually has interrupt_kernel\n",
        "                        client.interrupt_kernel()\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            if self._owns_kernel and self._km is not None:\n",
        "                                self._km.interrupt_kernel()\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                # After grace period, stop collecting messages and raise timeout\n",
        "                if timeout_grace_start and (time.time() - timeout_grace_start) > max_timeout_grace:\n",
        "                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n",
        "\n",
        "            try:\n",
        "                msg = client.get_iopub_msg(timeout=poll)\n",
        "            except _queue.Empty:\n",
        "                if _timeout_triggered and timeout_grace_start and (time.time() - timeout_grace_start) > max_timeout_grace:\n",
        "                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n",
        "                continue\n",
        "\n",
        "            if msg.get(\"parent_header\", {}).get(\"msg_id\") != msg_id:\n",
        "                continue\n",
        "\n",
        "            msg_type = msg.get(\"msg_type\")\n",
        "            content = msg.get(\"content\", {})\n",
        "\n",
        "            # After timeout is triggered, only collect essential messages and filter IPython errors\n",
        "            if _timeout_triggered:\n",
        "                # Only process status messages to detect idle state, ignore everything else\n",
        "                if msg_type == \"status\":\n",
        "                    if content.get(\"execution_state\") == \"idle\":\n",
        "                        break\n",
        "                # Skip all other messages after timeout to avoid IPython internal errors\n",
        "                continue\n",
        "\n",
        "            if msg_type == \"stream\":\n",
        "                text = content.get(\"text\", \"\")\n",
        "                if content.get(\"name\") == \"stdout\":\n",
        "                    stdout_parts.append(text)\n",
        "                else:\n",
        "                    stderr_parts.append(text)\n",
        "            elif msg_type == \"error\":\n",
        "                traceback_data = content.get(\"traceback\")\n",
        "                if traceback_data:\n",
        "                    stderr_parts.append(\"\\n\".join(traceback_data))\n",
        "                else:\n",
        "                    ename = content.get(\"ename\", \"\")\n",
        "                    evalue = content.get(\"evalue\", \"\")\n",
        "                    stderr_parts.append(f\"{ename}: {evalue}\".strip())\n",
        "            elif msg_type in {\"execute_result\", \"display_data\"}:\n",
        "                data = content.get(\"data\", {})\n",
        "                text = data.get(\"text/plain\")\n",
        "                if text:\n",
        "                    stdout_parts.append(text if text.endswith(\"\\n\") else f\"{text}\\n\")\n",
        "            elif msg_type == \"status\" and content.get(\"execution_state\") == \"idle\":\n",
        "                break\n",
        "\n",
        "        # shell reply (also wall-time protected)\n",
        "        # Reuse timeout_grace_start from iopub loop if timeout was already triggered\n",
        "        shell_timeout_grace_start = timeout_grace_start if _timeout_triggered else None\n",
        "\n",
        "        while True:\n",
        "            if _timed_out():\n",
        "                if not _timeout_triggered:\n",
        "                    _timeout_triggered = True\n",
        "                    shell_timeout_grace_start = time.time()\n",
        "                    try:\n",
        "                        client.interrupt_kernel()\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            if self._owns_kernel and self._km is not None:\n",
        "                                self._km.interrupt_kernel()\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                # After grace period, stop collecting messages and raise timeout\n",
        "                if shell_timeout_grace_start and (time.time() - shell_timeout_grace_start) > max_timeout_grace:\n",
        "                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n",
        "\n",
        "            try:\n",
        "                reply = client.get_shell_msg(timeout=poll)\n",
        "            except _queue.Empty:\n",
        "                if _timeout_triggered and shell_timeout_grace_start and (time.time() - shell_timeout_grace_start) > max_timeout_grace:\n",
        "                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n",
        "                continue\n",
        "\n",
        "            if reply.get(\"parent_header\", {}).get(\"msg_id\") != msg_id:\n",
        "                continue\n",
        "\n",
        "            reply_content = reply.get(\"content\", {})\n",
        "\n",
        "            # After timeout, skip error messages to avoid IPython internal errors\n",
        "            if _timeout_triggered and reply_content.get(\"status\") == \"error\":\n",
        "                # Skip IPython internal errors, just break to exit\n",
        "                break\n",
        "\n",
        "            if reply_content.get(\"status\") == \"error\":\n",
        "                traceback_data = reply_content.get(\"traceback\")\n",
        "                if traceback_data:\n",
        "                    stderr_parts.append(\"\\n\".join(traceback_data))\n",
        "                else:\n",
        "                    ename = reply_content.get(\"ename\", \"\")\n",
        "                    evalue = reply_content.get(\"evalue\", \"\")\n",
        "                    stderr_parts.append(f\"{ename}: {evalue}\".strip())\n",
        "            break\n",
        "\n",
        "        stdout = \"\".join(stdout_parts)\n",
        "        stderr = \"\".join(stderr_parts)\n",
        "\n",
        "        if stderr:\n",
        "            stdout = f\"{stdout.rstrip()}\\n{stderr}\" if stdout else stderr\n",
        "        if not stdout.strip():\n",
        "            stdout = \"[WARN] No output. Use print() to see results.\"\n",
        "        return stdout\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        import contextlib\n",
        "        with contextlib.suppress(Exception):\n",
        "            self._client.stop_channels()\n",
        "        if self._owns_kernel and self._km is not None:\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.shutdown_kernel(now=True)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()\n",
        "\n",
        "\n",
        "class PythonTool:\n",
        "    \"\"\"Python execution tool using Jupyter kernel.\"\"\"\n",
        "\n",
        "    def __init__(self, execution_backend: str | None = None, local_jupyter_timeout: float = 60.0):\n",
        "        self._local_jupyter_timeout = local_jupyter_timeout\n",
        "        self._execution_lock = threading.Lock()\n",
        "        self._jupyter_session: LocalJupyterSession | None = None\n",
        "        # Lazy initialization to avoid port conflicts during object creation\n",
        "        self._init_lock = threading.Lock()\n",
        "\n",
        "    def _ensure_session(self):\n",
        "        \"\"\"Lazily initialize the Jupyter session.\"\"\"\n",
        "        if self._jupyter_session is None:\n",
        "            with self._init_lock:\n",
        "                if self._jupyter_session is None:\n",
        "                    self._jupyter_session = LocalJupyterSession(timeout=self._local_jupyter_timeout)\n",
        "\n",
        "    @classmethod\n",
        "    def get_tool_name(cls) -> str:\n",
        "        return \"python\"\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return self.get_tool_name()\n",
        "\n",
        "    @property\n",
        "    def instruction(self) -> str:\n",
        "        return \"\"\"Use this tool to execute Python code. The code runs in a stateful Jupyter notebook. Use print() to see output.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def tool_config(self) -> ToolNamespaceConfig:\n",
        "        return ToolNamespaceConfig(\n",
        "            name=self.get_tool_name(),\n",
        "            description=self.instruction,\n",
        "            tools=[]\n",
        "        )\n",
        "\n",
        "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
        "        content = TextContent(text=output)\n",
        "        author = Author(role=Role.TOOL, name=self.get_tool_name())\n",
        "        message = Message(author=author, content=[content]).with_recipient(\"assistant\")\n",
        "        if channel:\n",
        "            message = message.with_channel(channel)\n",
        "        return message\n",
        "\n",
        "    def process_sync_plus(self, message: Message, timeout: float | None = None) -> list[Message]:\n",
        "        \"\"\"Execute code from message using Jupyter kernel.\"\"\"\n",
        "        self._ensure_session()\n",
        "        script = message.content[0].text\n",
        "        with self._execution_lock:\n",
        "            try:\n",
        "                output = self._jupyter_session.execute(script, timeout=timeout)\n",
        "            except TimeoutError as exc:\n",
        "                output = f\"[ERROR] {exc}\"\n",
        "            except Exception as exc:\n",
        "                output = f\"[ERROR] {exc}\"\n",
        "        return [self._make_response(output, channel=message.channel)]\n",
        "\n",
        "    def close(self):\n",
        "        if self._jupyter_session is not None:\n",
        "            self._jupyter_session.close()\n",
        "            self._jupyter_session = None\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "2gE43rboElfS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Setup"
      ],
      "metadata": {
        "id": "2DQvSbjfElfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import re\n",
        "import math\n",
        "import threading\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from openai import OpenAI\n",
        "from transformers import set_seed, AutoTokenizer\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "from openai_harmony import (\n",
        "    HarmonyEncodingName,\n",
        "    load_harmony_encoding,\n",
        "    Conversation,\n",
        "    Message,\n",
        "    Role,\n",
        "    SystemContent,\n",
        "    ReasoningEffort,\n",
        "    RenderConversationConfig,\n",
        ")\n",
        "\n",
        "from local_python_tool import PythonTool\n",
        "\n",
        "# Load Harmony encoding for GPT-OSS\n",
        "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "\n",
        "# Constants\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "MAX_LEN = 64 * 1024\n",
        "USE_BUDGET = False\n",
        "K = 8  # Number of parallel samples\n",
        "\n",
        "# Inference parameters (same as way-to-30 reference)\n",
        "TEMPERATURE = 1.0\n",
        "TOP_P = 1.0\n",
        "MIN_P = 0.02"
      ],
      "metadata": {
        "id": "FOoWO9urElfT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicTimeBudget:\n",
        "    \"\"\"Manages dynamic time allocation with rollover from early stopping.\"\"\"\n",
        "\n",
        "    def __init__(self, total_time_seconds: float, num_questions: int, buffer_seconds: float = 60):\n",
        "        self.total_time = total_time_seconds\n",
        "        self.num_questions = num_questions\n",
        "        self.buffer = buffer_seconds\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # Available time excluding buffer\n",
        "        self.available_time = total_time_seconds - buffer_seconds\n",
        "\n",
        "        # Track time usage\n",
        "        self.time_used = 0\n",
        "        self.questions_completed = 0\n",
        "        self.time_saved = 0  # Accumulated time from early stops\n",
        "\n",
        "    def get_deadline_for_question(self) -> float:\n",
        "        \"\"\"Calculate deadline for current question with rollover time.\"\"\"\n",
        "        questions_remaining = self.num_questions - self.questions_completed\n",
        "\n",
        "        if questions_remaining <= 0:\n",
        "            return time.time() + 60  # Emergency fallback\n",
        "\n",
        "        # Base time per remaining question\n",
        "        time_remaining = self.available_time - self.time_used\n",
        "        base_time = time_remaining / questions_remaining\n",
        "\n",
        "        # Add any saved time from early stopping\n",
        "        allocated_time = base_time + self.time_saved\n",
        "\n",
        "        # Reset saved time (it's now allocated to this question)\n",
        "        self.time_saved = 0\n",
        "\n",
        "        deadline = time.time() + allocated_time\n",
        "\n",
        "        print(f\"‚è±Ô∏è  Allocated {allocated_time:.1f}s for question {self.questions_completed + 1}\")\n",
        "        print(f\"   (Base: {base_time:.1f}s, Rollover: {self.time_saved:.1f}s, Remaining: {questions_remaining} questions)\")\n",
        "\n",
        "        return deadline\n",
        "\n",
        "    def record_question_completion(self, time_spent: float, early_stopped: bool = False):\n",
        "        \"\"\"Record completion and calculate time savings.\"\"\"\n",
        "        self.time_used += time_spent\n",
        "        self.questions_completed += 1\n",
        "\n",
        "        # If early stopped, calculate how much time was saved\n",
        "        if early_stopped:\n",
        "            questions_remaining = self.num_questions - self.questions_completed\n",
        "            if questions_remaining > 0:\n",
        "                expected_time = (self.available_time - self.time_used + time_spent) / (questions_remaining + 1)\n",
        "                time_saved = max(0, expected_time - time_spent)\n",
        "                self.time_saved += time_saved\n",
        "                print(f\"üí∞ Early stop saved {time_saved:.1f}s (total saved: {self.time_saved:.1f}s)\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7GjD6NIOElfU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start vLLM Server"
      ],
      "metadata": {
        "id": "Tt4-DrB_ElfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_vllm_server() -> subprocess.Popen:\n",
        "    \"\"\"Start vLLM server in background.\"\"\"\n",
        "    command = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\", \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
        "        \"--served-model-name\", \"gpt-oss\",\n",
        "        \"--tensor-parallel-size\", \"1\",\n",
        "        \"--max-num-seqs\", \"64\",\n",
        "        \"--gpu-memory-utilization\", \"0.96\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"8000\",\n",
        "        \"--dtype\", \"auto\",\n",
        "        \"--max-model-len\", str(MAX_LEN),\n",
        "        \"--stream-interval\", \"20\",\n",
        "    ]\n",
        "    with open(\"./vllm.log\", \"w\") as logfile:\n",
        "        process = subprocess.Popen(\n",
        "            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
        "        )\n",
        "    print(\"vLLM server started. Logs: ./vllm.log\")\n",
        "    return process\n",
        "\n",
        "\n",
        "vllm_process = start_vllm_server()"
      ],
      "metadata": {
        "id": "MvfZqbjRElfV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TIR Prompts"
      ],
      "metadata": {
        "id": "ZncyjbQ7ElfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Exact same as way-to-30 (proven 30/50 on LB)\n",
        "TIR_PROMPT_SIMPLE0 = \"\"\"You are an elite olympiad mathematician solving a national/international-level problem with full rigor; reason carefully, justify all nontrivial steps, explore multiple solution strategies when helpful, check edge cases, and use Python tool for computation or verification if needed, then return only the final verified answer in \\boxed{n}, where n ‚àà [0,99999], and never guess.\"\"\"\n",
        "\n",
        "# Use simple version (same as way-to-30) - change to TIR_PROMPT_ENHANCED if needed\n",
        "# TIR_PROMPTS = [TIR_PROMPT_SIMPLE]"
      ],
      "metadata": {
        "id": "jezAYkMpElfV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "gYlpcvZ2ElfV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Exact same as way-to-30 (proven 30/50 on LB)\n",
        "TIR_PROMPT_SIMPLE2 = \"\"\"Please reason step by step and use the python tool to solve the math problem.\n",
        "Finally, Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\"\"\"\n",
        "\n",
        "\n",
        "# Use both prompts to encourage diverse reasoning (simple + enhanced)\n",
        "TIR_PROMPTS = [TIR_PROMPT_SIMPLE2]"
      ],
      "metadata": {
        "trusted": true,
        "id": "556-L_CaElfV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencer with Harmony Protocol"
      ],
      "metadata": {
        "id": "Ky4Igxv4ElfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import queue\n",
        "from local_python_tool import PythonTool\n",
        "\n",
        "python_pool = queue.Queue(maxsize=K)\n",
        "\n",
        "for _ in range(K):\n",
        "    t = PythonTool(execution_backend=\"jupyter\", local_jupyter_timeout=60.0)\n",
        "    python_pool.put(t)\n",
        "print(\"Pool created!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "InqV5URVElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "CLEANUP_CODE = r\"\"\"\n",
        "import gc\n",
        "_keep = {\n",
        "    \"__builtins__\", \"__name__\", \"__doc__\", \"__package__\", \"__loader__\", \"__spec__\",\n",
        "    \"np\", \"sp\", \"math\",\n",
        "}\n",
        "g = globals()\n",
        "for k in list(g.keys()):\n",
        "    if k in _keep or k.startswith(\"_\"):\n",
        "        continue\n",
        "    try:\n",
        "        del g[k]\n",
        "    except Exception:\n",
        "        pass\n",
        "gc.collect()\n",
        "\"\"\"\n",
        "print(\"yes\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "AVvbp3eOElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class HarmonyTIRInferencer:\n",
        "    \"\"\"Inferencer using Harmony protocol with Tool-Integrated Reasoning (TIR).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        max_model_len: int = MAX_LEN,\n",
        "        temperature: float = TEMPERATURE,\n",
        "        top_p: float = TOP_P,\n",
        "        min_p: float = MIN_P,\n",
        "        seed: int = SEED,\n",
        "        k: int = K,\n",
        "        use_budget: bool = USE_BUDGET,\n",
        "        max_iter: int = 100,\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        self.model = \"gpt-oss\"\n",
        "        self.max_model_len = max_model_len\n",
        "        self.temperature = temperature\n",
        "        self.top_p = top_p\n",
        "        self.min_p = min_p\n",
        "        self.seed = seed\n",
        "        self.k = k\n",
        "        self.use_budget = use_budget\n",
        "        self.max_iter = max_iter\n",
        "        self.base_budget = 60 * 5.5  # 5.5 minutes base per problem\n",
        "        self.budget = 370              # initial budget in seconds (~6.1 min for first problem)\n",
        "        self.deadline = None\n",
        "\n",
        "        # Initialize the OpenAI-compatible client pointing to local vLLM server\n",
        "        self.client = OpenAI(\n",
        "            base_url=\"http://127.0.0.1:8000/v1\",\n",
        "            api_key=\"sk-local\",\n",
        "            timeout=360,\n",
        "        )\n",
        "        self.stop_token_ids = encoding.stop_tokens_for_assistant_actions()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    def wait_server(self):\n",
        "        \"\"\"Wait until the vLLM server is ready to accept requests.\"\"\"\n",
        "        for _ in range(15 * 60):\n",
        "            time.sleep(1)\n",
        "            try:\n",
        "                # List models to check if server is up\n",
        "                print(self.client.models.list())\n",
        "                return\n",
        "            except Exception:\n",
        "                continue\n",
        "        raise RuntimeError(\"vLLM server failed to start\")\n",
        "\n",
        "    def get_num_samples(self) -> int:\n",
        "        \"\"\"Determine number of parallel samples to generate based on remaining budget.\"\"\"\n",
        "        if not self.use_budget:\n",
        "            print(f\"Budget disabled -> N: {self.k}\")\n",
        "            return self.k\n",
        "        else:\n",
        "            return self.k\n",
        "\n",
        "    def apply_chat_template(self, prompt: str, python_tool: PythonTool) -> list[Message]:\n",
        "        \"\"\"Wrap user prompt into Harmony conversation format with system and tool info.\"\"\"\n",
        "        return [\n",
        "            Message.from_role_and_content(\n",
        "                Role.SYSTEM,\n",
        "                SystemContent.new()\n",
        "                .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
        "                .with_tools(python_tool.tool_config)\n",
        "            ),\n",
        "            Message.from_role_and_content(Role.USER, prompt),\n",
        "        ]\n",
        "\n",
        "    def format_prompts(self, problem: str) -> list[str]:\n",
        "        \"\"\"Create multiple prompts (possibly with different TIR strategies) for one problem.\"\"\"\n",
        "        num_samples = self.get_num_samples()\n",
        "        prompts = []\n",
        "        for i in range(num_samples):\n",
        "            # Alternate between the prompt templates for diversity\n",
        "            tir_prompt = TIR_PROMPTS[i % len(TIR_PROMPTS)]\n",
        "            prompts.append(problem + \"\\n\\n\" + tir_prompt)\n",
        "        return prompts\n",
        "\n",
        "    def inference(self, problem: str, deadline: float) -> tuple[int, float]:\n",
        "        \"\"\"Run the multi-sample inference for a single problem and return the final answer and saved time.\"\"\"\n",
        "        self.deadline = deadline\n",
        "        start_time = time.time()\n",
        "\n",
        "        prompts = self.format_prompts(problem)\n",
        "        responses = self._inference_parallel(prompts)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        saved_time = max(0.0, deadline - time.time())\n",
        "\n",
        "        print(f\"[Budget]: {(deadline - start_time):.2f}s\")\n",
        "        print(f\"[inference] Took {duration:.2f}s\")\n",
        "        print(f\"[Saved time]: {saved_time:.2f}s\")\n",
        "\n",
        "        return self.parse_responses(responses), saved_time\n",
        "\n",
        "\n",
        "    def single_generate_tir(self, prompt: str, stop_event: threading.Event, seed_offset: int = 0) -> str:\n",
        "        \"\"\"Generate single TIR response with tool execution (dynamic timeouts).\"\"\"\n",
        "        python_tool = None\n",
        "\n",
        "        def _compute_req_timeout() -> float:\n",
        "            # For vLLM request timeout\n",
        "            CUSHION = 0.5\n",
        "            MAX_REQ_TIMEOUT = 30.0\n",
        "            MIN_ALLOW = 0.2\n",
        "\n",
        "            if not getattr(self, \"deadline\", None):\n",
        "                return MAX_REQ_TIMEOUT\n",
        "\n",
        "            remaining = self.deadline - time.time()\n",
        "            if remaining <= 0:\n",
        "                return 0.0\n",
        "\n",
        "            t = remaining - CUSHION\n",
        "            if t <= 0:\n",
        "                return 0.0\n",
        "\n",
        "            return min(MAX_REQ_TIMEOUT, max(MIN_ALLOW, t))\n",
        "\n",
        "        def _compute_py_timeout() -> float:\n",
        "            # For python tool timeout\n",
        "            PY_CUSHION = 1.0\n",
        "            MAX_PY_TIMEOUT = 15.0\n",
        "            MIN_ALLOW = 0.2\n",
        "\n",
        "            if not getattr(self, \"deadline\", None):\n",
        "                return MAX_PY_TIMEOUT\n",
        "\n",
        "            remaining = self.deadline - time.time()\n",
        "            t = remaining - PY_CUSHION\n",
        "            if t <= 0:\n",
        "                return 0.0\n",
        "\n",
        "            return min(MAX_PY_TIMEOUT, max(MIN_ALLOW, t))\n",
        "\n",
        "        try:\n",
        "            # Use pool instead of creating new PythonTool\n",
        "            try:\n",
        "                python_tool = python_pool.get(timeout=30.0)\n",
        "            except queue.Empty:\n",
        "                print(\"‚ö†Ô∏è Failed to get python_tool from pool, creating new one\")\n",
        "                python_tool = PythonTool(execution_backend=\"jupyter\")\n",
        "                try:\n",
        "                    python_tool._ensure_session()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è python session init failed: {e}\")\n",
        "                    if python_tool is not None:\n",
        "                        try:\n",
        "                            python_tool.close()\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    return \"\"\n",
        "            else:\n",
        "                # Verify session is still alive\n",
        "                try:\n",
        "                    if python_tool._jupyter_session is None:\n",
        "                        python_tool._ensure_session()\n",
        "                    # Quick health check: try to execute a simple command\n",
        "                    test_output = python_tool._jupyter_session.execute(\"1+1\", timeout=2.0)\n",
        "                    if \"[ERROR]\" in test_output or \"Traceback\" in test_output:\n",
        "                        # Session is broken, recreate it\n",
        "                        try:\n",
        "                            python_tool.close()\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        python_tool._jupyter_session = None\n",
        "                        python_tool._ensure_session()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è python session health check failed: {e}, recreating\")\n",
        "                    try:\n",
        "                        python_tool.close()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    python_tool._jupyter_session = None\n",
        "                    try:\n",
        "                        python_tool._ensure_session()\n",
        "                    except Exception as e2:\n",
        "                        print(f\"‚ö†Ô∏è python session recreate failed: {e2}\")\n",
        "                        try:\n",
        "                            python_pool.put(python_tool, block=False)\n",
        "                        except queue.Full:\n",
        "                            pass\n",
        "                        return \"\"\n",
        "\n",
        "            messages = self.apply_chat_template(prompt, python_tool)\n",
        "            final_answer_found = \"\"\n",
        "\n",
        "            for iteration in range(self.max_iter):\n",
        "                # termination checks\n",
        "                if stop_event and stop_event.is_set():\n",
        "                    print(\"üõë Stop signal received\")\n",
        "                    break\n",
        "                if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n",
        "                    print(\"‚è∞ Deadline reached\")\n",
        "                    break\n",
        "                if final_answer_found:\n",
        "                    break\n",
        "\n",
        "                prompt_ids = encoding.render_conversation_for_completion(\n",
        "                    Conversation.from_messages(messages), Role.ASSISTANT\n",
        "                )\n",
        "                max_tokens = self.max_model_len - len(prompt_ids)\n",
        "                if max_tokens < 1:\n",
        "                    print(\"‚ö†Ô∏è Context full\")\n",
        "                    break\n",
        "\n",
        "                req_timeout = _compute_req_timeout()\n",
        "                if req_timeout <= 0:\n",
        "                    print(\"‚è∞ Not enough remaining time for vLLM request\")\n",
        "                    break\n",
        "\n",
        "                token_buffer: list[int] = []\n",
        "                token_buffer_str = \"\"\n",
        "                breaking = False\n",
        "\n",
        "                stream = None\n",
        "                try:\n",
        "                    stream = self.client.completions.create(\n",
        "                        model=self.model,\n",
        "                        prompt=prompt_ids,\n",
        "                        max_tokens=max_tokens,\n",
        "                        temperature=self.temperature,\n",
        "                        top_p=self.top_p,\n",
        "                        seed=self.seed + seed_offset,\n",
        "                        stream=True,\n",
        "                        extra_body=dict(\n",
        "                            min_p=self.min_p,\n",
        "                            stop_token_ids=self.stop_token_ids,\n",
        "                            return_token_ids=True,\n",
        "                        ),\n",
        "                        timeout=req_timeout,\n",
        "                    )\n",
        "\n",
        "                    for chunk in stream:\n",
        "                        try:\n",
        "                            if stop_event and stop_event.is_set():\n",
        "                                breaking = True\n",
        "                                break\n",
        "                            if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n",
        "                                breaking = True\n",
        "                                break\n",
        "\n",
        "                            # Safely extract chunk data\n",
        "                            if not chunk.choices or len(chunk.choices) == 0:\n",
        "                                continue\n",
        "\n",
        "                            choice = chunk.choices[0]\n",
        "                            token_chunk = getattr(choice, 'token_ids', None) or []\n",
        "                            text_chunk = getattr(choice, 'text', '') or ''\n",
        "\n",
        "                            if token_chunk:\n",
        "                                token_buffer.extend(token_chunk)\n",
        "                                token_buffer_str += text_chunk\n",
        "\n",
        "                            if len(token_buffer) > 60_000:\n",
        "                                print(\"‚ö†Ô∏è Token limit\")\n",
        "                                breaking = True\n",
        "                                break\n",
        "\n",
        "                            # early stop when boxed appears\n",
        "                            if \"}\" in text_chunk and self.extract_boxed_text(token_buffer_str) is not None:\n",
        "                                final_answer_found = token_buffer_str\n",
        "                                breaking = True\n",
        "                                break\n",
        "                        except StopIteration:\n",
        "                            # Stream ended normally\n",
        "                            break\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è Error processing stream chunk: {e}\")\n",
        "                            # Continue processing, but mark as potentially broken\n",
        "                            break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error creating/reading stream: {e}\")\n",
        "                    breaking = True\n",
        "                finally:\n",
        "                    if stream is not None:\n",
        "                        try:\n",
        "                            stream.close()\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        # Additional cleanup attempt\n",
        "                        try:\n",
        "                            del stream\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                if breaking:\n",
        "                    break\n",
        "\n",
        "                if not token_buffer:\n",
        "                    continue\n",
        "\n",
        "                # parse completion\n",
        "                try:\n",
        "                    new_messages = encoding.parse_messages_from_completion_tokens(\n",
        "                        token_buffer, Role.ASSISTANT\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing completion: {e}\")\n",
        "                    break\n",
        "\n",
        "                messages.extend(new_messages)\n",
        "                last_message = messages[-1]\n",
        "\n",
        "                if last_message.channel == \"final\" or token_buffer[-1] == 200002:\n",
        "                    break\n",
        "\n",
        "                if last_message.recipient == \"python\":\n",
        "                    if stop_event and stop_event.is_set():\n",
        "                        break\n",
        "                    if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n",
        "                        break\n",
        "\n",
        "                    py_timeout = _compute_py_timeout()\n",
        "                    if py_timeout <= 0 or py_timeout < 0.5:\n",
        "                        print(f\"‚è∞ Not enough remaining time for python ({py_timeout:.2f}s)\")\n",
        "                        break\n",
        "\n",
        "                    print(\"üêç Executing Python code...\")\n",
        "                    try:\n",
        "                        response_msgs = python_tool.process_sync_plus(last_message, timeout=py_timeout)\n",
        "                    except Exception as e:\n",
        "                        # treat any python tool failure as terminal for this sample\n",
        "                        print(f\"‚ö†Ô∏è python tool failed: {e}\")\n",
        "                        break\n",
        "\n",
        "                    messages.extend(response_msgs)\n",
        "\n",
        "            if final_answer_found:\n",
        "                return final_answer_found\n",
        "\n",
        "            return encoding.decode_utf8(\n",
        "                encoding.render_conversation_for_training(\n",
        "                    Conversation.from_messages(messages),\n",
        "                    RenderConversationConfig(auto_drop_analysis=False),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            # never swallow manual interrupts\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"Error in generation: {e}\")\n",
        "            print(f\"Traceback: {traceback.format_exc()}\")\n",
        "            return \"\"\n",
        "        finally:\n",
        "            # Return tool to pool instead of closing it\n",
        "            if python_tool is not None:\n",
        "                try:\n",
        "                    # Only return to pool if we got it from pool\n",
        "                    # Check if tool is still healthy before returning\n",
        "                    if python_tool._jupyter_session is not None:\n",
        "                        try:\n",
        "                            # Quick health check\n",
        "                            test_output = python_tool._jupyter_session.execute(\"1+1\", timeout=1.0)\n",
        "                            if \"[ERROR]\" not in test_output and \"Traceback\" not in test_output:\n",
        "                                # Tool is healthy, return to pool\n",
        "                                try:\n",
        "                                    python_pool.put(python_tool, block=False)\n",
        "                                except queue.Full:\n",
        "                                    # Pool is full, close the tool\n",
        "                                    python_tool.close()\n",
        "                            else:\n",
        "                                # Tool is broken, close it\n",
        "                                python_tool.close()\n",
        "                        except Exception:\n",
        "                            # Health check failed, close the tool\n",
        "                            try:\n",
        "                                python_tool.close()\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    else:\n",
        "                        # No session, safe to return to pool\n",
        "                        try:\n",
        "                            python_pool.put(python_tool, block=False)\n",
        "                        except queue.Full:\n",
        "                            pass\n",
        "                except Exception as e:\n",
        "                    # If anything goes wrong, try to close the tool\n",
        "                    try:\n",
        "                        python_tool.close()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "\n",
        "    def _inference_parallel(self, prompts: list[str]) -> list[str]:\n",
        "        \"\"\"Run multiple `single_generate_tir` in parallel and return all raw responses.\"\"\"\n",
        "        stop_event = threading.Event()\n",
        "        answers_collected: List[int] = []\n",
        "        raw_responses = [\"\"] * len(prompts)\n",
        "        majority_threshold = len(prompts) / 2  # more than half of the samples\n",
        "\n",
        "        print(f\"üöÄ Sampling {len(prompts)} times (threshold: > {majority_threshold})...\")\n",
        "\n",
        "        executor = ThreadPoolExecutor(max_workers=self.k)\n",
        "        futures = []\n",
        "        future_to_idx = {}\n",
        "        try:\n",
        "            for i, p in enumerate(prompts):\n",
        "                fut = executor.submit(self.single_generate_tir, p, stop_event, i)\n",
        "                futures.append(fut)\n",
        "                future_to_idx[fut] = i\n",
        "\n",
        "            completed_count = 0\n",
        "            for fut in as_completed(futures):\n",
        "                idx = future_to_idx.get(fut, -1)\n",
        "                if idx < 0:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    result_text = fut.result(timeout=1.0)\n",
        "                except Exception as e:\n",
        "                    import traceback\n",
        "                    print(f\"Task exception for idx {idx}: {e}\")\n",
        "                    print(f\"Traceback: {traceback.format_exc()}\")\n",
        "                    result_text = \"\"\n",
        "\n",
        "                raw_responses[idx] = result_text\n",
        "                completed_count += 1\n",
        "\n",
        "                ans = self.extract_boxed_text(result_text)\n",
        "                if ans is not None:\n",
        "                    answers_collected.append(ans)\n",
        "                    counts = Counter(answers_collected)\n",
        "                    if counts:\n",
        "                        most_common_ans, count = counts.most_common(1)[0]\n",
        "\n",
        "                        if count > majority_threshold:\n",
        "                            print(f\"üéØ Majority reached! {most_common_ans} appeared {count} times\")\n",
        "                            stop_event.set()\n",
        "\n",
        "                            # best-effort: cancel those not started yet\n",
        "                            for f in futures:\n",
        "                                if f is not fut and not f.done():\n",
        "                                    try:\n",
        "                                        f.cancel()\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                            break\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"Error in _inference_parallel: {e}\")\n",
        "            print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        finally:\n",
        "            stop_event.set()\n",
        "            # Ensure all futures are handled\n",
        "            for fut in futures:\n",
        "                if not fut.done():\n",
        "                    try:\n",
        "                        fut.cancel()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            # Shutdown executor with timeout protection\n",
        "            try:\n",
        "                # Python 3.9+ supports timeout, but we'll use a workaround for compatibility\n",
        "                import sys\n",
        "                if sys.version_info >= (3, 9):\n",
        "                    executor.shutdown(wait=True, timeout=60.0, cancel_futures=True)\n",
        "                else:\n",
        "                    # For older Python versions, use wait without timeout\n",
        "                    executor.shutdown(wait=True)\n",
        "            except TypeError:\n",
        "                # timeout parameter not supported, use without it\n",
        "                try:\n",
        "                    executor.shutdown(wait=True)\n",
        "                except Exception:\n",
        "                    executor.shutdown(wait=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: executor shutdown had issues: {e}\")\n",
        "                # Force shutdown\n",
        "                try:\n",
        "                    executor.shutdown(wait=False)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        return raw_responses\n",
        "\n",
        "\n",
        "    def extract_boxed_text(self, text: str) -> int | None:\n",
        "        \"\"\"Extract a numeric answer from '\\\\boxed{}' or 'final answer is ...' in the text.\"\"\"\n",
        "        # Pattern for \\boxed{NUMBER}\n",
        "        pattern = r'oxed{(.*?)}'\n",
        "        matches = re.findall(pattern, str(text))\n",
        "        if matches:\n",
        "            for match in reversed(matches):\n",
        "                if match:\n",
        "                    try:\n",
        "                        # Remove commas/spaces and parse as number (float covers scientific notation if any)\n",
        "                        clean_match = match.strip().replace(',', '').replace(' ', '')\n",
        "                        val = int(float(clean_match[:20]))\n",
        "                        if 0 <= val <= 99999:\n",
        "                            return val\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        # Pattern for \"final answer is X\" or \"Final Answer: X\"\n",
        "        pattern = r'(?i)final\\s+answer\\s*(?:is|:)?\\s*(\\d+)'\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            for match in reversed(matches):\n",
        "                if match:\n",
        "                    try:\n",
        "                        val = int(match)\n",
        "                        if 0 <= val <= 99999:\n",
        "                            return val\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def parse_responses(self, responses: list[str]) -> int:\n",
        "        \"\"\"Decide on the final answer from all responses by majority vote (with tie-break).\"\"\"\n",
        "        answers = [self.extract_boxed_text(r) for r in responses]\n",
        "\n",
        "        # Filter out any None values (cases where no answer was extracted)\n",
        "        valid_answers = [a for a in answers if a is not None]\n",
        "        if not valid_answers:\n",
        "            print(\"No valid answers found\")\n",
        "            return 8687\n",
        "\n",
        "        counter = Counter(valid_answers)\n",
        "        print(f\"Answers: {counter}\")\n",
        "\n",
        "        # Majority vote: pick the most common answer; break ties by choosing the largest answer\n",
        "        most_common_list = counter.most_common(2)\n",
        "        if len(most_common_list) > 1 and most_common_list[0][1] == most_common_list[1][1]:\n",
        "            tied_answers = [ans for ans, cnt in counter.items() if cnt == most_common_list[0][1]]\n",
        "            answer = max(tied_answers)\n",
        "        else:\n",
        "            answer = most_common_list[0][0]\n",
        "        return answer"
      ],
      "metadata": {
        "id": "quGKySo4ElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# time_budget_manager = DynamicTimeBudget(TOTAL_TIME, NUM_QUESTIONS, BUFFER_TIME)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CiVG6ZxOElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the inferencer with the model path and parameters\n",
        "inferencer = HarmonyTIRInferencer(\n",
        "    \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
        "    use_budget=USE_BUDGET,\n",
        "    k=K,\n",
        ")"
      ],
      "metadata": {
        "id": "5QaZB27SElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# inferencer.time_budget_manager = time_budget_manager"
      ],
      "metadata": {
        "trusted": true,
        "id": "I6tm8kjQElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inferencer.wait_server()"
      ],
      "metadata": {
        "id": "5Qwv6kJoElfW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "r5Wg4WDXElfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_time = time.time()\n",
        "cutoff_times = [int(x) for x in np.linspace(final_cutoff_time, init_time, 50 + 1)]\n",
        "cutoff_times.pop()"
      ],
      "metadata": {
        "id": "1ISFhinfElfX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"Make a prediction.\"\"\"\n",
        "    global correct_count, total_count, predictions, cutoff_times\n",
        "\n",
        "    question_id = id_.item(0)\n",
        "    question_text = question.item(0)\n",
        "\n",
        "    print(\"------\")\n",
        "    print(f\"ID: {question_id}\")\n",
        "    print(f\"Question: {question_text[:200]}...\")\n",
        "\n",
        "    current_deadline = cutoff_times[-1]\n",
        "    answer,saved_time = inferencer.inference(question_text, deadline=current_deadline)\n",
        "    cutoff_times.pop()\n",
        "\n",
        "    # ‚è±Ô∏è Dynamically recompute cutoff_times and distribute saved_time\n",
        "    if len(cutoff_times) > 0:\n",
        "        now = time.time()\n",
        "        num_remaining = len(cutoff_times)\n",
        "        base_times = np.linspace(final_cutoff_time, now, num_remaining + 1)\n",
        "        base_times = base_times[:-1]  # keep only N timestamps\n",
        "        extra = saved_time / num_remaining\n",
        "        cutoff_times = [int(t + extra) for t in base_times]\n",
        "\n",
        "    # Store prediction\n",
        "    predictions[question_id] = answer\n",
        "\n",
        "    # Check accuracy if ground truth available\n",
        "    total_count += 1\n",
        "    if question_id in ground_truth:\n",
        "        gt = ground_truth[question_id]\n",
        "        is_correct = (answer == gt)\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "        print(f\"Answer: {answer} | Ground Truth: {gt} | {status}\")\n",
        "        print(f\"üìä Running Accuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"Answer: {answer}\")\n",
        "\n",
        "    print(\"------\\n\")\n",
        "\n",
        "    return pl.DataFrame({\"id\": question_id, \"answer\": answer})"
      ],
      "metadata": {
        "id": "ighINPKhElfX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load reference data and keep ground truth for accuracy calculation\n",
        "df = pd.read_csv(\n",
        "    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n",
        ")\n",
        "\n",
        "# Store ground truth answers for accuracy calculation (only in local mode)\n",
        "ground_truth = dict(zip(df[\"id\"], df[\"answer\"])) if \"answer\" in df.columns else {}\n",
        "\n",
        "# Create input file without answers\n",
        "df.drop(\"answer\", axis=1, errors=\"ignore\").to_csv(\"reference.csv\", index=False)\n",
        "\n",
        "# Track predictions for accuracy calculation\n",
        "predictions = {}\n",
        "correct_count = 0\n",
        "total_count = 0"
      ],
      "metadata": {
        "id": "92gCfzqkElfX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle_evaluation.aimo_3_inference_server\n",
        "\n",
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway((\"reference.csv\",))\n",
        "\n",
        "    # Print final accuracy summary\n",
        "    if ground_truth and total_count > 0:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"üìä FINAL ACCURACY SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Correct: {correct_count}/{total_count}\")\n",
        "        print(f\"Accuracy: {100*correct_count/total_count:.1f}%\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Show details\n",
        "        print(\"\\nDetails:\")\n",
        "        for qid, pred in predictions.items():\n",
        "            if qid in ground_truth:\n",
        "                gt = ground_truth[qid]\n",
        "                status = \"‚úÖ\" if pred == gt else \"‚ùå\"\n",
        "                print(f\"  {qid}: pred={pred}, gt={gt} {status}\")"
      ],
      "metadata": {
        "id": "bB4DwKGHElfX"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}