{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaH100",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 289055161,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 510391,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 404485,
          "modelId": 422384
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Experiments with AIMO 3 | GPT-OSS-120B(with tools)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantexolution/aimo/blob/main/Experiments_with_AIMO_3_%7C_GPT_OSS_120B(with_tools).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "7CWf_qTgE2fB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "andreasbis_aimo_3_utils_path = kagglehub.notebook_output_download('andreasbis/aimo-3-utils')\n",
        "danielhanchen_gpt_oss_120b_transformers_default_1_path = kagglehub.model_download('danielhanchen/gpt-oss-120b/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "sYIfJxGPE2fC"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:14:34.630182Z",
          "iopub.execute_input": "2026-01-09T17:14:34.630322Z",
          "iopub.status.idle": "2026-01-09T17:15:37.535476Z",
          "shell.execute_reply.started": "2026-01-09T17:14:34.630307Z",
          "shell.execute_reply": "2026-01-09T17:15:37.534959Z"
        },
        "id": "N3EnkLL1E2fC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:15:37.536279Z",
          "iopub.execute_input": "2026-01-09T17:15:37.536431Z",
          "iopub.status.idle": "2026-01-09T17:15:37.539064Z",
          "shell.execute_reply.started": "2026-01-09T17:15:37.536412Z",
          "shell.execute_reply": "2026-01-09T17:15:37.538711Z"
        },
        "id": "KemHrbrAE2fC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:15:37.539962Z",
          "iopub.execute_input": "2026-01-09T17:15:37.540102Z",
          "iopub.status.idle": "2026-01-09T17:15:37.570055Z",
          "shell.execute_reply.started": "2026-01-09T17:15:37.540088Z",
          "shell.execute_reply": "2026-01-09T17:15:37.569539Z"
        },
        "id": "c8lugOE3E2fC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def set_env(input_archive, temp_dir):\n",
        "\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
        "\n",
        "    subprocess.run([\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'pip',\n",
        "        'install',\n",
        "        '--no-index',\n",
        "        '--find-links',\n",
        "        f'{temp_dir}/wheels',\n",
        "        'unsloth',\n",
        "        'trl',\n",
        "        'vllm',\n",
        "        'openai_harmony'\n",
        "    ], check=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:15:37.570808Z",
          "iopub.execute_input": "2026-01-09T17:15:37.571011Z",
          "iopub.status.idle": "2026-01-09T17:15:37.576783Z",
          "shell.execute_reply.started": "2026-01-09T17:15:37.570993Z",
          "shell.execute_reply": "2026-01-09T17:15:37.576205Z"
        },
        "id": "y_PSnkj-E2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_env(\n",
        "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz',\n",
        "    temp_dir='/kaggle/tmp/setup'\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:15:37.577455Z",
          "iopub.execute_input": "2026-01-09T17:15:37.577606Z",
          "iopub.status.idle": "2026-01-09T17:19:11.769153Z",
          "shell.execute_reply.started": "2026-01-09T17:15:37.577591Z",
          "shell.execute_reply": "2026-01-09T17:19:11.76863Z"
        },
        "id": "VFZ96vD-E2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:19:11.769765Z",
          "iopub.execute_input": "2026-01-09T17:19:11.769893Z",
          "iopub.status.idle": "2026-01-09T17:19:11.778359Z",
          "shell.execute_reply.started": "2026-01-09T17:19:11.769879Z",
          "shell.execute_reply": "2026-01-09T17:19:11.777995Z"
        },
        "id": "i9ChPs99E2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
        "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
        "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:19:11.778882Z",
          "iopub.execute_input": "2026-01-09T17:19:11.779015Z",
          "iopub.status.idle": "2026-01-09T17:19:11.782749Z",
          "shell.execute_reply.started": "2026-01-09T17:19:11.779003Z",
          "shell.execute_reply": "2026-01-09T17:19:11.782372Z"
        },
        "id": "FLCTqVV_E2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import contextlib\n",
        "from typing import Optional\n",
        "from jupyter_client import KernelManager\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from openai_harmony import (\n",
        "    HarmonyEncodingName,\n",
        "    load_harmony_encoding,\n",
        "    SystemContent,\n",
        "    DeveloperContent,\n",
        "    ReasoningEffort,\n",
        "    ToolNamespaceConfig,\n",
        "    Author,\n",
        "    Message,\n",
        "    Role,\n",
        "    TextContent,\n",
        "    Conversation\n",
        ")\n",
        "\n",
        "from transformers import set_seed\n",
        "import kaggle_evaluation.aimo_3_inference_server"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T17:19:11.783956Z",
          "iopub.execute_input": "2026-01-09T17:19:11.784267Z",
          "iopub.status.idle": "2026-01-09T17:19:18.272602Z",
          "shell.execute_reply.started": "2026-01-09T17:19:11.784254Z",
          "shell.execute_reply": "2026-01-09T17:19:18.2722Z"
        },
        "id": "28CJLpZPE2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "\n",
        "    system_prompt_1 = (\n",
        "        'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
        "        'The final answer must be a non-negative integer between 0 and 99999. '\n",
        "        'You must place the final integer answer inside \\\\boxed{}.'\n",
        "    )\n",
        "\n",
        "    tool_prompt = (\n",
        "        'Use this tool to execute Python code. '\n",
        "        'The environment is a stateful Jupyter notebook. '\n",
        "        'You must use print() to output results.'\n",
        "    )\n",
        "\n",
        "        # Behavioral instructions (go in DEVELOPER role - active commands)\n",
        "    developer_prompt_1 = (\n",
        "        'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
        "        'The final answer must be a non-negative integer between 0 and 99999. '\n",
        "        'Output the probability of your answer being correct inside \\\\probability{}. '\n",
        "        'You must place the final integer answer inside \\\\boxed{}. '\n",
        "    )\n",
        "\n",
        "    # developer_prompt_2 = (\n",
        "    #     'You are an expert mathematician who solves problems systematically. '\n",
        "    #     'Break down the problem into steps. Verify each step with Python. '\n",
        "    #     'The final answer must be an integer in [0, 99999] inside \\\\boxed{}.'\n",
        "    # )\n",
        "\n",
        "    # developer_prompt_3 = (\n",
        "    #     'You are a computational mathematician. Use Python to verify ALL calculations. '\n",
        "    #     'Never perform arithmetic mentally - always use code. '\n",
        "    #     'Final answer: integer in [0, 99999] inside \\\\boxed{}.'\n",
        "    # )\n",
        "\n",
        "        # Array for ensemble rotation\n",
        "    developer_prompts = [\n",
        "        developer_prompt_1,\n",
        "\n",
        "    ]\n",
        "\n",
        "    # === PROMPT ENSEMBLE (Diversity for better coverage) ===\n",
        "    system_prompts = [\n",
        "        # Prompt 0: Standard IMO competitor\n",
        "        (\n",
        "            'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
        "            'The final answer must be a non-negative integer between 0 and 99999. '\n",
        "            'Output the probability of your answer being correct inside \\\\probability{}. '\n",
        "            'You must place the final integer answer inside \\\\boxed{}.'\n",
        "        ),\n",
        "        # # Prompt 1: Methodical step-by-step\n",
        "        # (\n",
        "        #     'You are an expert mathematician who solves problems systematically. '\n",
        "        #     'Break down the problem into clear steps. Verify each step with Python code. '\n",
        "        #     'The final answer must be an integer in [0, 99999] inside \\\\boxed{}.'\n",
        "        # ),\n",
        "        # # Prompt 2: Computation-focused (always use code)\n",
        "        # (\n",
        "        #     'You are a computational mathematician. Use Python to verify ALL calculations. '\n",
        "        #     'Never perform arithmetic mentally - always use code to compute and verify. '\n",
        "        #     'Final answer: integer in [0, 99999] inside \\\\boxed{}.'\n",
        "        # ),\n",
        "    ]\n",
        "\n",
        "    preference_prompts = [\n",
        "        'Use all tools possible for correct mathematical computations. You have a jupyter notebook available which keeps full memory of all intermediate variables.',\n",
        "        'Use `sympy` for symbolic computation and verify results numerically.'\n",
        "    ]\n",
        "\n",
        "    served_model_name = 'gpt-oss'\n",
        "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
        "\n",
        "    kv_cache_dtype = 'fp8_e4m3'\n",
        "    dtype = 'auto'\n",
        "\n",
        "    high_problem_timeout = 900\n",
        "    base_problem_timeout = 300\n",
        "\n",
        "    notebook_limit = 17400\n",
        "    server_timeout = 180\n",
        "\n",
        "    session_timeout = 960\n",
        "    jupyter_timeout = 10\n",
        "    sandbox_timeout = 5\n",
        "\n",
        "    stream_interval = 200\n",
        "    context_tokens = 65536\n",
        "    search_tokens = 128\n",
        "    buffer_tokens = 512\n",
        "    batch_size = 256\n",
        "\n",
        "    # === EARLY STOPPING ===\n",
        "    early_stop = 4                 # Votes needed to stop early\n",
        "    min_samples_before_stop = 4     # Min samples before early stop allowed\n",
        "\n",
        "    attempts = 8\n",
        "    workers = 16\n",
        "    turns = 128\n",
        "    seed = 68\n",
        "\n",
        "    gpu_memory_utilization = 0.96\n",
        "    temperature = 0.5\n",
        "    min_p = 0.01\n",
        "\n",
        "    # === TEMPERATURE SCHEDULE ===\n",
        "    base_temperature = 0.75\n",
        "    temp_low = 0                  # Lower bound for temperature variation\n",
        "    temp_high = 1.4                # Upper bound for temperature variation\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:35:40.272216Z",
          "iopub.execute_input": "2026-01-09T18:35:40.272423Z",
          "iopub.status.idle": "2026-01-09T18:35:40.277102Z",
          "shell.execute_reply.started": "2026-01-09T18:35:40.272408Z",
          "shell.execute_reply": "2026-01-09T18:35:40.276584Z"
        },
        "id": "lT7eOjkbE2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:35:53.689313Z",
          "iopub.execute_input": "2026-01-09T18:35:53.689835Z",
          "iopub.status.idle": "2026-01-09T18:35:53.693326Z",
          "shell.execute_reply.started": "2026-01-09T18:35:53.689817Z",
          "shell.execute_reply": "2026-01-09T18:35:53.692834Z"
        },
        "id": "qKZcTc1sE2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Template:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
        "\n",
        "        return (\n",
        "            SystemContent.new()\n",
        "            .with_model_identity(system_prompt)\n",
        "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
        "            .with_tools(tool_config)\n",
        "        )\n",
        "\n",
        "    def apply_chat_template(\n",
        "        self,\n",
        "        system_prompt: str,\n",
        "        developer_prompt: str,  # Changed from system_prompt\n",
        "        user_prompt: str,\n",
        "        tool_config: ToolNamespaceConfig\n",
        "    ) -> list[Message]:\n",
        "\n",
        "        # 1. SYSTEM message: Identity + tools only\n",
        "        system_content = self.get_system_content(system_prompt, tool_config)\n",
        "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
        "\n",
        "        # 2. DEVELOPER message: Behavioral instructions (ACTIVE)\n",
        "        developer_content = DeveloperContent.new().with_instructions(developer_prompt)\n",
        "        developer_message = Message.from_role_and_content(Role.DEVELOPER, developer_content)\n",
        "\n",
        "        # 3. USER message: The problem\n",
        "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
        "\n",
        "        return [system_message, developer_message, user_message]\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:36:11.44484Z",
          "iopub.execute_input": "2026-01-09T18:36:11.445471Z",
          "iopub.status.idle": "2026-01-09T18:36:11.449547Z",
          "shell.execute_reply.started": "2026-01-09T18:36:11.445453Z",
          "shell.execute_reply": "2026-01-09T18:36:11.449066Z"
        },
        "id": "o4vN6_ytE2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Sandbox:\n",
        "\n",
        "    _port_lock = threading.Lock()\n",
        "    _next_port = 50000\n",
        "\n",
        "    @classmethod\n",
        "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
        "\n",
        "        with cls._port_lock:\n",
        "            ports = list(range(cls._next_port, cls._next_port + count))\n",
        "            cls._next_port += count\n",
        "\n",
        "            return ports\n",
        "\n",
        "    def __init__(self, timeout: float):\n",
        "\n",
        "        self._default_timeout = timeout\n",
        "        self._owns_kernel = False\n",
        "        self._client = None\n",
        "        self._km = None\n",
        "\n",
        "        ports = self._get_next_ports(5)\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
        "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
        "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
        "        env['PYTHONWARNINGS'] = 'ignore'\n",
        "        env['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "        self._km = KernelManager()\n",
        "        self._km.shell_port = ports[0]\n",
        "        self._km.iopub_port = ports[1]\n",
        "        self._km.stdin_port = ports[2]\n",
        "        self._km.hb_port = ports[3]\n",
        "        self._km.control_port = ports[4]\n",
        "\n",
        "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
        "\n",
        "        self._client = self._km.blocking_client()\n",
        "        self._client.start_channels()\n",
        "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
        "        self._owns_kernel = True\n",
        "\n",
        "        # self.execute(\n",
        "        #     'import math\\n'\n",
        "        #     'import sympy\\n'\n",
        "        #     'import itertools\\n'\n",
        "        #     'import collections\\n'\n",
        "        #     'import numpy as np\\n'\n",
        "        #     'import mpmath\\n'\n",
        "        #     'mpmath.mp.dps = 64\\n'\n",
        "        #     'from fractions import Fraction\\n'\n",
        "\n",
        "        # )\n",
        "        self.execute(\n",
        "            'import math, cmath, decimal, fractions, itertools, functools, collections, random, sys\\n'\n",
        "            'from fractions import Fraction\\n'\n",
        "            'from decimal import Decimal\\n'\n",
        "            'from collections import Counter, defaultdict, deque\\n'\n",
        "            'import numpy as np\\n'\n",
        "            'import mpmath\\n'\n",
        "            'mpmath.mp.dps = 64\\n'\n",
        "            'import sympy\\n'\n",
        "            'from sympy import *\\n'\n",
        "            'from sympy.ntheory import *\\n'\n",
        "            'from sympy.ntheory.modular import crt\\n'\n",
        "            'decimal.getcontext().prec = 50\\n'\n",
        "            'sys.setrecursionlimit(10000)\\n'\n",
        "        )\n",
        "\n",
        "\n",
        "    def _format_error(self, traceback: list[str]) -> str:\n",
        "\n",
        "        clean_lines = []\n",
        "\n",
        "        for frame in traceback:\n",
        "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
        "\n",
        "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
        "                continue\n",
        "\n",
        "            clean_lines.append(clean_frame)\n",
        "\n",
        "        return ''.join(clean_lines)\n",
        "\n",
        "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
        "\n",
        "        client = self._client\n",
        "        effective_timeout = timeout or self._default_timeout\n",
        "\n",
        "        msg_id = client.execute(\n",
        "            code,\n",
        "            store_history=True,\n",
        "            allow_stdin=False,\n",
        "            stop_on_error=False\n",
        "        )\n",
        "\n",
        "        stdout_parts = []\n",
        "        stderr_parts = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        while True:\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            if elapsed > effective_timeout:\n",
        "                self._km.interrupt_kernel()\n",
        "\n",
        "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
        "\n",
        "            try:\n",
        "                msg = client.get_iopub_msg(timeout=1.0)\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
        "                continue\n",
        "\n",
        "            msg_type = msg.get('msg_type')\n",
        "            content = msg.get('content', {})\n",
        "\n",
        "            if msg_type == 'stream':\n",
        "                text = content.get('text', '')\n",
        "\n",
        "                if content.get('name') == 'stdout':\n",
        "                    stdout_parts.append(text)\n",
        "\n",
        "                else:\n",
        "                    stderr_parts.append(text)\n",
        "\n",
        "            elif msg_type == 'error':\n",
        "                traceback_list = content.get('traceback', [])\n",
        "\n",
        "                stderr_parts.append(self._format_error(traceback_list))\n",
        "\n",
        "            elif msg_type in {'execute_result', 'display_data'}:\n",
        "                data = content.get('data', {})\n",
        "                text = data.get('text/plain')\n",
        "\n",
        "                if text:\n",
        "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
        "\n",
        "            elif msg_type == 'status':\n",
        "                if content.get('execution_state') == 'idle':\n",
        "                    break\n",
        "\n",
        "        stdout = ''.join(stdout_parts)\n",
        "        stderr = ''.join(stderr_parts)\n",
        "\n",
        "        if stderr:\n",
        "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
        "\n",
        "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
        "\n",
        "    def close(self):\n",
        "\n",
        "        with contextlib.suppress(Exception):\n",
        "            if self._client:\n",
        "                self._client.stop_channels()\n",
        "\n",
        "        if self._owns_kernel and self._km is not None:\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.shutdown_kernel(now=True)\n",
        "\n",
        "            with contextlib.suppress(Exception):\n",
        "                self._km.cleanup_resources()\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.execute('%reset -f')\n",
        "        self.execute('import gc; gc.collect()')\n",
        "\n",
        "        self.execute(\n",
        "            'import math\\n'\n",
        "            'import sympy\\n'\n",
        "            'import itertools\\n'\n",
        "            'import collections\\n'\n",
        "            'import numpy as np\\n'\n",
        "            'import mpmath\\n'\n",
        "            'mpmath.mp.dps = 64\\n'\n",
        "\n",
        "        )\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:36:13.807745Z",
          "iopub.execute_input": "2026-01-09T18:36:13.807986Z",
          "iopub.status.idle": "2026-01-09T18:36:13.819878Z",
          "shell.execute_reply.started": "2026-01-09T18:36:13.807967Z",
          "shell.execute_reply": "2026-01-09T18:36:13.819354Z"
        },
        "id": "meDto7GoE2fD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Tool:\n",
        "\n",
        "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
        "\n",
        "        self._local_jupyter_timeout = local_jupyter_timeout\n",
        "        self._tool_prompt = tool_prompt\n",
        "        self._jupyter_session = sandbox\n",
        "\n",
        "        self._owns_session = sandbox is None\n",
        "\n",
        "        self._execution_lock = threading.Lock()\n",
        "        self._init_lock = threading.Lock()\n",
        "\n",
        "    def _ensure_session(self):\n",
        "\n",
        "        if self._jupyter_session is None:\n",
        "            with self._init_lock:\n",
        "                if self._jupyter_session is None:\n",
        "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
        "\n",
        "    def _ensure_last_print(self, code: str) -> str:\n",
        "\n",
        "        lines = code.strip().split('\\n')\n",
        "\n",
        "        if not lines:\n",
        "            return code\n",
        "\n",
        "        last_line = lines[-1].strip()\n",
        "\n",
        "        if 'print' in last_line or 'import' in last_line:\n",
        "            return code\n",
        "\n",
        "        if not last_line:\n",
        "            return code\n",
        "\n",
        "        if last_line.startswith('#'):\n",
        "            return code\n",
        "\n",
        "        lines[-1] = 'print(' + last_line + ')'\n",
        "\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    @property\n",
        "    def instruction(self) -> str:\n",
        "\n",
        "        return self._tool_prompt\n",
        "\n",
        "    @property\n",
        "    def tool_config(self) -> ToolNamespaceConfig:\n",
        "\n",
        "        return ToolNamespaceConfig(\n",
        "            name='python',\n",
        "            description=self.instruction,\n",
        "            tools=[]\n",
        "        )\n",
        "\n",
        "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
        "\n",
        "        content = TextContent(text=output)\n",
        "        author = Author(role=Role.TOOL, name='python')\n",
        "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
        "\n",
        "        if channel:\n",
        "            message = message.with_channel(channel)\n",
        "\n",
        "        return message\n",
        "\n",
        "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
        "\n",
        "        self._ensure_session()\n",
        "        raw_script = message.content[0].text\n",
        "        final_script = self._ensure_last_print(raw_script)\n",
        "        MAX_OUTPUT_LEN = 680\n",
        "\n",
        "        with self._execution_lock:\n",
        "            try:\n",
        "                output = self._jupyter_session.execute(final_script)\n",
        "                # 2. OUTPUT TRUNCATI10N\n",
        "                if len(output) > MAX_OUTPUT_LEN:\n",
        "                    output = output[:MAX_OUTPUT_LEN] + f\"\\n... [Output truncated. Total length: {len(output)} chars]\"\n",
        "            except TimeoutError as exc:\n",
        "                output = f'[ERROR] {exc}'\n",
        "\n",
        "        return [self._make_response(output, channel=message.channel)]\n",
        "\n",
        "    def close(self):\n",
        "\n",
        "        if self._jupyter_session is not None:\n",
        "            if self._owns_session:\n",
        "                self._jupyter_session.close()\n",
        "\n",
        "            self._jupyter_session = None\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        self.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:01:56.934604Z",
          "iopub.execute_input": "2026-01-09T18:01:56.935182Z",
          "iopub.status.idle": "2026-01-09T18:01:56.942376Z",
          "shell.execute_reply.started": "2026-01-09T18:01:56.935157Z",
          "shell.execute_reply": "2026-01-09T18:01:56.941913Z"
        },
        "id": "UCYY4GiaE2fE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_error_guidance(error_text: str) -> str:\n",
        "    \"\"\"Provide helpful guidance for common Python errors.\"\"\"\n",
        "    error_lower = error_text.lower()\n",
        "\n",
        "    guidance_map = {\n",
        "        'timeout': \"Computation timed out. Try a more efficient algorithm or reduce search space.\",\n",
        "        'memory': \"Memory limit exceeded. Use generators, process in batches, or reduce data size.\",\n",
        "        'killed': \"Process killed (likely memory). Reduce memory usage.\",\n",
        "        'overflow': \"Numerical overflow. Use modular arithmetic or sympy.Integer for big numbers.\",\n",
        "        'zerodivision': \"Division by zero. Add a check before dividing.\",\n",
        "        'index': \"Index out of range. Check array bounds before accessing.\",\n",
        "        'keyerror': \"Key not found. Verify dictionary keys exist before access.\",\n",
        "        'typeerror': \"Type mismatch. Check operation validity for data types used.\",\n",
        "        'syntax': \"Syntax error. Check for missing colons, parentheses, or indentation.\",\n",
        "        'nameerror': \"Undefined variable. Ensure all variables are defined before use.\",\n",
        "        'recursion': \"Recursion limit. Use iteration instead or sys.setrecursionlimit().\",\n",
        "    }\n",
        "\n",
        "    for keyword, guidance in guidance_map.items():\n",
        "        if keyword in error_lower:\n",
        "            return guidance\n",
        "\n",
        "    return \"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T18:01:57.140588Z",
          "iopub.execute_input": "2026-01-09T18:01:57.140835Z",
          "iopub.status.idle": "2026-01-09T18:01:57.144706Z",
          "shell.execute_reply.started": "2026-01-09T18:01:57.140819Z",
          "shell.execute_reply": "2026-01-09T18:01:57.144262Z"
        },
        "id": "wrrNqsYVE2fE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AIMO3Solver:\n",
        "\n",
        "    def __init__(self, cfg, port: int = 8000):\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.port = port\n",
        "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
        "        self.api_key = 'sk-local'\n",
        "        self.template = AIMO3Template()\n",
        "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
        "\n",
        "        self._preload_model_weights()\n",
        "\n",
        "        self.server_process = self._start_server()\n",
        "\n",
        "        self.client = OpenAI(\n",
        "            base_url=self.base_url,\n",
        "            api_key=self.api_key,\n",
        "            timeout=self.cfg.session_timeout\n",
        "        )\n",
        "\n",
        "        self._wait_for_server()\n",
        "        self._initialize_kernels()\n",
        "\n",
        "        self.notebook_start_time = time.time()\n",
        "        self.problems_remaining = 50\n",
        "\n",
        "    def _get_attempt_config(self, attempt_index: int, problem: str) -> dict:\n",
        "        \"\"\"Get configuration for a specific attempt.\"\"\"\n",
        "\n",
        "        # Rotate through prompt ensembles\n",
        "        num_system = len(self.cfg.system_prompts)\n",
        "        num_dev = len(self.cfg.developer_prompts)\n",
        "        num_pref = len(self.cfg.preference_prompts)\n",
        "\n",
        "        system_prompt = self.cfg.system_prompts[attempt_index % num_system]\n",
        "        pref_prompt = self.cfg.preference_prompts[attempt_index % num_pref]\n",
        "        developer_prompt = self.cfg.developer_prompts[attempt_index % num_dev]\n",
        "\n",
        "        # Temperature variation: first half lower, second half higher\n",
        "        base_temp = self.cfg.base_temperature\n",
        "        half_attempts = self.cfg.attempts // 2\n",
        "\n",
        "        if attempt_index < half_attempts:\n",
        "            # Lower temperature for more deterministic reasoning\n",
        "            temp_offset = -0.25 * (attempt_index % 3)\n",
        "            temperature = max(self.cfg.temp_low, base_temp + temp_offset)\n",
        "        else:\n",
        "            # Higher temperature for more creative approaches\n",
        "            temp_offset = 0.25 * ((attempt_index - half_attempts) % 3)\n",
        "            temperature = min(self.cfg.temp_high, base_temp + temp_offset)\n",
        "\n",
        "        # Better seed variation (avoid hash collisions)\n",
        "        # seed = self.cfg.seed + attempt_index * 1000 + hash(problem[:50]) % 1000\n",
        "        seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
        "\n",
        "        return {\n",
        "            'system_prompt': system_prompt,\n",
        "            'developer_prompt': developer_prompt,\n",
        "            'preference_prompt': pref_prompt,\n",
        "            'temperature': temperature,\n",
        "            'seed': seed,\n",
        "        }\n",
        "\n",
        "\n",
        "    def _preload_model_weights(self) -> None:\n",
        "\n",
        "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        files_to_load = []\n",
        "        total_size = 0\n",
        "\n",
        "        for root, _, files in os.walk(self.cfg.model_path):\n",
        "            for file_name in files:\n",
        "                file_path = os.path.join(root, file_name)\n",
        "\n",
        "                if os.path.isfile(file_path):\n",
        "                    files_to_load.append(file_path)\n",
        "                    total_size += os.path.getsize(file_path)\n",
        "\n",
        "        def _read_file(path: str) -> None:\n",
        "\n",
        "            with open(path, 'rb') as file_object:\n",
        "                while file_object.read(1024 * 1024 * 1024):\n",
        "                    pass\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            list(executor.map(_read_file, files_to_load))\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
        "\n",
        "    def _start_server(self) -> subprocess.Popen:\n",
        "\n",
        "        cmd = [\n",
        "            sys.executable,\n",
        "            '-m',\n",
        "            'vllm.entrypoints.openai.api_server',\n",
        "            '--seed',\n",
        "            str(self.cfg.seed),\n",
        "            '--model',\n",
        "            self.cfg.model_path,\n",
        "            '--served-model-name',\n",
        "            self.cfg.served_model_name,\n",
        "            '--tensor-parallel-size',\n",
        "            '1',\n",
        "            '--max-num-seqs',\n",
        "            str(self.cfg.batch_size),\n",
        "            '--gpu-memory-utilization',\n",
        "            str(self.cfg.gpu_memory_utilization),\n",
        "            '--host',\n",
        "            '0.0.0.0',\n",
        "            '--port',\n",
        "            str(self.port),\n",
        "            '--dtype',\n",
        "            self.cfg.dtype,\n",
        "            '--kv-cache-dtype',\n",
        "            self.cfg.kv_cache_dtype,\n",
        "            '--max-model-len',\n",
        "            str(self.cfg.context_tokens),\n",
        "            '--stream-interval',\n",
        "            str(self.cfg.stream_interval),\n",
        "            '--enable-prefix-caching'\n",
        "        ]\n",
        "\n",
        "        self.log_file = open('vllm_server.log', 'w')\n",
        "\n",
        "        return subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=self.log_file,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            start_new_session=True\n",
        "        )\n",
        "\n",
        "    def _wait_for_server(self):\n",
        "\n",
        "        print('Waiting for vLLM server...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        for _ in range(self.cfg.server_timeout):\n",
        "            return_code = self.server_process.poll()\n",
        "\n",
        "            if return_code is not None:\n",
        "                self.log_file.flush()\n",
        "\n",
        "                with open('vllm_server.log', 'r') as log_file:\n",
        "                    logs = log_file.read()\n",
        "\n",
        "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
        "\n",
        "            try:\n",
        "                self.client.models.list()\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
        "\n",
        "                return\n",
        "\n",
        "            except Exception:\n",
        "                time.sleep(1)\n",
        "\n",
        "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
        "\n",
        "    def _initialize_kernels(self) -> None:\n",
        "\n",
        "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.sandbox_pool = queue.Queue()\n",
        "\n",
        "        def _create_sandbox():\n",
        "\n",
        "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
        "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                self.sandbox_pool.put(future.result())\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
        "\n",
        "    def _scan_for_answer(self, text: str) -> int | None:\n",
        "\n",
        "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        if matches:\n",
        "            try:\n",
        "                clean_value = matches[-1].replace(',', '')\n",
        "                value = int(clean_value)\n",
        "\n",
        "                if 0 <= value <= 99999:\n",
        "                    return value\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "\n",
        "        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "\n",
        "        if matches:\n",
        "            try:\n",
        "                clean_value = matches[-1].replace(',', '')\n",
        "                value = int(clean_value)\n",
        "\n",
        "                if 0 <= value <= 99999:\n",
        "                    return value\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def _scan_for_probability(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Scans for the probability format \\probability{0.95}\n",
        "        Returns 0.0 if not found or invalid.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return 0.0\n",
        "\n",
        "        # Pattern looks for \\probability{...}\n",
        "        # It captures content inside the curly braces\n",
        "        pattern = r\"\\\\probability\\{(.*?)\\}\"\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        if not matches:\n",
        "            return 0.0\n",
        "\n",
        "        # We take the LAST probability output, just like we do for boxed answers\n",
        "        last_match = matches[-1].strip()\n",
        "\n",
        "        try:\n",
        "            prob = float(last_match)\n",
        "            # Clamp between 0 and 1 just in case\n",
        "            return max(0.0, min(1.0, prob))\n",
        "        except ValueError:\n",
        "            return 0.0\n",
        "\n",
        "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
        "\n",
        "        if not logprobs_buffer:\n",
        "            return float('inf')\n",
        "\n",
        "        total_entropy = 0.0\n",
        "        token_count = 0\n",
        "\n",
        "        for top_logprobs_dict in logprobs_buffer:\n",
        "\n",
        "            if not isinstance(top_logprobs_dict, dict):\n",
        "                continue\n",
        "\n",
        "            if not top_logprobs_dict:\n",
        "                continue\n",
        "\n",
        "            token_entropy = 0.0\n",
        "\n",
        "            for token_str, log_prob in top_logprobs_dict.items():\n",
        "                prob = math.exp(log_prob)\n",
        "\n",
        "                if prob > 0:\n",
        "                    token_entropy -= prob * math.log2(prob)\n",
        "\n",
        "            total_entropy += token_entropy\n",
        "            token_count += 1\n",
        "\n",
        "        if token_count == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        return total_entropy / token_count\n",
        "\n",
        "\n",
        "    def _process_attempt_v2(\n",
        "        self,\n",
        "        problem: str,\n",
        "        system_prompt: str,\n",
        "        attempt_index: int,\n",
        "        stop_event: threading.Event,\n",
        "        deadline: float\n",
        "    ) -> dict:\n",
        "        \"\"\"Process a single attempt with detailed logging for analysis.\"\"\"\n",
        "\n",
        "        attempt_start = time.time()\n",
        "\n",
        "        if stop_event.is_set() or time.time() > deadline:\n",
        "            return {\n",
        "                'Attempt': attempt_index + 1,\n",
        "                'Answer': None,\n",
        "                'Python Calls': 0,\n",
        "                'Python Errors': 0,\n",
        "                'Response Length': 0,\n",
        "                'Turns': 0,\n",
        "                'Time': 0,\n",
        "                'Exit Reason': 'skipped',\n",
        "            }\n",
        "\n",
        "        # Get attempt-specific configuration\n",
        "        config = self._get_attempt_config(attempt_index, problem)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ðŸš€ ATTEMPT {attempt_index + 1} STARTED\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Temperature: {config['temperature']:.2f}\")\n",
        "        print(f\"  Seed: {config['seed']}\")\n",
        "        print(f\"  System prompt: {config['system_prompt'][:80]}...\")\n",
        "        print(f\"  Preference: {config['preference_prompt'][:60]}...\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        local_tool = None\n",
        "        sandbox = None\n",
        "        python_calls = 0\n",
        "        python_errors = 0\n",
        "        total_tokens = 0\n",
        "        final_answer = None\n",
        "        final_prob = 0.0\n",
        "        consecutive_errors = 0\n",
        "        max_consecutive_errors = 3\n",
        "        turn_count = 0\n",
        "        exit_reason = 'unknown'\n",
        "        search_text = ''\n",
        "\n",
        "        # Track per-turn data for analysis\n",
        "        turn_log = []\n",
        "\n",
        "        try:\n",
        "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
        "\n",
        "            local_tool = AIMO3Tool(\n",
        "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
        "                tool_prompt=self.cfg.tool_prompt,\n",
        "                sandbox=sandbox\n",
        "            )\n",
        "\n",
        "            # Build prompt with preference\n",
        "            full_problem = f\"{problem} {config['preference_prompt']}\"\n",
        "\n",
        "            encoding = self.encoding\n",
        "            messages = self.template.apply_chat_template(\n",
        "                config['system_prompt'],\n",
        "                config['developer_prompt'],\n",
        "                full_problem,\n",
        "                local_tool.tool_config\n",
        "            )\n",
        "\n",
        "            conversation = Conversation.from_messages(messages)\n",
        "            waiting_for_summary = False\n",
        "\n",
        "            for turn in range(self.cfg.turns):\n",
        "                turn_start = time.time()\n",
        "                turn_count = turn + 1\n",
        "\n",
        "                if stop_event.is_set():\n",
        "                    exit_reason = 'early_stop_signal'\n",
        "                    print(f\"  [Turn {turn_count}] âš¡ Early stop signal received\")\n",
        "                    break\n",
        "\n",
        "                if time.time() > deadline:\n",
        "                    exit_reason = 'timeout'\n",
        "                    print(f\"  [Turn {turn_count}] â° Deadline exceeded\")\n",
        "                    break\n",
        "\n",
        "                prompt_ids = encoding.render_conversation_for_completion(\n",
        "                    conversation, Role.ASSISTANT\n",
        "                )\n",
        "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
        "\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # ðŸ§  SMART CONTEXT MANAGEMENT (Summarize -> Prune)\n",
        "                # ------------------------------------------------------------------\n",
        "\n",
        "                current_tokens = len(prompt_ids)\n",
        "                TOKEN_LIMIT = self.cfg.context_tokens  # Adjust to your model (e.g. 32000 for deepseek/qwen)\n",
        "\n",
        "                # 2. Check if we just received the summary we asked for\n",
        "                if waiting_for_summary:\n",
        "                    print(f\"  [Turn {turn}] ðŸ“‰ Summary received. Compressing history...\")\n",
        "\n",
        "                    # The last message is the Model's Summary.\n",
        "                    # The message before that was our \"Please Summarize\" request.\n",
        "                    # We want to keep: System(0), Problem(1), and the Summary(-1).\n",
        "\n",
        "                    summary_content = conversation.messages[-1].content[0].text\n",
        "                    print(f\" Summary content : {summary_content}\")\n",
        "\n",
        "                    # FIXED: Use TextContent + List for the bridge message\n",
        "                    # state_msg = Message(\n",
        "                    #     role=Role.USER,\n",
        "                    #     content=[\n",
        "                    #         TextContent(text=(\n",
        "                    #             f\"--- PREVIOUS WORK SUMMARY ---\\n\"\n",
        "                    #             f\"{summary_content}\\n\"\n",
        "                    #             f\"-----------------------------\\n\"\n",
        "                    #             f\"[SYSTEM: Memory cleared. The summary above contains all derived facts. \"\n",
        "                    #             f\"All Python variables are still active. Proceed with the next step.]\"\n",
        "                    #         ))\n",
        "                    #     ]\n",
        "                    # )\n",
        "\n",
        "                    state_msg = Message.from_role_and_content(\n",
        "                        Role.USER,\n",
        "                        f\"--- PREVIOUS WORK SUMMARY ---\\n\"\n",
        "                        f\"{summary_content}\\n\"\n",
        "                        f\"-----------------------------\\n\"\n",
        "                        f\"[SYSTEM: Memory cleared. The summary above contains all derived facts. \"\n",
        "                        f\"All Python variables are still active. Proceed with the next step.]\"\n",
        "                    )\n",
        "\n",
        "                    # Rebuild History: System -> Problem -> Summary State\n",
        "                    conversation.messages = [\n",
        "                        conversation.messages[0],\n",
        "                        conversation.messages[1],\n",
        "                        state_msg\n",
        "                    ]\n",
        "\n",
        "                    waiting_for_summary = False\n",
        "\n",
        "                    # Re-render tokens since history changed\n",
        "                    prompt_ids = encoding.render_conversation_for_completion(\n",
        "                        conversation, Role.ASSISTANT\n",
        "                    )\n",
        "\n",
        "                # 3. Trigger Summarization if Context is Full (and we aren't already waiting)\n",
        "                elif current_tokens > (TOKEN_LIMIT * 1.85):\n",
        "                    print(f\"  [Turn {turn}] ðŸ§¹ Context full ({current_tokens} tok). Requesting summary...\")\n",
        "\n",
        "                    # # FIXED: Use TextContent + List for the request message\n",
        "                    # summary_request = Message(\n",
        "                    #     role=Role.USER,\n",
        "                    #     content=[\n",
        "                    #         TextContent(text=(\n",
        "                    #             \"âš ï¸ SYSTEM ALERT: MEMORY LIMIT REACHED.\\n\"\n",
        "                    #             \"We must clear the context window.\\n\\n\"\n",
        "                    #             \"Please provide a CONCISE SUMMARY of the current state.\\n\"\n",
        "                    #             \"1. List all verified variable values (e.g., n=5, k=12).\\n\"\n",
        "                    #             \"2. State the last derived mathematical fact.\\n\"\n",
        "                    #             \"3. State the immediate next step.\\n\"\n",
        "                    #             \"DO NOT output Python code. Just text.\"\n",
        "                    #         ))\n",
        "                    #     ]\n",
        "                    # )\n",
        "\n",
        "                    # FIXED: Request summary when context is full\n",
        "                    summary_request = Message.from_role_and_content(\n",
        "                        Role.USER,\n",
        "                        \"âš ï¸ SYSTEM ALERT: MEMORY LIMIT REACHED.\\n\"\n",
        "                        \"We must clear the context window.\\n\\n\"\n",
        "                        \"Please provide a CONCISE SUMMARY of the current state.\\n\"\n",
        "                        \"1. List all verified variable values (e.g., n=5, k=12).\\n\"\n",
        "                        \"2. State the last derived mathematical fact.\\n\"\n",
        "                        \"3. State the immediate next step.\\n\"\n",
        "                        \"DO NOT output Python code. Just text.\"\n",
        "                    )\n",
        "\n",
        "                    conversation.messages.append(summary_request)\n",
        "                    waiting_for_summary = True\n",
        "\n",
        "                    # We continue the loop so the model can generate the summary in this turn\n",
        "                    # The NEXT iteration will catch it in step #2 above and prune.\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "                if max_tokens < self.cfg.buffer_tokens:\n",
        "                    exit_reason = 'context_full'\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ“¦ Context window full ({len(prompt_ids)} tokens used)\")\n",
        "                    break\n",
        "\n",
        "                print(f\"  [Turn {turn_count}] ðŸ”„ Generating... (context: {len(prompt_ids)} tokens, max_new: {max_tokens})\")\n",
        "\n",
        "                stream = self.client.completions.create(\n",
        "                    model=self.cfg.served_model_name,\n",
        "                    temperature=config['temperature'],\n",
        "                    max_tokens=max_tokens,\n",
        "                    prompt=prompt_ids,\n",
        "                    seed=config['seed'],\n",
        "                    stream=True,\n",
        "                    extra_body={\n",
        "                        'min_p': self.cfg.min_p,\n",
        "                        'stop_token_ids': self.stop_token_ids,\n",
        "                        'return_token_ids': True\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                turn_tokens = 0\n",
        "                try:\n",
        "                    token_buffer = []\n",
        "                    text_chunks = []\n",
        "                    num_answers_found = 0\n",
        "                    answers_found = []\n",
        "\n",
        "\n",
        "                    for chunk in stream:\n",
        "                        if stop_event.is_set() or time.time() > deadline:\n",
        "                            break\n",
        "\n",
        "                        new_tokens = chunk.choices[0].token_ids\n",
        "                        new_text = chunk.choices[0].text\n",
        "\n",
        "                        if new_tokens:\n",
        "                            token_buffer.extend(new_tokens)\n",
        "                            total_tokens += len(new_tokens)\n",
        "                            turn_tokens += len(new_tokens)\n",
        "                            text_chunks.append(new_text)\n",
        "\n",
        "                        # Early answer detection during streaming\n",
        "                        if '}' in new_text:\n",
        "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
        "                            answer = self._scan_for_answer(search_text)\n",
        "\n",
        "                            if answer is not None:\n",
        "                                final_answer = answer\n",
        "                                final_prob = self._scan_for_probability(search_text)\n",
        "                                num_answers_found += 1\n",
        "                                answers_found.append(final_answer)\n",
        "\n",
        "                                print(f\"  [Turn {turn_count}] ðŸŽ¯ Answer found in stream: {answer}, num_answers_found: {num_answers_found}\")\n",
        "\n",
        "                                if num_answers_found > 2:\n",
        "                                    exit_reason = 'answer_found_streaming'\n",
        "                                    print(answers_found)\n",
        "                                    print(f\"  [Turn {turn_count}] ðŸŽ¯ Answer found in stream: {answer}\")\n",
        "                                    break\n",
        "\n",
        "                finally:\n",
        "                    stream.close()\n",
        "\n",
        "                turn_time = time.time() - turn_start\n",
        "                full_response = ''.join(text_chunks)\n",
        "\n",
        "                # Log turn summary\n",
        "                turn_info = {\n",
        "                    'turn': turn_count,\n",
        "                    'tokens': turn_tokens,\n",
        "                    'time': turn_time,\n",
        "                    'response_preview': full_response[:200] if full_response else '',\n",
        "                }\n",
        "                turn_log.append(turn_info)\n",
        "\n",
        "                if final_answer is not None:\n",
        "                    break\n",
        "\n",
        "                if not token_buffer:\n",
        "                    exit_reason = 'empty_response'\n",
        "                    print(f\"  [Turn {turn_count}] âš ï¸ Empty response from model\")\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    new_messages = encoding.parse_messages_from_completion_tokens(\n",
        "                        token_buffer, Role.ASSISTANT\n",
        "                    )\n",
        "                except Exception:\n",
        "                    # If parsing fails (e.g. due to interruption/EOS),\n",
        "                    # we just ignore this incomplete turn and stop.\n",
        "                    print(f\"  [Turn {turn}] âš ï¸ Parsing failed (incomplete output ignored)\")\n",
        "                    break\n",
        "\n",
        "                conversation.messages.extend(new_messages)\n",
        "                # Safety check: ensure we actually got messages back\n",
        "                if not new_messages:\n",
        "                    break\n",
        "\n",
        "                last_message = new_messages[-1]\n",
        "\n",
        "                # Log what the model is doing\n",
        "                if last_message.channel == 'final':\n",
        "                    answer_text = last_message.content[0].text\n",
        "                    final_answer = self._scan_for_answer(answer_text)\n",
        "                    final_prob = self._scan_for_probability(search_text)\n",
        "                    exit_reason = 'final_channel'\n",
        "\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ“ Final response ({turn_tokens} tokens, {turn_time:.1f}s)\")\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ’¬ Content: {answer_text[:300]}{'...' if len(answer_text) > 300 else ''}\")\n",
        "\n",
        "                    if final_answer is not None:\n",
        "                        print(f\"  [Turn {turn_count}] ðŸŽ¯ Answer extracted: {final_answer}\")\n",
        "                    else:\n",
        "                        print(f\"  [Turn {turn_count}] âŒ No valid answer found in response\")\n",
        "                    break\n",
        "\n",
        "                # Handle Python tool calls\n",
        "                if last_message.recipient == 'python':\n",
        "                    python_calls += 1\n",
        "                    code_content = last_message.content[0].text if last_message.content else \"N/A\"\n",
        "\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ Python call #{python_calls} ({turn_tokens} tokens, {turn_time:.1f}s)\")\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ“„ Code:\\n{'â”€'*40}\")\n",
        "                    # Indent code for readability\n",
        "                    for line in code_content.split('\\n')[:15]:  # First 15 lines\n",
        "                        print(f\"    {line}\")\n",
        "                    if code_content.count('\\n') > 15:\n",
        "                        print(f\"    ... ({code_content.count(chr(10)) - 15} more lines)\")\n",
        "                    print(f\"{'â”€'*40}\")\n",
        "\n",
        "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
        "                    response_text = tool_responses[0].content[0].text\n",
        "\n",
        "                    is_error = (\n",
        "                        response_text.startswith('[ERROR]') or\n",
        "                        'Traceback' in response_text or\n",
        "                        'Error:' in response_text or\n",
        "                        'Exception' in response_text\n",
        "                    )\n",
        "\n",
        "                    if is_error:\n",
        "                        python_errors += 1\n",
        "                        consecutive_errors += 1\n",
        "\n",
        "                        print(f\"  [Turn {turn_count}] âŒ Error #{python_errors} (consecutive: {consecutive_errors})\")\n",
        "                        print(f\"  [Turn {turn_count}] ðŸ“„ Error output:\\n{'â”€'*40}\")\n",
        "                        for line in response_text.split('\\n')[:10]:\n",
        "                            print(f\"    {line}\")\n",
        "                        print(f\"{'â”€'*40}\")\n",
        "\n",
        "                        # Add error guidance\n",
        "                        if consecutive_errors <= max_consecutive_errors:\n",
        "                            guidance = get_error_guidance(response_text)\n",
        "                            if guidance:\n",
        "                                print(f\"  [Turn {turn_count}] ðŸ’¡ Guidance: {guidance}\")\n",
        "                                enhanced_response = f\"{response_text}\\n\\nHint: {guidance}\"\n",
        "                                tool_responses[0].content[0].text = enhanced_response\n",
        "\n",
        "                        if (consecutive_errors >= max_consecutive_errors) and (consecutive_errors < max_consecutive_errors + 2):\n",
        "                            enhanced_response = f\"{response_text}\\n\\nHint: Take it easy. Reset everything and approach the problem from a fresh perspective.\"\n",
        "                            tool_responses[0].content[0].text = enhanced_response\n",
        "\n",
        "                            # force_msg = Message(\n",
        "                            #     role=Role.USER,  # Use the Enum if possible, or try 'user'\n",
        "                            #     content=[\n",
        "                            #         TextContent(text=(\n",
        "                            #             \"STOP. You have triggered 3 consecutive Python errors. \"\n",
        "                            #             \"Take it easy. Reset everything and approach the problem from a fresh perspective.\"\n",
        "                            #         ))\n",
        "                            #     ]\n",
        "                            # )\n",
        "\n",
        "                            # FIXED: Suggest fresh approach after consecutive errors\n",
        "                            force_msg = Message.from_role_and_content(\n",
        "                                Role.USER,\n",
        "                                \"STOP. You have triggered 3 consecutive Python errors. \"\n",
        "                                \"Take it easy. Reset everything and approach the problem from a fresh perspective.\"\n",
        "                            )\n",
        "\n",
        "                            conversation.messages.append(force_msg)\n",
        "                            print(f\"  [Turn {turn_count}] ðŸ”„ Suggesting fresh approach\")\n",
        "\n",
        "                        if consecutive_errors >= max_consecutive_errors + 2:\n",
        "                            exit_reason = 'too_many_errors'\n",
        "                            print(f\"  [Turn {turn_count}] ðŸ›‘ Too many consecutive errors, stopping\")\n",
        "                            break\n",
        "                    else:\n",
        "                        consecutive_errors = 0\n",
        "\n",
        "                        # Show successful output (truncated)\n",
        "                        print(f\"  [Turn {turn_count}] âœ… Success! Output:\")\n",
        "                        print(f\"{'â”€'*40}\")\n",
        "                        output_lines = response_text.split('\\n')\n",
        "                        for line in output_lines[:8]:\n",
        "                            print(f\"    {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
        "                        if len(output_lines) > 8:\n",
        "                            print(f\"    ... ({len(output_lines) - 8} more lines)\")\n",
        "                        print(f\"{'â”€'*40}\")\n",
        "\n",
        "                        # Check if output contains a potential answer\n",
        "                        potential_answer = self._scan_for_answer(response_text)\n",
        "                        if potential_answer is not None:\n",
        "                            print(f\"  [Turn {turn_count}] ðŸ‘€ Potential answer in output: {potential_answer}\")\n",
        "\n",
        "                    conversation.messages.extend(tool_responses)\n",
        "                else:\n",
        "                    # Model did something else (reasoning, etc.)\n",
        "                    print(f\"  [Turn {turn_count}] ðŸ’­ Model response ({turn_tokens} tokens, {turn_time:.1f}s)\")\n",
        "                    print(f\"  [Turn {turn_count}] Channel: {last_message.channel}, Recipient: {last_message.recipient}\")\n",
        "                    if last_message.content:\n",
        "                        preview = last_message.content[0].text[:200] if hasattr(last_message.content[0], 'text') else str(last_message.content[0])[:200]\n",
        "                        print(f\"  [Turn {turn_count}] Preview: {preview}...\")\n",
        "\n",
        "            else:\n",
        "                # Loop completed without break\n",
        "                exit_reason = 'max_turns'\n",
        "                print(f\"  âš ï¸ Reached max turns ({self.cfg.turns})\")\n",
        "\n",
        "        except Exception as exc:\n",
        "            import traceback\n",
        "            python_errors += 1\n",
        "            exit_reason = f'exception:{type(exc).__name__}'\n",
        "\n",
        "            print(f\"\\n{'!'*60}\")\n",
        "            print(f\"âŒ EXCEPTION in attempt {attempt_index + 1}\")\n",
        "            print(f\"{'!'*60}\")\n",
        "            print(f\"Type: {type(exc).__name__}\")\n",
        "            print(f\"Message: {exc}\")\n",
        "            print(f\"Traceback:\")\n",
        "            traceback.print_exc()\n",
        "            print(f\"{'!'*60}\\n\")\n",
        "\n",
        "        finally:\n",
        "            if local_tool is not None:\n",
        "                local_tool.close()\n",
        "\n",
        "            if sandbox is not None:\n",
        "                sandbox.reset()\n",
        "                self.sandbox_pool.put(sandbox)\n",
        "\n",
        "        attempt_time = time.time() - attempt_start\n",
        "\n",
        "        # Print attempt summary\n",
        "        print(f\"\\n{'â”€'*60}\")\n",
        "        print(f\"ðŸ“Š ATTEMPT {attempt_index + 1} SUMMARY\")\n",
        "        print(f\"{'â”€'*60}\")\n",
        "        print(f\"  Answer: {final_answer if final_answer is not None else 'None'}\")\n",
        "        print(f\"  Prob: {final_prob if final_prob is not None else 'None'}\")\n",
        "        print(f\"  Exit Reason: {exit_reason}\")\n",
        "        print(f\"  Turns: {turn_count}\")\n",
        "        print(f\"  Total Tokens: {total_tokens}\")\n",
        "        print(f\"  Python Calls: {python_calls}\")\n",
        "        print(f\"  Python Errors: {python_errors}\")\n",
        "        print(f\"  Time: {attempt_time:.1f}s\")\n",
        "        print(f\"  Tokens/sec: {total_tokens/attempt_time:.1f}\" if attempt_time > 0 else \"  Tokens/sec: N/A\")\n",
        "        print(f\"{'â”€'*60}\\n\")\n",
        "\n",
        "        return {\n",
        "            'Attempt': attempt_index + 1,\n",
        "            'Answer': final_answer,\n",
        "            'Prob' : final_prob,\n",
        "            'Python Calls': python_calls,\n",
        "            'Python Errors': python_errors,\n",
        "            'Response Length': total_tokens,\n",
        "            'Turns': turn_count,\n",
        "            'Time': round(attempt_time, 1),\n",
        "            'Exit Reason': exit_reason,\n",
        "            'Temperature': config['temperature'],\n",
        "            'System prompt': config[\"system_prompt\"],\n",
        "            'Dev prompt': config[\"developer_prompt\"],\n",
        "            'Pref prompt': config[\"preference_prompt\"],\n",
        "            'Final Text': search_text,\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _process_attempt(\n",
        "        self,\n",
        "        problem: str,\n",
        "        system_prompt: str,\n",
        "        attempt_index: int,\n",
        "        stop_event: threading.Event,\n",
        "        deadline: float\n",
        "    ) -> dict:\n",
        "        \"\"\"Process a single attempt with improved error handling.\"\"\"\n",
        "\n",
        "        if stop_event.is_set() or time.time() > deadline:\n",
        "            return {\n",
        "                'Attempt': attempt_index + 1,\n",
        "                'Answer': None,\n",
        "                'Python Calls': 0,\n",
        "                'Python Errors': 0,\n",
        "                'Response Length': 0,\n",
        "                'Confidence': 0.0,\n",
        "            }\n",
        "\n",
        "        # Get attempt-specific configuration\n",
        "        config = self._get_attempt_config(attempt_index, problem)\n",
        "\n",
        "        local_tool = None\n",
        "        sandbox = None\n",
        "        python_calls = 0\n",
        "        python_errors = 0\n",
        "        total_tokens = 0\n",
        "        final_answer = None\n",
        "        consecutive_errors = 0\n",
        "        max_consecutive_errors = 3\n",
        "        search_text = ''\n",
        "\n",
        "        try:\n",
        "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
        "\n",
        "            local_tool = AIMO3Tool(\n",
        "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
        "                tool_prompt=self.cfg.tool_prompt,\n",
        "                sandbox=sandbox\n",
        "            )\n",
        "\n",
        "            # Build prompt with preference\n",
        "            full_problem = f\"{problem} {config['preference_prompt']}\"\n",
        "\n",
        "            encoding = self.encoding\n",
        "            messages = self.template.apply_chat_template(\n",
        "                config['system_prompt'],\n",
        "                config['developer_prompt'],\n",
        "                full_problem,\n",
        "                local_tool.tool_config\n",
        "            )\n",
        "\n",
        "            conversation = Conversation.from_messages(messages)\n",
        "\n",
        "            for turn in range(self.cfg.turns):\n",
        "                if stop_event.is_set() or time.time() > deadline:\n",
        "                    break\n",
        "\n",
        "                prompt_ids = encoding.render_conversation_for_completion(\n",
        "                    conversation, Role.ASSISTANT\n",
        "                )\n",
        "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
        "\n",
        "                if max_tokens < self.cfg.buffer_tokens:\n",
        "                    break\n",
        "\n",
        "                stream = self.client.completions.create(\n",
        "                    model=self.cfg.served_model_name,\n",
        "                    temperature=config['temperature'],  # Use varied temperature\n",
        "                    max_tokens=max_tokens,\n",
        "                    prompt=prompt_ids,\n",
        "                    seed=config['seed'],\n",
        "                    stream=True,\n",
        "                    extra_body={\n",
        "                        'min_p': self.cfg.min_p,\n",
        "                        'stop_token_ids': self.stop_token_ids,\n",
        "                        'return_token_ids': True\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    token_buffer = []\n",
        "                    text_chunks = []\n",
        "\n",
        "                    for chunk in stream:\n",
        "                        if stop_event.is_set() or time.time() > deadline:\n",
        "                            break\n",
        "\n",
        "                        new_tokens = chunk.choices[0].token_ids\n",
        "                        new_text = chunk.choices[0].text\n",
        "\n",
        "                        if new_tokens:\n",
        "                            token_buffer.extend(new_tokens)\n",
        "                            total_tokens += len(new_tokens)\n",
        "                            text_chunks.append(new_text)\n",
        "\n",
        "                        # Early answer detection during streaming\n",
        "                        if '}' in new_text:\n",
        "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
        "                            answer = self._scan_for_answer(search_text)\n",
        "\n",
        "                            if answer is not None:\n",
        "                                final_answer = answer\n",
        "                                break\n",
        "\n",
        "                finally:\n",
        "                    stream.close()\n",
        "\n",
        "                if final_answer is not None:\n",
        "                    break\n",
        "\n",
        "                if not token_buffer:\n",
        "                    break\n",
        "\n",
        "                new_messages = encoding.parse_messages_from_completion_tokens(\n",
        "                    token_buffer, Role.ASSISTANT\n",
        "                )\n",
        "                conversation.messages.extend(new_messages)\n",
        "                last_message = new_messages[-1]\n",
        "                # print(last_message)\n",
        "\n",
        "                if last_message.channel == 'final':\n",
        "                    answer_text = last_message.content[0].text\n",
        "                    # print(f\"Answer text {answer_text}\")\n",
        "                    final_answer = self._scan_for_answer(answer_text)\n",
        "                    break\n",
        "\n",
        "                # Handle Python tool calls with error recovery\n",
        "                if last_message.recipient == 'python':\n",
        "                    python_calls += 1\n",
        "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
        "                    response_text = tool_responses[0].content[0].text\n",
        "\n",
        "                    is_error = (\n",
        "                        response_text.startswith('[ERROR]') or\n",
        "                        'Traceback' in response_text or\n",
        "                        'Error:' in response_text or\n",
        "                        'Exception' in response_text\n",
        "                    )\n",
        "\n",
        "                    if is_error:\n",
        "                        python_errors += 1\n",
        "                        consecutive_errors += 1\n",
        "\n",
        "                        # Add error guidance if we haven't hit max consecutive errors\n",
        "                        if consecutive_errors <= max_consecutive_errors:\n",
        "                            guidance = get_error_guidance(response_text)\n",
        "                            if guidance:\n",
        "                                enhanced_response = f\"{response_text}\\n\\nHint: {guidance}\"\n",
        "                                tool_responses[0].content[0].text = enhanced_response\n",
        "\n",
        "                        # If too many consecutive errors, might be stuck\n",
        "                        if (consecutive_errors >= max_consecutive_errors) and (consecutive_errors < max_consecutive_errors + 2):\n",
        "                            enhanced_response = f\"{response_text}\\n\\nHint: Take it easy. Reset everything and approach the problem from a fresh perspective.\"\n",
        "                            tool_responses[0].content[0].text = enhanced_response\n",
        "\n",
        "                        # If too many consecutive errors, might be stuck\n",
        "                        if consecutive_errors >= max_consecutive_errors + 2:\n",
        "                            break\n",
        "\n",
        "                    else:\n",
        "                        consecutive_errors = 0  # Reset on success\n",
        "\n",
        "                    conversation.messages.extend(tool_responses)\n",
        "\n",
        "        except Exception as exc:\n",
        "            import traceback\n",
        "            python_errors += 1\n",
        "\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"ERROR in attempt {attempt_index + 1}\")\n",
        "            print(f\"{'='*50}\")\n",
        "            print(f\"Exception type: {type(exc).__name__}\")\n",
        "            print(f\"Exception message: {exc}\")\n",
        "            print(f\"Traceback:\")\n",
        "            traceback.print_exc()\n",
        "            print(f\"{'='*50}\")\n",
        "            print(f\"State at failure:\")\n",
        "            print(f\"  - Python calls: {python_calls}\")\n",
        "            print(f\"  - Python errors: {python_errors}\")\n",
        "            print(f\"  - Total tokens: {total_tokens}\")\n",
        "            print(f\"  - Final answer: {final_answer}\")\n",
        "            print(f\"  - Time remaining: {deadline - time.time():.1f}s\")\n",
        "            print(f\"{'='*50}\\n\")\n",
        "\n",
        "        finally:\n",
        "            if local_tool is not None:\n",
        "                local_tool.close()\n",
        "\n",
        "            if sandbox is not None:\n",
        "                sandbox.reset()\n",
        "                self.sandbox_pool.put(sandbox)\n",
        "\n",
        "        ## Config info\n",
        "        return {\n",
        "            'Attempt': attempt_index + 1,\n",
        "            'Response Length': total_tokens,\n",
        "            'Python Calls': python_calls,\n",
        "            'Python Errors': python_errors,\n",
        "            'Answer': final_answer,\n",
        "            'Temperature': config[\"temperature\"],\n",
        "            'System prompt': config[\"system_prompt\"],\n",
        "            'Dev prompt': config[\"developer_prompt\"],\n",
        "            'Pref prompt': config[\"preference_prompt\"],\n",
        "            'Final Text': search_text,\n",
        "\n",
        "        }\n",
        "\n",
        "    def _select_answer(self, detailed_results: list) -> int:\n",
        "\n",
        "        stats = defaultdict(lambda: {'votes': 0, 'calls': 0})\n",
        "\n",
        "        for result in detailed_results:\n",
        "            answer = result['Answer']\n",
        "\n",
        "            if answer is not None:\n",
        "                stats[answer]['votes'] += 1\n",
        "                stats[answer]['calls'] += result['Python Calls']\n",
        "\n",
        "        sorted_stats = sorted(\n",
        "            stats.items(),\n",
        "            key=lambda item: (item[1]['votes'], item[1]['calls']),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        vote_data = []\n",
        "\n",
        "        for answer, data in sorted_stats:\n",
        "            vote_data.append((answer, data['votes'], data['calls']))\n",
        "\n",
        "        vote_dataframe = pd.DataFrame(vote_data, columns=['Answer', 'Votes', 'Calls'])\n",
        "        display(vote_dataframe)\n",
        "\n",
        "        final_answer = sorted_stats[0][0]\n",
        "        final_votes = sorted_stats[0][1]['votes']\n",
        "        final_calls = sorted_stats[0][1]['calls']\n",
        "\n",
        "        print(f'\\nFinal Result: {final_answer} | Votes: {final_votes} | Calls: {final_calls}\\n')\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    def solve_problem_v2(self, problem: str) -> int:\n",
        "        \"\"\"Solve a problem with ensemble prompts and adaptive early stopping.\"\"\"\n",
        "\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"# NEW PROBLEM\")\n",
        "        print(f\"{'#'*70}\")\n",
        "        print(f\"\\n{problem}\\n\")\n",
        "        print(f\"{'#'*70}\\n\")\n",
        "\n",
        "        # Calculate time budget\n",
        "        elapsed_global = time.time() - self.notebook_start_time\n",
        "        time_left = self.cfg.notebook_limit - elapsed_global\n",
        "        problems_left_others = max(0, self.problems_remaining - 1)\n",
        "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
        "\n",
        "        budget = time_left - reserved_time\n",
        "        budget = min(budget, self.cfg.high_problem_timeout)\n",
        "        budget = max(budget, self.cfg.base_problem_timeout)\n",
        "\n",
        "        deadline = time.time() + budget\n",
        "\n",
        "        print(f\"â±ï¸  Time Budget: {budget:.0f}s | Problems Remaining: {self.problems_remaining}\")\n",
        "        print(f\"â±ï¸  Global Elapsed: {elapsed_global:.0f}s | Reserved for others: {reserved_time:.0f}s\\n\")\n",
        "\n",
        "        # Prepare tasks\n",
        "        tasks = []\n",
        "        for attempt_index in range(self.cfg.attempts):\n",
        "            config = self._get_attempt_config(attempt_index, problem)\n",
        "            tasks.append((config['system_prompt'], attempt_index))\n",
        "\n",
        "        detailed_results = []\n",
        "        valid_answers = []\n",
        "\n",
        "        stop_event = threading.Event()\n",
        "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
        "\n",
        "        solve_start = time.time()\n",
        "\n",
        "        try:\n",
        "            futures = []\n",
        "\n",
        "            for (system_prompt, attempt_index) in tasks:\n",
        "                config = self._get_attempt_config(attempt_index, problem)\n",
        "                full_problem = f\"{problem} {config['preference_prompt']}\"\n",
        "\n",
        "                future = executor.submit(\n",
        "                    self._process_attempt_v2,\n",
        "                    full_problem,\n",
        "                    system_prompt,\n",
        "                    attempt_index,\n",
        "                    stop_event,\n",
        "                    deadline\n",
        "                )\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    detailed_results.append(result)\n",
        "\n",
        "                    if result['Answer'] is not None:\n",
        "                        valid_answers.append(result['Answer'])\n",
        "\n",
        "                    # Adaptive early stopping\n",
        "                    completed = len(detailed_results)\n",
        "\n",
        "                    if completed >= self.cfg.min_samples_before_stop:\n",
        "                        counts = Counter(valid_answers).most_common()\n",
        "\n",
        "                        if counts:\n",
        "                            top_count = counts[0][1]\n",
        "                            threshold = max(\n",
        "                                self.cfg.early_stop,\n",
        "                                (completed // 2) + 1\n",
        "                            )\n",
        "\n",
        "                            if top_count >= threshold:\n",
        "                                has_clear_winner = (\n",
        "                                    len(counts) == 1 or\n",
        "                                    counts[0][1] > counts[1][1] + 1\n",
        "                                )\n",
        "\n",
        "                                if has_clear_winner:\n",
        "                                    print(f\"\\nâš¡ EARLY STOP: Answer {counts[0][0]} has {top_count}/{completed} votes\")\n",
        "                                    stop_event.set()\n",
        "\n",
        "                                    for f in futures:\n",
        "                                        f.cancel()\n",
        "                                    break\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f'âŒ Future failed: {exc}')\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            executor.shutdown(wait=False, cancel_futures=True)\n",
        "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
        "\n",
        "        solve_time = time.time() - solve_start\n",
        "\n",
        "        # ========== ANALYSIS SUMMARY ==========\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ðŸ“Š PROBLEM ANALYSIS SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if detailed_results:\n",
        "            results_df = pd.DataFrame(detailed_results)\n",
        "            results_df['Answer'] = results_df['Answer'].astype('Int64')\n",
        "\n",
        "\n",
        "            # --- NEW CODE START ---\n",
        "            # 1. Add a column to identify which problem these attempts belong to\n",
        "            # (Using the first 100 characters of the problem text as a unique tag)\n",
        "            results_df['Problem_Preview'] = problem[:100]\n",
        "\n",
        "            # 2. Define filename\n",
        "            output_filename = \"all_detailed_results.csv\"\n",
        "\n",
        "            # 3. Check if file exists so we only write the header once\n",
        "            write_header = not os.path.exists(output_filename)\n",
        "\n",
        "            # 4. Append to CSV (mode='a')\n",
        "            try:\n",
        "                results_df.to_csv(output_filename, mode='a', header=write_header, index=False)\n",
        "                print(f\"  ðŸ’¾ Saved attempt details to {output_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  âš ï¸ Could not save to file: {e}\")\n",
        "            # --- NEW CODE END ---\n",
        "\n",
        "            # Basic stats\n",
        "            total_attempts = len(detailed_results)\n",
        "            successful = sum(1 for r in detailed_results if r['Answer'] is not None)\n",
        "            total_python_calls = sum(r['Python Calls'] for r in detailed_results)\n",
        "            total_python_errors = sum(r['Python Errors'] for r in detailed_results)\n",
        "            total_tokens = sum(r['Response Length'] for r in detailed_results)\n",
        "\n",
        "            print(f\"\\nðŸ“ˆ ATTEMPT STATISTICS:\")\n",
        "            print(f\"  Attempts completed: {total_attempts}/{self.cfg.attempts}\")\n",
        "            print(f\"  Successful (found answer): {successful}/{total_attempts} ({100*successful/total_attempts:.0f}%)\")\n",
        "            print(f\"  Total time: {solve_time:.1f}s\")\n",
        "            print(f\"  Total tokens: {total_tokens:,}\")\n",
        "            print(f\"  Tokens/second: {total_tokens/solve_time:.0f}\")\n",
        "\n",
        "            print(f\"\\nðŸ PYTHON TOOL USAGE:\")\n",
        "            print(f\"  Total calls: {total_python_calls}\")\n",
        "            print(f\"  Total errors: {total_python_errors}\")\n",
        "            if total_python_calls > 0:\n",
        "                print(f\"  Error rate: {100*total_python_errors/total_python_calls:.1f}%\")\n",
        "            print(f\"  Avg calls/attempt: {total_python_calls/total_attempts:.1f}\")\n",
        "\n",
        "            # Exit reasons analysis\n",
        "            exit_reasons = Counter(r.get('Exit Reason', 'unknown') for r in detailed_results)\n",
        "            print(f\"\\nðŸšª EXIT REASONS:\")\n",
        "            for reason, count in exit_reasons.most_common():\n",
        "                print(f\"  {reason}: {count}\")\n",
        "\n",
        "            # Temperature analysis\n",
        "            if 'Temperature' in results_df.columns:\n",
        "                temp_success = {}\n",
        "                for r in detailed_results:\n",
        "                    temp = r.get('Temperature', 0)\n",
        "                    temp_bucket = f\"{temp:.1f}\"\n",
        "                    if temp_bucket not in temp_success:\n",
        "                        temp_success[temp_bucket] = {'total': 0, 'success': 0}\n",
        "                    temp_success[temp_bucket]['total'] += 1\n",
        "                    if r['Answer'] is not None:\n",
        "                        temp_success[temp_bucket]['success'] += 1\n",
        "\n",
        "                print(f\"\\nðŸŒ¡ï¸  TEMPERATURE ANALYSIS:\")\n",
        "                for temp, stats in sorted(temp_success.items()):\n",
        "                    rate = 100 * stats['success'] / stats['total'] if stats['total'] > 0 else 0\n",
        "                    print(f\"  Temp {temp}: {stats['success']}/{stats['total']} success ({rate:.0f}%)\")\n",
        "\n",
        "            # Answer distribution\n",
        "            print(f\"\\nðŸŽ¯ ANSWER DISTRIBUTION:\")\n",
        "            answer_counts = Counter(valid_answers)\n",
        "            for answer, count in answer_counts.most_common():\n",
        "                pct = 100 * count / len(valid_answers) if valid_answers else 0\n",
        "                print(f\"  {answer}: {count} votes ({pct:.0f}%)\")\n",
        "\n",
        "            # Display full results table\n",
        "            print(f\"\\nðŸ“‹ DETAILED RESULTS: TABLES:\")\n",
        "            display(results_df)\n",
        "\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        if not valid_answers:\n",
        "            print('âŒ No valid answers found. Returning 0.\\n')\n",
        "            return 0\n",
        "\n",
        "        return self._select_answer(detailed_results)\n",
        "\n",
        "\n",
        "    def solve_problem(self, problem: str) -> int:\n",
        "        \"\"\"Solve a problem with ensemble prompts and adaptive early stopping.\"\"\"\n",
        "\n",
        "        print(f'\\nProblem: {problem}\\n')\n",
        "\n",
        "        # Calculate time budget\n",
        "        elapsed_global = time.time() - self.notebook_start_time\n",
        "        time_left = self.cfg.notebook_limit - elapsed_global\n",
        "        problems_left_others = max(0, self.problems_remaining - 1)\n",
        "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
        "\n",
        "        budget = time_left - reserved_time\n",
        "        budget = min(budget, self.cfg.high_problem_timeout)\n",
        "        budget = max(budget, self.cfg.base_problem_timeout)\n",
        "\n",
        "        deadline = time.time() + budget\n",
        "\n",
        "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
        "\n",
        "        # Prepare tasks with ensemble configurations\n",
        "        tasks = []\n",
        "        for attempt_index in range(self.cfg.attempts):\n",
        "            config = self._get_attempt_config(attempt_index, problem)\n",
        "            tasks.append((config['system_prompt'], config[\"developer_prompt\"], attempt_index))\n",
        "\n",
        "        detailed_results = []\n",
        "        valid_answers = []\n",
        "\n",
        "        stop_event = threading.Event()\n",
        "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
        "\n",
        "        try:\n",
        "            futures = []\n",
        "\n",
        "            for (system_prompt, dev_prompt, attempt_index) in tasks:\n",
        "                # Add preference prompt to problem\n",
        "                config = self._get_attempt_config(attempt_index, problem)\n",
        "                full_problem = f\"{problem} {config['preference_prompt']}\"\n",
        "\n",
        "                future = executor.submit(\n",
        "                    self._process_attempt_v2,\n",
        "                    full_problem,\n",
        "                    system_prompt,\n",
        "                    attempt_index,\n",
        "                    stop_event,\n",
        "                    deadline\n",
        "                )\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    detailed_results.append(result)\n",
        "\n",
        "                    if result['Answer'] is not None:\n",
        "                        valid_answers.append(result['Answer'])\n",
        "\n",
        "                    # Adaptive early stopping\n",
        "                    completed = len(detailed_results)\n",
        "\n",
        "                    if completed >= self.cfg.min_samples_before_stop:\n",
        "                        counts = Counter(valid_answers).most_common()\n",
        "\n",
        "                        if counts:\n",
        "                            top_count = counts[0][1]\n",
        "\n",
        "                            # Dynamic threshold: need > 50% of completed\n",
        "                            threshold = max(\n",
        "                                self.cfg.early_stop,\n",
        "                                (completed // 2) + 1\n",
        "                            )\n",
        "\n",
        "                            # Also check for no close second\n",
        "                            if top_count >= threshold:\n",
        "                                has_clear_winner = (\n",
        "                                    len(counts) == 1 or\n",
        "                                    counts[0][1] > counts[1][1] + 1\n",
        "                                )\n",
        "\n",
        "                                if has_clear_winner:\n",
        "                                    print(f'Early stop: {counts[0][0]} has '\n",
        "                                          f'{top_count}/{completed} votes')\n",
        "                                    stop_event.set()\n",
        "\n",
        "                                    for f in futures:\n",
        "                                        f.cancel()\n",
        "                                    break\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f'Future failed: {exc}')\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            executor.shutdown(wait=False, cancel_futures=True)\n",
        "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
        "\n",
        "        if detailed_results:\n",
        "            results_df = pd.DataFrame(detailed_results)\n",
        "            results_df['Answer'] = results_df['Answer'].astype('Int64')\n",
        "            display(results_df)\n",
        "\n",
        "        if not valid_answers:\n",
        "            print('\\nResult: 0\\n')\n",
        "            return 0\n",
        "\n",
        "        return self._select_answer(detailed_results)\n",
        "\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        if hasattr(self, 'server_process'):\n",
        "            self.server_process.terminate()\n",
        "            self.server_process.wait()\n",
        "\n",
        "        if hasattr(self, 'log_file'):\n",
        "            self.log_file.close()\n",
        "\n",
        "        if hasattr(self, 'sandbox_pool'):\n",
        "            while not self.sandbox_pool.empty():\n",
        "                try:\n",
        "                    sb = self.sandbox_pool.get_nowait()\n",
        "                    sb.close()\n",
        "\n",
        "                except Exception:\n",
        "                    pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:22:36.804095Z",
          "iopub.execute_input": "2026-01-09T19:22:36.804732Z",
          "iopub.status.idle": "2026-01-09T19:22:36.85978Z",
          "shell.execute_reply.started": "2026-01-09T19:22:36.804712Z",
          "shell.execute_reply": "2026-01-09T19:22:36.859354Z"
        },
        "id": "pHUXxLqOE2fF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "solver = AIMO3Solver(CFG)"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:22:37.087254Z",
          "iopub.execute_input": "2026-01-09T19:22:37.087761Z",
          "iopub.status.idle": "2026-01-09T19:23:37.935273Z",
          "shell.execute_reply.started": "2026-01-09T19:22:37.087745Z",
          "shell.execute_reply": "2026-01-09T19:23:37.934796Z"
        },
        "id": "JgczgwxQE2fJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "22IrQUg-E2fJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
        "    global correct_count, total_count, predictions\n",
        "\n",
        "    question_id = id_.item(0)\n",
        "    question_text = question.item(0)\n",
        "\n",
        "    print(\"------\")\n",
        "    print(f\"ID: {question_id}\")\n",
        "    print(f\"Question: {question_text[:200]}...\")\n",
        "\n",
        "    final_answer = solver.solve_problem_v2(question_text)\n",
        "    predictions[question_id] = final_answer\n",
        "\n",
        "    # Check accuracy if ground truth available\n",
        "    total_count += 1\n",
        "    if question_id in ground_truth:\n",
        "        gt = ground_truth[question_id]\n",
        "        is_correct = (final_answer == gt)\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
        "        print(f\"Answer: {final_answer} | Ground Truth: {gt} | {status}\")\n",
        "        print(f\"ðŸ“Š Running Accuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"Answer: {final_answer}\")\n",
        "\n",
        "    print(\"------\\n\")\n",
        "\n",
        "    return pl.DataFrame({'id': question_id, 'answer': final_answer})\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:23:37.936015Z",
          "iopub.execute_input": "2026-01-09T19:23:37.936172Z",
          "iopub.status.idle": "2026-01-09T19:23:37.944116Z",
          "shell.execute_reply.started": "2026-01-09T19:23:37.936157Z",
          "shell.execute_reply": "2026-01-09T19:23:37.943655Z"
        },
        "id": "LjnWo80yE2fJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Load the existing reference data\n",
        "# Make sure to point to the correct path where your current reference file resides\n",
        "df = pd.read_csv(\n",
        "    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n",
        ")\n",
        "\n",
        "# Define the 10 new hard problems\n",
        "new_problems = [\n",
        "    {\n",
        "        \"problem\": r\"Let $B$ be the set of rectangular boxes with surface area $54$ and volume $23$. Let $r$ be the radius of the smallest sphere that can contain each of the rectangular boxes that are elements of $B$. The value of $r^2$ can be written as $\\frac{p}{q}$, where $p$ and $q$ are relatively prime positive integers. Find $p + q$.\",\n",
        "        \"answer\": 721\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Find the largest prime number $p < 1000$ for which there exists a complex number $z$ satisfying: the real and imaginary parts of $z$ are both integers; $|z| = \\sqrt{p}$; and there exists a triangle whose three side lengths are $p$, the real part of $z^3$, and the imaginary part of $z^3$.\",\n",
        "        \"answer\": 349\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"For each positive integer $n$ let $a_n$ be the least positive integer multiple of $23$ such that $a_n \\equiv 1 \\pmod{2^n}$. Find the number of positive integers $n$ less than or equal to $1000$ that satisfy $a_n = a_{n+1}$.\",\n",
        "        \"answer\": 363\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Let $x, y, z$ be positive real numbers satisfying $\\sqrt{2x - xy} + \\sqrt{2y - xy} = 1$, $\\sqrt{2y - yz} + \\sqrt{2z - yz} = \\sqrt{2}$, $\\sqrt{2z - zx} + \\sqrt{2x - zx} = \\sqrt{3}$. Then $[(1-x)(1-y)(1-z)]^2$ can be written as $\\frac{n}{m}$ where $m,n$ are coprime positive integers. Find $m+n$.\",\n",
        "        \"answer\": 33\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Let $S$ be the set of positive integers $k$ such that the two parabolas $y = x^2 - k$ and $x = 2(y-20)^2 - k$ intersect in four distinct points, and these four points lie on a circle with radius at most $21$. Find the sum of the least element of $S$ and the greatest element of $S$.\",\n",
        "        \"answer\": 285\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Let $ABC$ be an acute triangle with circumcircle $\\omega$, and let $H$ be the intersection of the altitudes. Suppose the tangent to the circumcircle of $\\triangle HBC$ at $H$ intersects $\\omega$ at $X$ and $Y$ with $HA=3, HX=2, HY=6$. The area of $\\triangle ABC$ is $m\\sqrt{n}$, where $n$ is square-free. Find $m+n$.\",\n",
        "        \"answer\": 58\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Let $\\triangle ABC$ be an acute scalene triangle with circumcircle $\\omega$. The tangents to $\\omega$ at $B$ and $C$ intersect at $T$. Let $X$ and $Y$ be the projections of $T$ onto lines $AB$ and $AC$, respectively. Suppose $BT=CT=16$, $BC=22$, and $TX^2+TY^2+XY^2=1143$. Find $XY^2$.\",\n",
        "        \"answer\": 717\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"The area of the smallest equilateral triangle with one vertex on each of the sides of the right triangle with side lengths $2\\sqrt{3}, 5, \\sqrt{37}$ is $\\frac{m\\sqrt{p}}{n}$, where $m,n$ are coprime and $p$ is square-free. Find $m+n+p$.\",\n",
        "        \"answer\": 145\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"Tetrahedron $ABCD$ has $AD=BC=28, AC=BD=44, AB=CD=52$. For any point $X$ in space, define $f(X) = AX+BX+CX+DX$. The least possible value of $f(X)$ can be expressed as $m\\sqrt{n}$, where $m,n$ are integers and $n$ is square-free. Find $m+n$.\",\n",
        "        \"answer\": 682\n",
        "    },\n",
        "    {\n",
        "        \"problem\": r\"For each integer $n \\geq 2$, let $A(n)$ be the area of the region in the coordinate plane defined by the inequalities $1 \\le x < n$ and $0 \\le y \\le x \\lfloor \\sqrt{x} \\rfloor$. Find the number of values of $n$ with $2 \\le n \\le 1000$ for which $A(n)$ is an integer.\",\n",
        "        \"answer\": 483\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create a DataFrame for the new problems\n",
        "new_rows = []\n",
        "for p in new_problems:\n",
        "    # Generate a random 6-character hex ID\n",
        "    new_rows.append([str(uuid.uuid4().hex)[:6], p['problem'], p['answer']])\n",
        "\n",
        "df_new = pd.DataFrame(new_rows, columns=['id', 'problem', 'answer'])\n",
        "\n",
        "# Concatenate the original and new DataFrames\n",
        "df = pd.concat([df, df_new], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame back to CSV\n",
        "df.to_csv(\"reference.csv\", index=False)\n",
        "# df_new.to_csv(\"reference2.csv\", index=False)\n",
        "\n",
        "print(\"Successfully appended 10 problems. New shape:\", df.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:23:37.944744Z",
          "iopub.execute_input": "2026-01-09T19:23:37.944924Z",
          "iopub.status.idle": "2026-01-09T19:23:37.970612Z",
          "shell.execute_reply.started": "2026-01-09T19:23:37.944909Z",
          "shell.execute_reply": "2026-01-09T19:23:37.970289Z"
        },
        "id": "B06glBNrE2fJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load reference data and keep ground truth for accuracy calculation\n",
        "\n",
        "# Store ground truth answers for accuracy calculation (only in local mode)\n",
        "ground_truth = dict(zip(df[\"id\"], df[\"answer\"])) if \"answer\" in df.columns else {}\n",
        "\n",
        "# Create input file without answers\n",
        "df.drop(\"answer\", axis=1, errors=\"ignore\").to_csv(\"reference.csv\", index=False)\n",
        "\n",
        "# Track predictions for accuracy calculation\n",
        "predictions = {}\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:23:37.971533Z",
          "iopub.execute_input": "2026-01-09T19:23:37.971792Z",
          "iopub.status.idle": "2026-01-09T19:23:37.982444Z",
          "shell.execute_reply.started": "2026-01-09T19:23:37.971777Z",
          "shell.execute_reply": "2026-01-09T19:23:37.982148Z"
        },
        "id": "44Yj4PnmE2fJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:23:37.983197Z",
          "iopub.execute_input": "2026-01-09T19:23:37.983347Z",
          "iopub.status.idle": "2026-01-09T19:23:37.998195Z",
          "shell.execute_reply.started": "2026-01-09T19:23:37.983334Z",
          "shell.execute_reply": "2026-01-09T19:23:37.99782Z"
        },
        "id": "afIfgiD7E2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle_evaluation.aimo_3_inference_server\n",
        "\n",
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway((\"reference.csv\",))\n",
        "    # inference_server.run_local_gateway((\"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\",))\n",
        "\n",
        "    # Print final accuracy summary\n",
        "    if ground_truth and total_count > 0:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"ðŸ“Š FINAL ACCURACY SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Correct: {correct_count}/{total_count}\")\n",
        "        print(f\"Accuracy: {100*correct_count/total_count:.1f}%\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Show details\n",
        "        print(\"\\nDetails:\")\n",
        "        for qid, pred in predictions.items():\n",
        "            if qid in ground_truth:\n",
        "                gt = ground_truth[qid]\n",
        "                status = \"âœ…\" if pred == gt else \"âŒ\"\n",
        "                print(f\"  {qid}: pred={pred}, gt={gt} {status}\")"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-01-09T19:23:37.998708Z",
          "iopub.execute_input": "2026-01-09T19:23:37.998833Z",
          "iopub.status.idle": "2026-01-09T20:04:27.076325Z",
          "shell.execute_reply.started": "2026-01-09T19:23:37.99882Z",
          "shell.execute_reply": "2026-01-09T20:04:27.07587Z"
        },
        "id": "EMfN5jmvE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "WX6GAfxzE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "VzIO0ojpE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "diZkCRSGE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "8Rj492SME2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Z0fo0dbGE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "7COEBHp8E2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "os5gvBHOE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lu2u_obkE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-P4zAEBpE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Q4D3wAkuE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kxN5Zw8UE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uRk3OFPsE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pSptWppkE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "qvYrjTsJE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wMC-UCuLE2fK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "4SJSA1HGE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "VJQm7w71E2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "vc41zIqkE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cdtd5LbzE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "SjsU94IWE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "YWHW9YpWE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KppgFAFqE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "aTBQ6TVeE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "u5q8eLp0E2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pX2Qr2NgE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "aUxfrG-dE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "8E-_OvXvE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "neUJ6mI8E2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0KRUq1uuE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "K-wd_Z6TE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fPZ4HnvrE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0l3qcWPwE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lKkQEMAvE2fL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "2hCYgW22E2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UmS-72vdE2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "U8Ar2vL6E2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "9FqzvLcUE2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "_bRZy82lE2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "znDJmo_tE2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kcD-W-d6E2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KQLtJlP1E2fM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "GbPSqtpsE2fM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}